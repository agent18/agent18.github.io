I"≤3<h2 id="statistics-and-probability">Statistics and probability</h2>

<p><a href="https://www.khanacademy.org/math/statistics-probability">link</a></p>

<p><strong>Goal</strong>: get to annova chi, t statistic and hypothesis testing‚Ä¶ Do
one question in each and understand how to do them‚Ä¶</p>

<p>Will be useful while doing linear regression‚Ä¶</p>

<p>linear regression classification and linear regression float.</p>

<h2 id="central-limit-theoram">Central limit theoram</h2>

<p>Any distribution ++ sample size tending to inf ++ function (mean sum
anything), then you get close normal dist‚Ä¶</p>

<h2 id="sampling-distribution-of-the-sample-mean">Sampling distribution of the sample mean</h2>

<p>We care about this as it gives the mean of the actual
distribution. Example <a href="https://www.khanacademy.org/math/statistics-probability/sampling-distributions-library/sample-means/v/sampling-distribution-of-the-sample-mean?modal=1">here</a>.</p>

<ol>
  <li>n==5 vs n==10 (Lesser kurtosis closer to 0 and skew closer to 0)</li>
</ol>

<p>Kurtosis‚Äì&gt; how pointy or non pointy it is from the normal (it‚Äôs
almost always a negative kurtosis)</p>

<p>skew ‚Äì&gt; whether the mean moves away from the actual mean in what
direction (right is negative)</p>

<ol>
  <li>Variance of sampling distribution of sampling mean <code class="language-plaintext highlighter-rouge">\sigma.\bar{x}
== \sigma.mu /sqrt(n)</code></li>
</ol>

<p><strong>Standard deviation of sampling distribution (of the sample mean) is called Standard error
of the mean</strong></p>

<h2 id="sdsm">SDSM</h2>

<ul>
  <li>Sampling distribution of sampling mean</li>
  <li>Sampling distribution of sampling proportion</li>
</ul>

<p>SD of SDSP == sqrt(p(1-p)/n)</p>

<p>SE of SDSP == sqrt(phat(1-phat)/n)</p>

<p>Margin of error == 2 SDs of SDSP</p>

<h2 id="what-does-ci-of-95-mean">What does CI of 95% mean</h2>

<p>You start with a sampling proportion of <code class="language-plaintext highlighter-rouge">phat</code> and an True proportion
of <code class="language-plaintext highlighter-rouge">p</code>. You determine the SE based on phat (SE of SDSP formula
above). SE is assumed to be an estimate of the SD of the SDSP.</p>

<p>So what we have then is a sample proportion and an SE. There is a 95%
chance that the sample proportion is within the tru proportion +- 2SE
(95% in normal dist). This is the same as the tru proportion being
within +-2SE of the sample proportion.</p>

<p>Also another way of seeing it is, out of 100 sample proportions 95 of
them will contain the true proportion.</p>

<h2 id="t-statistic">t statistic</h2>

<p>Usually we have :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>\bar{x} +- Z * SE/sqrt(n)
</code></pre></div></div>

<p>Z is based on an actual normal distribution.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>\bar{x} +- t * SE/sqrt(n)
</code></pre></div></div>

<p>The above gives 95% confidence when we say 95% confidence.</p>

<p><strong>Why we need Z statistic</strong></p>

<p><a href="https://youtu.be/gLE6y_NwmhQ">Explanation here</a>‚Ä¶</p>

<p><strong>But because we use SE we can‚Äôt use Z anymore.</strong> It‚Äôs SE which is the
problem as shown below.</p>

<p>So basically</p>

<ol>
  <li>
    <p>Z + actual sigma ‚Äì&gt; 95% confidence band spots the mean 95% of the
times</p>
  </li>
  <li>
    <p>Z + SE ‚Äì&gt; 92%</p>
  </li>
  <li>
    <p>T + SE ‚Äì&gt; 95% again.</p>
  </li>
</ol>

<h3 id="code">Code</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="n">st</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">pop</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">pop</span><span class="p">);</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">pop</span><span class="p">.</span><span class="nb">max</span><span class="p">(),</span><span class="n">pop</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">st</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">));</span>
<span class="n">st</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">cdf</span><span class="p">(</span><span class="mf">1.96</span><span class="p">)</span> <span class="c1">## out --&gt; 97.5% of the cumilative dater
</span><span class="n">st</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">st</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">cdf</span><span class="p">(</span><span class="mf">1.96</span><span class="p">))</span> <span class="c1">## out --&gt; 1.96 is the Z score for
#cumilative percentage
</span></code></pre></div></div>

<p>ppf and cdf are based on cumilative values</p>

<h3 id="conditions-for-using-ci-check-it-out-more-thoroughly">Conditions for using CI (check it out more thoroughly)</h3>

<ol>
  <li>Random sample</li>
  <li>Normal distribution of the sample</li>
</ol>

<p>This is that you have &gt;=10 items in successes and failures.</p>

<p>In the <a href="https://youtu.be/WPxnNjD_yoU">t-statistic conditions video</a> when someone did an SRS of 20
to determine a statistic (mean of age) then the teacher says it is not
enough as we need an n&gt;=30. ‚ÄúIf n&gt;=30, then the sampling distribution
will approximately be normal‚Äù.</p>

<ul>
  <li>So basically n&gt;=30 helps the sample be normal</li>
  <li>if distribution is already normal then ‚Ä¶</li>
  <li>if the sample is symmetric</li>
</ul>

<ol>
  <li>Independence condition</li>
</ol>

<p>It‚Äôs preferred if you sample with replacement but that might not be
always possible (e.g., people going out of store). But n&lt;10% of entire
sample  will help uphold the independence condition.</p>

<h2 id="hypothesis-testing">Hypothesis testing</h2>

<p>It‚Äôs about inference to the population.</p>

<p>H0: average liquid dispensed = 530ml
Ha: average liquid dispensed &gt;530ml</p>

<p>And find P(mean of sample| H0 is true). What are the chances of
getting H0 is true given the mean of the sample.</p>

<h3 id="type-1-and-type-2-era">Type 1 and type 2 era</h3>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>H0 True</th>
      <th>H0 False</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>reject H0</td>
      <td>Type I</td>
      <td>Correct</td>
    </tr>
    <tr>
      <td>Fail to reject H0</td>
      <td>Correct</td>
      <td>Type II</td>
    </tr>
  </tbody>
</table>

<h3 id="probability-of-making-a-type-ii-error-later">probability of making a type II error (later)</h3>

<p><a href="https://youtu.be/6_Cuz0QqRWc">This lecture</a></p>

<h3 id="definitive-example">Definitive example</h3>

<p>https://youtu.be/-FtlH4svqx4</p>

<p>Could write a blog post on it‚Ä¶</p>

<h3 id="z-vs-t">Z vs T</h3>

<p><code class="language-plaintext highlighter-rouge">n&gt;30</code> Z, otherwise T (6:20 <a href="https://youtu.be/5ABpqVSx33I">here</a>)</p>

<h3 id="difference-of-means-example">difference of means example</h3>

<p>https://youtu.be/hxZ6uooEJOk</p>

<h2 id="chi-square">Chi square</h2>

<p>Used for goodness of fit as shown <a href="https://youtu.be/2QeDRsxSF9M">here</a>, in this example.</p>

<p>Let X1 be a random pick from a normal std distribution.</p>

<p><code class="language-plaintext highlighter-rouge">Q1 = X1^^2</code> <strong>Chi squared variable</strong></p>

<p><code class="language-plaintext highlighter-rouge">Q2 = X1^^2 + X2^^2</code> <strong>Chi squared variable</strong> ‚Ä¶</p>

<p>And then we have distributions of Q1 and Q2. 1, 2 standing for <code class="language-plaintext highlighter-rouge">dofs+1</code>.</p>

<p>Chi Squared plot is just another PDF. Which we can look up.</p>

<h3 id="example">Example:</h3>

<p>A restaurant says this is their weekly distribution</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>M</th>
      <th>T</th>
      <th>W</th>
      <th>T</th>
      <th>F</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Expected</td>
      <td>30</td>
      <td>14</td>
      <td>50</td>
      <td>20</td>
      <td>40</td>
    </tr>
    <tr>
      <td>Actual</td>
      <td>10</td>
      <td>20</td>
      <td>30</td>
      <td>40</td>
      <td>50</td>
    </tr>
    <tr>
      <td>Difference</td>
      <td>20</td>
      <td>6</td>
      <td>20</td>
      <td>20</td>
      <td>10</td>
    </tr>
    <tr>
      <td>Normalized diff</td>
      <td>20/30</td>
      <td>6/14</td>
      <td>20/50</td>
      <td>‚Ä¶</td>
      <td>‚Ä¶</td>
    </tr>
  </tbody>
</table>

<p>So the above has 4 Dofs. We take the difference and work look up the
Chi^^2 distribution for a given dof.</p>

<p>To know if the Difference is ‚Äúsignificant‚Äù, we look at the Chi^^2
value of difference and see how likely is it to appear in the Chi^^2
PDF for dof=4.</p>

<p>BTW we need to normalize the difference</p>

<p>DOF == 5-1 = 4</p>

<h2 id="linear-regression-and-ci-around-the-fitted-line">Linear Regression and CI around the fitted line</h2>

<h3 id="conditions">conditions</h3>

<p>LINER ‚Äì&gt; Linear, Independent, normal, equal variance, Random</p>

<p><strong>I really need to understand how the SE of slope and SE of constants
come about</strong></p>

<p><strong>What is correlation and covariance etc‚Ä¶</strong></p>

<h2 id="anova-analysis-of-variance-with-f-statistic">Anova (Analysis of Variance) with F-statistic</h2>

<p><a href="https://youtu.be/Xg8_iSkJpAE">Ling</a></p>

<p>Similar to regular HT, we compute some parameter and then check
against a PDF. Here we use F-statistic. But to get there let‚Äôs take an
example.</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>food 1</th>
      <th>food 2</th>
      <th>food 3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Person1</td>
      <td>3</td>
      <td>5</td>
      <td>5</td>
    </tr>
    <tr>
      <td>Person2</td>
      <td>2</td>
      <td>3</td>
      <td>6</td>
    </tr>
    <tr>
      <td>Person3</td>
      <td>1</td>
      <td>4</td>
      <td>7</td>
    </tr>
    <tr>
      <td>Mean</td>
      <td>2</td>
      <td>4</td>
      <td>6</td>
    </tr>
  </tbody>
</table>

<p>\bar{X} = 4</p>

<p>SST‚Äì&gt; Sum of Squares Total
SSB‚Äì&gt; Sum of Squares Between different Foods
SSW‚Äì&gt; Sum of Squares within each food</p>

<p>SST == (3-4)^^2 + (5-4)^^2 ‚Ä¶. DOF=8
	== 30</p>

<p>SSB == 3*(2-4)^^2 +3*(4-4)^^2 ‚Ä¶. DOF=2
	== 24</p>

<p>SSW == (3-2)^^2 + (2-2)^^2 ‚Ä¶. DOF ==6
	== 6</p>

<p>SSB + SSW == SST</p>

<p>SSB DOF + SSW DOF == SST DOF</p>

<p>H0: Food doesn‚Äôt make a difference</p>

<p>H1: It does</p>

<p><code class="language-plaintext highlighter-rouge">F_stat = SSB/dof_ssb / (SSW/dof_ssw)</code></p>

<p>In this case Fstat = 24/2 /(6/6)</p>

<p>There is more variation across foods than within persons.</p>

<p>If F_stat is really big then there seems to be a high chance that H0
is false.</p>

<p>And then we look it up on the F_stat chart for alpha=5% or something‚Ä¶</p>

<h2 id="linear-regression-statquest">Linear Regression (Statquest)</h2>

<ol>
  <li>
    <p>fit line based on sum of squared residuals minimization</p>
  </li>
  <li>
    <p>Calculate R^^2 ‚Äì&gt; explains variation around the mean is
explained by the line. SS‚Äì&gt; Sum of squares.</p>

    <p>(SS(mean)- SS(fit))/ SS(mean) = R^^2</p>
  </li>
  <li>
    <p>Calculate P-value for R^^2 to determine if this value is randomly
possible (for example with 2 points you get R^^2=100%)</p>
  </li>
</ol>

<p>LS fits a line for 1 dimension and a plane for 2 dimensions and so on
for n-dimensions.</p>

<p>weight of mouse = 0.1 + height of mouse + flip a coin</p>

<ul>
  <li>
    <p>Random events could reduce sum of squares fit, and increase R^^2 (what
we want but not through random events). (<a href="https://youtu.be/nk2CQITm_eo?t=956">Here</a>)</p>
  </li>
  <li>
    <p>But if it is a random parameter like size of hair or astrology sign,
those that will create nno change to your sum of squares, will
automatically be eliminated</p>
  </li>
</ul>

<h3 id="multiple-regression">Multiple regression</h3>
:ET