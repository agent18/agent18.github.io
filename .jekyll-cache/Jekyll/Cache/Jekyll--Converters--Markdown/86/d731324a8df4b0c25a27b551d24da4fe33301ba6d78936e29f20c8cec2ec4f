I"0Ü<h2 id="issues">Issues</h2>

<h3 id="could-be-issues-hypothetical-or-from-past">Could be issues hypothetical or from past</h3>

<blockquote>
  <p>There are (many issues)[1] we haven‚Äôt been able to look into yet, so
we expect there are other (high-impact areas we haven‚Äôt
listed)[3].</p>
</blockquote>

<p><strong>Claim</strong>: There could be other [3].</p>

<p><strong>Question</strong>: Could there be other [3]?</p>

<p><strong>Example A</strong>: Until a few years back 80khours thought that the best
places to work on were ‚Äúreducing near-term life risks aka reducing
global health risks‚Äù but when they explored that there were global
catastrophic risks that could kill the entire planet and future
generations, they have now changed their stance on where people should
be working considering the impact.</p>

<p><strong>Example Hypothetical</strong>: If medical research into ‚Äòhow to slow aging‚Äô seems
largely promising (90% chance of making it with 10b <span>$</span>
with a 100 people extra), in delivering a mechanism that doubles the
human life expectancy, it could be beneficial to work on it as it
could save <code class="language-plaintext highlighter-rouge">95% * 7b expected people lives/100 = 66m expected people
lives per person working on it</code></p>

<p><em>In this case, I could give a hypothetical example or an example from
the past, which is what I have done here. Can you comment?</em></p>

<hr />

<h3 id="because">Because</h3>

<p><strong>Claim</strong>: Due to [1], it is good to work on [2].</p>

<p><em>because</em></p>

<p>We skip these with just that word there</p>

<h3 id="given-ab">Given AB</h3>

<blockquote>
  <p>Given our take on (the world‚Äôs most pressing problems)[1] and the
(most pressing bottlenecks these issues face)[2], we think the
following (five broad categories of career)[3] are a good place to
(start generating ideas)[4] if (you have the flexibility to consider a
new career path)[5].</p>
</blockquote>

<p><strong>Claim</strong>: Given [1] and [2], it appears that following [3] is a good
place for [4].</p>

<p><strong>Question</strong>: Is following [3], a good place for [4]?</p>

<p><em>I am not sure how to give an example Given [1] and [2]. So I skip
this for now.</em></p>

<h3 id="bad-writing-vague-words">Bad writing vague words</h3>

<blockquote>
  <p>Given our take on (the world‚Äôs most pressing problems)[1] and the
(most pressing bottlenecks these issues face)[2], we think the
following (five broad categories of career)[3] are a good place to
(start generating ideas)[4] if (you have the flexibility to consider a
new career path)[5].</p>
</blockquote>

<p><strong>Claim</strong>: It appears that following [3], is a good place for [4].</p>

<p><strong>Question</strong>: Is following [3], a good place for [4], if [5]?</p>

<p><em>What does 4, even mean? What is the point of it?</em> Should I bother
myself or skip these type of seemingly shit sentences. Who cares about
generating ideas? why?</p>

<h3 id="in-general">In general</h3>

<blockquote>
  <p>(Many of the top problem areas we focus on)[1] are mainly (constrained by
a need for additional research)[2], and we‚Äôve argued that (research)[3] seems
like a high-impact path in general)[4].</p>
</blockquote>

<p><strong>Claim</strong>: [3] seems like [4].</p>

<p><strong>Example</strong>: Working in MIRI as a researcher could save 57k lives and
has a bang-for-the-buck as compared to Climate change (about 1000
times better).</p>

<p><em>I don‚Äôt know what general means so I skip it!</em></p>

<h3 id="contrasting">Contrasting</h3>

<blockquote>
  <p>You can also (support the work of other researchers)[1] in a
(complementary role, such as a project manager, executive assistant,
fundraiser or operations)[2]. We‚Äôve argued (these roles)[3] are often
neglected, and therefore especially high-impact. It‚Äôs often useful
to have (graduate training in the relevant area)[4] before taking these
roles.</p>
</blockquote>

<p><strong>Claim</strong>: It is good to [1] in [2].</p>

<p><strong>Example</strong>: As discussed earlier, AI safety is really quite neglected
with 100 people working on it with 10m <span>$</span>. Neil Bowerman
from 80khours is trying to add people required to fill the ‚Äútalent
gaps‚Äù. If Neil is able to add 10 more people and even claim 1% of
their total impact that would be 570 lives saved just for his work in
a few years. Contrast that to a DS job which saves 400 people</p>

<p><em>I think it is important to contrast it with something otherwise it is
hard for someone to understand if it is good or bad. Agree: to always
contrast?</em></p>

<h3 id="how-useful">How useful</h3>

<blockquote>
  <p>You can also (support the work of other researchers)[1] in a
(complementary role, such as a project manager, executive assistant,
fundraiser or operations)[2]. We‚Äôve argued (these roles)[3] are often
neglected, and therefore especially high-impact. It‚Äôs often useful
to have (graduate training in the relevant area)[4] before taking these
roles.</p>
</blockquote>

<p><strong>Claim</strong>: It is useful to have [4] before [3].</p>

<p><strong>Example</strong>:</p>

<p><strong>Split</strong>:</p>

<p>For [4] before [3]: Neil Bowerman has a PhD (equivalent) in Physics,
where he worked on existential risks of extreme climate change with a
focus on providing emission targets.</p>

<p>Also Sean O hEigeartaigh, from CSER has a PhD in Genome Evolution, he
is also known to increase the number of people at FHI and secure
rougly 3m <span>$</span> in funding. Now he is completely in
operations such as grantwriting, fundraising, long-term planning
etc‚Ä¶</p>

<p><em>Not sure how ‚Äúuseful‚Äù [4] is before [3]! or how to even go about answering
it!</em></p>

<p>This seems to be exactly what happened with Rob Mather from AMF who
claimed that his sales experience helped him.</p>

<h3 id="facts">facts</h3>

<blockquote>
  <p>(Mozart)[8] scored a (130 percent on the precocity index)[9] whereas
(his current contemporaries)[10] scored (thirty to five-hundred
percent)[10a].</p>
</blockquote>

<p><strong>Claim</strong>: [8] scored [9].</p>

<p><strong>Example</strong>: <em>All this probably requires is a citation?</em></p>

<h3 id="if-then-proof-type-statements">if then proof-type statements</h3>

<blockquote>
  <p>If talent existed and (refused to show itself even after so many
years of life)[6], it beckons if (inate ability)[7] (talent) even exists.</p>
</blockquote>

<p><strong>Claim</strong>: If [7] exists and it [6], then [7] doesn‚Äôt exist</p>

<p><strong>Example</strong>: <em>unable to give examples for this if-then/proof-type
statements</em></p>

<h3 id="could-lead-to">Could lead to</h3>

<blockquote>
  <p>(These advances)[5] could lead to (extremely positive developments,
presenting solutions to now-intractable global problems)[6], but they
also pose (severe risks)[7]. (Humanity‚Äôs superior intelligence)[8] is
pretty much the sole reason that (it is the dominant species on the
planet)[9]. If (machines surpass humans in intelligence)[10], then just
as the fate of gorillas currently depends on the actions of humans,
the (fate of humanity may come to depend more on the actions of
machines than our own)[1].</p>
</blockquote>

<p><strong>Claim</strong>: [5]/[3]/[1] could lead to [6].</p>

<p><strong>Example</strong>:</p>

<p>AlphaGo, identified superior ways of playing GO which were previously
considered rubbish by humans for thousands of years. Computers seem
like they can go beyond what humans can see with years and years of
work, within just a year. Similarly, it could be possible to cure
cancer and other diseases.</p>

<p><em>how to do I give an example for ‚Äúcould lead to‚Äù? I don‚Äôt think I have
given one above!‚Äù</em></p>

<h3 id="if-then">if then</h3>

<blockquote>
  <p>(These advances)[5] could lead to (extremely positive developments,
presenting solutions to now-intractable global problems)[6], but
they also pose (severe risks)[7]. (Humanity‚Äôs superior
intelligence)[8] is pretty much the sole reason that (it is the
dominant species on the planet)[9]. If (machines surpass humans in
intelligence)[10], then just as the fate of gorillas currently
depends on the actions of humans, the (fate of humanity may come to
depend more on the actions of machines than our own)[1].</p>
</blockquote>

<p><strong>Claim</strong>: if [10] then [1].</p>

<p><em>I have no idea how to answer this claim, how do I give an example
that will inform if A then B.</em></p>

<h2 id="entire-article-from-start-to-finish-challenge">Entire article from start to finish challenge</h2>

<blockquote>
  <p>At 80,000 Hours, we (help people find careers that more effectively
‚Äòmake a difference‚Äô)[1], ‚Äòdo good‚Äô, or ‚Äòhave a positive impact‚Äô on a
(large scale)[2].</p>
</blockquote>

<p><strong>Question</strong>: Does 80khours do [1] on [2]?</p>

<p><strong>Example</strong>: <a href="https://80000hours.org/about/impact/studies-of-career-change/#1-owen-cb">Here</a> we see studies after studies about how peoples
plans ‚Äúamazingly changed‚Äù to something else. Zero fucks seem to be
given about some numbers that would say things like, ‚Äúif he continued
he would have got X impact, but he didn‚Äôt and look at his current
impact! Suck it‚Äù. Maybe there is more explanation later.</p>

<p>Making a difference on a large scale maybe thought of as things like
saving 1000 people over a lifetime. I will take it as a win then? or</p>

<blockquote>
  <p>Here, we lay out what we mean by these phrases. In a nutshell
‚Äò(making a difference)[3]‚Äô is about (promoting the long-term welfare
of everyone)[4] in ways that (respect the rights of others)[5].</p>
</blockquote>

<p>Not a claim but a definition.</p>

<p><strong>Split</strong>:</p>

<p>For [4], what does it mean? increasing the life expectancy? in
underdeveloped countries?</p>

<p>From <a href="https://www.againstmalaria.com/WhyMalaria.aspx">AMF website</a>, we think of decreasing the number of deaths
from 1.5 million to 0.5 million and allowing 1 million people to
live. All this per year!</p>

<p>For [5], I am not really sure what sort of example would cut it. Right
to pray? right to eat? right to live in their environment, right to
healthcare, right to cheap education.</p>

<p>But I still don‚Äôt know what it means to do [3] in ways of [4]. I skip
it for now. Not worth spending time on this shit!</p>

<p><strong>Example</strong>:</p>

<blockquote>
  <p>This section also sketches out some of the ethical considerations
that inform our advice. Much of our advice doesn‚Äôt entirely depend
on these views, but we think it‚Äôs important to be transparent about
them. If you want to read our practical suggestions about which
global problems and careers to focus on, skip ahead.</p>
</blockquote>

<p>So we skip the whole thing above, and we deal with it as it comes in
the passage. I am not interested if they have actually detailed
something out in the</p>

<h4 id="impartial-concern">impartial concern</h4>

<blockquote>
  <p>When it comes to (making a difference)[1], we aim to be (impartial)[2]
in the sense that we give (equal weight to everyone‚Äôs
interests)[3]. This means we strive to (avoid bias against people
based on their race, gender, sexuality or other identities)[4]. We
also try not to (privilege any particular place, nation, time, or
even species above any other)[5].</p>
</blockquote>

<p><strong>Question</strong>: Does 80khrs aim to give [3]?</p>

<p><strong>Example</strong>: I will skip this, and move to the next sentence that has
more concrete explanations of what they mean with ‚Äú[3]‚Äù.</p>

<p><em>Aim implies that an example needs to be provided which aims at being
impartial, but needn‚Äôt succeed?</em></p>

<p><strong>Question</strong>: Does 80khours strive to avoid bias against race?</p>

<p><strong>Example</strong>: 80khours supports to give donation to Effective Altruism
Foundation which in turn pumps funds to GiveWell, which pumps funds to
reduce deaths in sub-saharan Africa. <em>This only shows that it is not
biased against black people?</em></p>

<p><strong>Question</strong>: Does 80k hours strive to avoid bias agains people based
on gender?</p>

<p><strong>Example</strong>: 80khours supports donations to sub-saharan africa where
gender doesn‚Äôt come in the picture.</p>

<p><em>Is this an example that is enough?</em></p>

<p><strong>Question</strong>: Does 80khours try not to [5]?</p>

<p><strong>Split</strong>:</p>

<p><em>What does above any other even mean?</em></p>

<blockquote>
  <p>Instead, we aim to have (moral concern for the interests of all
sentient beings in proportion to how much they will gain or lose by
our actions.)[6] This includes (those who are far away from us)[7]
as well as (those who will (potentially) be members of future
generations)[8].</p>
</blockquote>

<p><strong>Question</strong>: 80khours has [6].</p>

<p><strong>Example</strong>: 80khours support donations to GiveWell (indirectly via
EAF) which primarily pumps fund to sub-saharan Africa. GiveWell looks
at the number of lives saved per <span>$</span> spent to identify the
best charities and pumps money to these charities. It happens that in
sub-saharan africa the <span>$</span> value per life saved is the
highest. This way they don‚Äôt focus on everyone in need, but just the
people in need that they can help the most.</p>

<p><strong>Question</strong>:  Does the people 80khours focus on, [7]?</p>

<p><strong>Example</strong>: 80khours is based in California but tries to focus on
health in poor countries (sub-saharan africa).</p>

<p><strong>Question</strong>: Does 80khours focus on [8].</p>

<p><strong>Example</strong>: 80khours rates AI danger, climate change and also factory
farming as top priority problems as it has the potential to end the
world completely reducing the population to 0. which will probably be
tolerable for this generation, but it‚Äôs really scary what happens to
the rest of the world.</p>

<blockquote>
  <p>From this perspective, we aim to increase (the expected welfare of
others)[9] by (as much as possible)[10], (enabling more individuals to have
lives that are long, healthy, fulfilled, and free from avoidable
suffering)[11].</p>
</blockquote>

<p><strong>Question</strong>: Does 80k aim to increase [9]?</p>

<p><strong>Example</strong>: 80k tries to get people to donate to GiveWell which
inturn pumps funds to AMF, which has reduced the number of deaths in
sub-saharan africa.</p>

<p><strong>Question</strong>: Has 80k, done [11]?</p>

<p><strong>Example</strong>: 80k tries to get people to donate to GiveWell which
inturn pumps funds to AMF, which has reduced the number of deaths in
sub-saharan africa.</p>

<blockquote>
  <p>As individuals, (we all have other goals)[12] besides (impartially
making a difference in this way)[13]. (We care about our friends,
personal projects, other moral aims, and so on)[14]. But we think
the (impartial perspective)[15] is an important one, and it‚Äôs what
our (research and recommendations)[16] are focused on.</p>
</blockquote>

<p><strong>Question</strong>: Do we ALL care about other things, besides [13]?</p>

<p><strong>Example</strong>: <em>Am like what does goal mean? based on the definition of
goals, can the claim be true or not true. then once you read the rest
I guess they are talking about caring!</em></p>

<p>I care about making a significant design contribution like the entire
layout of the stage, for my work besides EA.</p>

<p><strong>Question</strong>: [15] is an important one?</p>

<p><strong>Example</strong>: GiveWell suggests to put money in sub-saharan africa to
cure malaria and save X number of lives for Y <span>$</span>. GiveWell
doesn‚Äôt support the money being used to solve homelessness in the US
for example. Thereby GiveWell Saves more lives.</p>

<p><strong>Question</strong>: 80k‚Äôs research is focused on [15].</p>

<p><strong>Example</strong>: 80k, in it‚Äôs important problem profiles, treats ‚Äòhealth in poor
countries‚Äô as well as ‚Äòfactory farming and its inhumane treatment of
animals‚Äô.</p>

<h4 id="longtermism-checking-and-correcting">longtermism (checking and correcting)</h4>

<blockquote>
  <p>(Homo sapiens)[0] is still an (infant species)[1]. We evolved
around 200,000 years ago, and industrial civilization only
began several hundred years ago; however, the average
species lasts for 1-10 million years. With the (benefit of
technology and foresight)[5], humanity could in principle survive
for at least as long as the earth is habitable ‚Äî (probably hundreds
of millions of)[6] years.</p>
</blockquote>

<p><strong>Question</strong>: Is [0], [1]?</p>

<p><strong>Example</strong>: Humans are around since 200k years ago. Average species
lasts for 1 to 10 million years.</p>

<p><strong>Question</strong>: With [5], we could survive for [6], years.</p>

<p><strong>Example</strong>: If technology is able to solve potential existential
risks like, climate change, meteor crashing into the earth, and also
able to eradicate life shortening diseases like malaria, cancer,
immunity syndromes etc‚Ä¶ then potentially we can live as long as the
earth is habitable.</p>

<p><em>How do you give an example of something you predict in
the future?</em></p>

<blockquote>
  <p>The (possibility of a long future)[1] means there will, in
expectation, be (far more people in the future than there are alive
today)[2]. (Impartial concern)[3] most likely implies (we should value
their welfare as much as anyone‚Äôs)[4]. If (our actions)[5] can predictably
(affect future generations in nontrivial ways)[5a], then because the
(welfare of so many others would be at stake)[6], (these effects would be
what most matter morally about our actions)[7].</p>
</blockquote>

<p><strong>Question</strong>: Does [1], mean [2]?</p>

<p><strong>Example</strong>:</p>

<p>If:</p>
<ul>
  <li>
    <p>we live in a planet where due to technology diseases are going to be
rare and threat from existential risks are low,</p>
  </li>
  <li>
    <p>and as a result if continue with the same growth rate 1.07%/year,</p>
  </li>
</ul>

<p>then: 
we will have 20 billion people by 21 billion people! (3 times within
100 years!</p>

<p>The Population of the world from 1804 has increased from
1 billion to 4 billion in &lt; 100 years starting from 1804. If
technology continues to</p>

<p><em>How do you give an example of something you predict in the future?</em></p>

<p><strong>Question</strong>: Does [3], most likely imply [4]?</p>

<p><strong>Example</strong>:</p>

<p><em>I dont‚Äô know how to give an example for <code class="language-plaintext highlighter-rouge">deductions and
reasoning</code> like the above. or this is a definition</em></p>

<p><em>I see ‚Äúmost likely‚Äù, I guess all this means is 1 example from my
side!</em></p>

<p><strong>Question</strong>: Can [5], do [5a]?</p>

<p><strong>Example</strong>:  <a href="https://climate.nasa.gov/news/2458/why-a-half-degree-temperature-rise-is-a-big-deal/">According to here</a>, we see that causing a 2 degree
increase within this century can increase the sealevels <a href="https://www.activesustainability.com/climate-change/sea-level-rise-causes-and-consequences/">by 0.9m</a> which
poses an increased chance of floods starting with coastal regions
around the world, expected to displace 200-300 million people.</p>

<p><strong>Question</strong>: if [5] can [5a], then [7] is because of [6].</p>

<p><strong>Example</strong>: <em>‚ÄúBecause‚Äù, so we skip for now</em></p>

<p>As there are many billions more lives at stake this implies that this
is one of the most important problems.</p>

<hr />

<p><strong>Breaking down 7</strong></p>

<blockquote>
  <p>(these effects)[a] would be what (most matter morally)[b] about our (actions)[c]</p>
</blockquote>

<p>For A, we think of the rise in sea levels and potential increase in
chances of floods. i.e., death of billions of lives and potential
failure to continue mankind</p>

<p>For B, we think of billions of lives saved (over a million years)</p>

<p>For C, we think of not doing anything about the rise in temperature,
i.e., no change in policies to control rise in temperatures.</p>

<blockquote>
  <p>If (this)[0] is correct, then (approaches to improving the world)[1] should be
evaluated mainly in terms of (their potential long-term impact, over
thousands, millions, or even billions of years)[2]. Making these
evaluations is part of an emerging field of study called
longtermism.</p>
</blockquote>

<p>For ‚Äú[7] is correct‚Äù, we think of ‚Äúsaving n number of lives over a
billion years‚Äù is what matters to us.</p>

<p><strong>Claim</strong>: [1] should be evaluated based on [2], if [0]or[7] is true.</p>

<p><strong>Question</strong>: Should [1], be evaluated based on [2], given [7] is correct?</p>

<p><strong>Example</strong>:</p>

<p>Working on keeping the increase in temperature within 2 degrees within
the century could potentially save billions of lives over millions of
years, as the earth would still be available for habitation. Instead
if we choose to improve the quality of lives of the homeless in the
US, we might not end up keeping the temperature within 2 degrees and
possibly loose billions of lives.</p>

<p><em>This one paragraph above took me 2 hrs atleast over multiple days, I
think primarily because I didn‚Äôt understand [7]. The clue was in
properly understanding what 7 meant, hence the breakdown. Your
comments? Is the depth at which this example was covered satisfactory?
i.e., do we need to know what ‚Äúpotentially save billions of lives over
millions of years‚Äù is exactly?</em></p>

<hr />

<blockquote>
  <p>It‚Äôs difficult to (predict the long-term effects of our actions)[1],
but we think it‚Äôs clear that (the interests of future
generations)[2] are neglected by (most people and institutions
today)[3], suggesting there are (untaken opportunities to
help)[4]. We also think (some of our actions)[5] do have( very
long-term effects)[6] ‚Äî at the very least we can affect the
(probability of existential risks)[7], as covered in the next
section, and there may be (other ways to affect the future as
well)[8]</p>
</blockquote>

<p><strong>Claim</strong>: Difficult to predict [1].</p>

<p><strong>Question</strong>: Is it Difficult to predict [1]?</p>

<p><strong>Example</strong>: One estimate of temperature increase is <a href="https://www.givewell.org/shallow/climate-change/impacts#Malaria">4 degrees</a>. The
uncertainty lies between 2.4 to 6.4 degrees Celsius. Damages from
climate change are proportional to the square of temperature
change. So it is difficult to predict what is going to happen a 2
degree increase leading to 9 meters sea level increase or a 6 degree
increase which is going to be ‚Äúmuch much worse‚Äù.</p>

<p><em>I Googled climate change for a while, but am not able to
come up with a proper example. The example, would need to look like
this I guess: Look at climate model A,B,C and see the variance in the
estimates? or something that shows the uncertainty in prediction of
likelihood of a disaster</em></p>

<p><strong>Claim</strong>: [2] is neglected by [3].</p>

<p><strong>Question</strong>: Who or What are the [3], that neglect [2].</p>

<p><strong>Example</strong>: There are about 100 people worldwide working on ‚Äúcontrol
problem‚Äù for AI, so that machines can pursue ‚Äúrealistic human goals
safely‚Äù. One simple example of the harms of AI could be that AI thinks
killing people is a better way to contain a virus from growing. AI
could thus affect the future generation.</p>

<p><strong>Claim</strong>: [2] neglected by [3] suggests [4].</p>

<p><strong>Question</strong>: Is there [4], given [3] neglect [2]?</p>

<p><strong>Example</strong>: ‚ÄúPositively shaping AI‚Äù has 100 people working it with a
budget of 100m$ for something that could ‚Äúpotentially save billions of
lives.‚Äù Every additional person working here could potentially
contribute to saving billions of lives.</p>

<p><em>I don‚Äôt know if the above example makes sense for ‚Äúgiven [3] neglect [2]‚Äù</em></p>

<p><strong>Claim</strong>: [5] has [6].</p>

<p><strong>Question</strong>: What are the [6], of [5]?</p>

<p><strong>Example</strong>:</p>

<p>According to <a href="https://www.nature.com/scitable/knowledge/library/what-happens-after-global-warming-25887608">here</a>, it takes much greater than 100 years before
the greenhouse gases subsude to levels that bring the temperature down
by 1 degree even. We are expecting a 6 degree rise with a 10%
probability by 2100. The number of people living in water-stressed
river basins increases by</p>

<p><em>Actually I can‚Äôt find one spot where I can identify it as a long term
impact‚Äù. In most cases either the number of lives are not accounted
for or the outcome is better with CC than without!</em></p>

<p><strong>Claim</strong>: We can affect [7], other than 7.</p>

<p><strong>Claim</strong>: There may be [8].</p>

<blockquote>
  <p>We remain unsure about (many of these arguments)[1], but overall
we‚Äôre persuaded that (focusing more on the very long-term
effects of our actions)[2] is (one of the most important ways we can
do more good)[3]. Such a (radical claim)[4] requires (much more
argument)[5], and we outline the (considerations for and against
it)[6], as well as (list further reading)[7], in our full article on
this topic.</p>
</blockquote>

<p><strong>Claim</strong>: We are unsure about [1].</p>

<p><strong>Question:</strong> Are we unsure about [1]?</p>

<p><strong>Example</strong>: I don‚Äôt know what they are unsure about and why!</p>

<p>For [1] we think of, ‚ÄúWith the benefit of technology and foresight,
humanity could survive at least as long as the earth is habitable ‚Äì a
few 100‚Äôs of millions of years.‚Äù</p>

<p><strong>Claim</strong>: We are persuaded that [2] is [3].</p>

<p><strong>Question</strong>: Are we persuaded that [2] is [3]?</p>

<p><strong>Example</strong>: We can save a few 10‚Äôs of people by focusing on homeless
people in US, or we can save possibly billion people by focussing on
climate change. The later keeps in mind future generations!</p>

<p><strong>Claim</strong>: [4] requires much more of [5].</p>

<p><strong>Question</strong>: Does [4], require much more of [5]?</p>

<p><strong>Example</strong>: There is probability involved. There are no numbers
given. <em>is this an example?</em></p>

<p><strong>Claim</strong>: We outline [6], as well as [7] in <a href="https://80000hours.org/articles/future-generations/">this article</a>.</p>

<p><strong>Question</strong>:</p>

<p><strong>Example</strong>:</p>

<p>Need to reflect on the article! no internet access for now!</p>

<h3 id="moral-uncertainty-and-respecting-rights">Moral uncertainty and respecting rights!</h3>

<blockquote>
  <p>As covered, we think that the most important thing for us to
focus on from an impartial perspective is increasing the long-term
welfare of everyone, such as by helping people have longer, more
fulfilling, and happier lives. However, we are not sure that this is
the only thing that matters morally.</p>
</blockquote>

<blockquote>
  <p>(Some moral views)[1] that were widely held in the past are regarded
as (flawed or even abhorrent today)[1a]. (This)[2] suggests we
should expect our (own moral views)[3] to be (flawed in ways that
are difficult for us to recognize)[4]. What‚Äôs more, there is still
(significant moral disagreement)[5] within (society)[6], among
contemporary moral philosophers)[7], and, indeed, (within the 80,000
Hours team)[8]. It‚Äôs also (extremely difficult)[9] to know all the
(ethical implications of our actions)[10], and (grand projects to
advance abstract ethical aims)[11] often go badly.</p>
</blockquote>

<p><strong>Claim</strong>: [1], that were widely held in the past are regarded as
[1a].</p>

<p><strong>Question</strong>: Were [1] held in the past abhorrent today?</p>

<p><strong>Example</strong>: There was a time when blacks were to be seen only as
slaves, but now it is widely unpopular to do anything like that!</p>

<p><strong>Claim</strong>: [2] suggests [3] to be [4].</p>

<p><strong>Question</strong>: Does [2] suggest that [3] is [4]?</p>

<p><em>This is reasoning. How do you give an example that it suggests [3]
is flawed; Skipping this for now!</em></p>

<p><strong>Claim</strong>: [3] could be [4]</p>

<p><strong>Question</strong>: Is [3], [4]?</p>

<p><strong>Example</strong>: Until a few years back all I was focusing on was
Women and not on things like EA which is what I should have always
been focusing on. If it were not for an STM who explained to me with
countless hours how my moral views were flawed and needed to be
oriented towards EA, I am not sure I would have recognized it.</p>

<p><strong>Claim</strong>: There is [5] within [6].</p>

<p><strong>Question</strong>: Is there [5] within [6]?</p>

<p><strong>Example</strong>: Recently Anti-abortion laws were passed in Alabama,
Missouri and Georgia. The law said that even in the case of rape the
abortion cannot go through. This is a predominantly Republican view
and Democrats seem to be completely against that.</p>

<p><strong>Claim</strong>: there is [5] within [7].</p>

<p><strong>Question</strong>: Is there [5], within [7]?</p>

<p><strong>Example</strong>: <em>Can‚Äôt find an example for it. Looked at Robin Hanson and
Eliezer and also things to do with Peter Singer</em></p>

<p><strong>Claim</strong>: There is [5] within [8].</p>

<p><em>Not sure I can come up with this without needless research on the
whole 80000 website. So I skip this!</em></p>

<p><strong>Claim</strong>: It is [9], to know all [10].</p>

<p><strong>Question</strong>: Is it [9], to know all [10]?</p>

<p><strong>Example</strong>: I work for a world leader in Lithography and I am able to
donate 10% of my income. I think I have saved at the end of the year
approximately one life. What I don‚Äôt know is how many lives I have
taken as a result of promoting this company. For example, this company
needs material and sources from the earth, I don‚Äôt know where its
materials are sourced from and if for example child labor was
involved, or poor conditions of work and health leading to reduced
life of someone else. Or another example would be the depletion of
resources in the end bringing the earth to a stop a few days
earlier. I have no way of estimating it and hence it is difficult.</p>

<p><strong>Claim</strong>: [11] often goes bad!</p>

<p><strong>Question</strong>: Does [11], often go bad?</p>

<p><strong>Split</strong>:</p>

<p>For grand projects, we think of Heifer International and their giving
of livestock to ‚Äúpeople in need‚Äù.</p>

<p>For abstract ethical aims, we think of their aim to ‚Äúimprove the
lives‚Äù of people or ‚Äúbring them out of poverty‚Äù</p>

<p>For goes badly, we think of the lack of evidence from Heifer‚Äôs side to
prove the claim that : ‚ÄúA cow is better for you than anything else you
could buy with what the cow costs‚Äù.</p>

<p><a href="https://blog.givewell.org/2009/12/28/celebrated-charities-that-we-dont-recommend/">Here there are more ‚Äúgrand schemes‚Äù</a> that GIVEWELL doesn‚Äôt support aka,
‚Äúgo badly‚Äù.</p>

<p><em>Once more a reminder that without evidence you are nothing!</em></p>

<hr />

<blockquote>
  <p>As a result (of (‚Äúgrand projects going badly‚Äù)[1]), we think it‚Äôs
important to be (modest about our moral views)[2], and in the (rare
cases where there‚Äôs a tension)[3], try very hard to (avoid actions
that seem seriously wrong from a common-sense perspective)[4]. This
is both because (such actions)[5] might be (wrong in themselves)[6],
and because they seem likely to lead to (worse long-term
consequences)[7].</p>
</blockquote>

<blockquote>
  <p>More generally, we aim to factor ‚Äòmoral uncertainty‚Äô into our views
and to uphold cooperative norms. We do this by taking into account a
variety of reasonable ethical perspectives, rather than simply
acting in line with a single point of view.</p>
</blockquote>

<p><strong>Claim</strong>: Due to [1], it is important to be [2].</p>

<p><strong>Question</strong>: Why does [1], lead to [2] being important?</p>

<p><strong>Example</strong>: <em>because</em></p>

<p><strong>Claim</strong>: It is important to be [2].</p>

<p><strong>Question</strong>: Why is it important to be [2]?</p>

<p><strong>Example</strong>:</p>

<p>For moral views, we think of ‚Äúa cow is better for poor people than
giving them cash for the cows worth‚Äù.</p>

<p>For not modest, we think of Heifer International‚Äôs continued claim to
support giving of livestock, without demonstrating the effectiveness
of such a gift.</p>

<p>For important, we think of 0 funds being delivered to Heifer from
GiveWell. And possibly people taking out funds or demanding more
evidence from them.</p>

<p><em>For important I wanted to say something about the wastage of funds as
HI spends on something whose effectiveness it doesn‚Äôt know, but I
don‚Äôt have any numbers!</em></p>

<p>Some mental masturbation perhaps! but whateves. Am I actually enjoying
it? I feel happy when I am able to come up with an example as a result
of my research! Sometimes I mentally masturbate, procrastinate while
looking for info!</p>

<p><strong>Claim</strong>: Due to [1], during [3], try very hard to [4].</p>

<p><em>The English is very troubling. Are they bad writers? or what? How
would any one know what they mean by common sense perspective (I guess
unless they read 80khours in and out, aka, articles related to it!). With my
current example above, it‚Äôs not common sense I think to trust nothing
but controlled randomized experiments. Period!</em></p>

<p><em>In hindsight I think I am working on a summary, and maybe that is the
reason I feel like dying! ;)</em></p>

<p><strong>Example</strong>: <em>because!</em></p>

<p><strong>Claim</strong>: During [3], try very hard to [4]!</p>

<p><strong>Question</strong>: During [3], should we try very hard for [4]?</p>

<p><strong>Example</strong>: <em>No idea! Not gonna break my head over this English! But
as I read some related articles on further reading, maybe they are
talking about‚Ä¶</em> <em>this whole common sense thing is wierd. I am trying
to look in my current commong sense thinking which has been quite
upgraded due to an STM</em></p>

<p>During WWII, there was a lot of tension between nazi Germany and
others! And many Germans might be in favor of killing other races. In
such cases it would be worthwhile to try very hard not to kill people?</p>

<p><strong>Claim</strong>: During [3], try very hard to [4], because [5] might be [6].</p>

<p><strong>Example</strong>: <em>because</em></p>

<p><strong>Claim</strong>: During [3], try very hard to [4], because they seem to lead
to [7].</p>

<p><strong>Example</strong>: <em>because</em></p>

<blockquote>
  <p>More generally, we aim to (factor ‚Äòmoral uncertainty‚Äô into our views)[1]
and to (uphold cooperative norms)[2]. We do this by (taking into account a
variety of reasonable ethical perspectives)[3], rather than simply
(acting in line with a single point of view)[4].</p>
</blockquote>

<p><strong>Claim</strong>: We aim to do [1].</p>

<p><strong>Question</strong>: Does 80k aim to do [1].</p>

<p><em>How am I supposed to come up with examples for these? Am I? It‚Äôs so
fucking hard. What is the goal here? I am not even sure if there is a
point to this! Please tell me your highness</em></p>

<p><strong>Example</strong>:</p>

<p><strong>Split</strong>: For moral uncertainty I think of my uncertainty in the
value of animal lives, in the value of insects, bees, cockroaches.</p>

<p>How do you decide which theory to follow‚Ä¶. total util has the
drawback that it will have some really unhappy people</p>

<p>For moral uncertainty, we think of not know if stealing in a
particular case is wrong!</p>

<p><strong>Claim</strong>: We aim to do [2].</p>

<p><strong>Claim</strong>: We do this by [3] instead of [4].</p>

<p><strong>What is unclear?</strong></p>

<ul>
  <li>
    <p>I don‚Äôt have an example for [1]. I don‚Äôt know how it looks. As a
result I don‚Äôt know how 80khours does [1] by [3] instead of [4].</p>

    <p>I spent 2-3 hrs on this and still not good. I will move on for
now. In parallel I should read about it, find examples about
reasoning amidst moral uncertainty.</p>
  </li>
</ul>

<p><strong>Claim</strong>: We should factor moral uncertainty by using multiple
theories instead of just sticking to one.</p>

<p><strong>Example</strong>:</p>

<blockquote>
  <p>We think that a rights framework captures much of what matters in
these considerations. So we formulate the one-sentence version of
our views as: promoting long-term welfare while respecting the
rights of others.</p>
</blockquote>

<blockquote>
  <p>These are some of the reasons that we think it‚Äôs so important to
respect the rights of others at the same time as aiming to promote
long-term welfare.</p>
</blockquote>

<p>Skip! Lite!</p>

<h2 id="global-priorities">Global Priorities</h2>

<blockquote>
  <p>Now that we have a sense of (what ‚Äòmaking a difference‚Äô means)[1], we can
ask (which career options make a difference most effectively)[2].</p>
</blockquote>

<p><strong>Claim</strong>: We have a sense of [1].</p>

<p><strong>Question</strong>: Do we have a sense of [1]?</p>

<p><strong>Example</strong>: 80khours promotes working on climate change primarily
because of the risks it poses to the lives of future generations. One
estimate suggests <a href="https://www.givewell.org/shallow/climate-change/extreme-risks">300 million people might need to be
displaced</a>. To put that in perspective, we know that displacing 6
million people from Syria, <a href="https://www.npr.org/sections/parallels/2018/03/09/589973165/europe-does-not-see-us-as-human-stranded-refugees-struggle-in-greece?t=1559420984970">was already quite hard for the refugees as
many countries were not accepting refugees.</a></p>

<p><strong>Claim</strong>: Knowing [1], leads to asking [2].</p>

<p><strong>Question</strong>: Does knowing [1], lead to asking [2]?</p>

<p><strong>Example</strong>: Working as a management Consultant can earn money and not
do much for the 300 million people about to be displaced in 2100,
where as becoming president of US, you could change policies to bring
the temperature increase within controllable limits and promote clean
technology.</p>

<blockquote>
  <p>We think that probably the most important single factor that
determines the (expected impact of your work)[1] is the (issue you choose
to focus on ‚Äî whether that‚Äôs climate change, education,
technological development or something else)[2].</p>
</blockquote>

<p><em>Rant: I guess they are trying to segue into their page which talks
about different issues. It‚Äôs been a few hours of having worked on this
statement, but, I still don‚Äôt get it. Don‚Äôt get what? I am not able to
wrap my head around why the issue you focus on most important? Like
Why? As an STM oftens frets about to me, ‚Äúmeasuring anything but the
actual outcome you are interested in is all hogwash!‚Äù</em></p>

<p><em>Now Having said that, disprove the motherfucker!</em></p>

<p><strong>Claim</strong>: [2] is most important factor, which determines [1].</p>

<p><strong>Question</strong>: Is, [2] really the most important factor when it is
judged based on [1].</p>

<p><strong>Example</strong>:</p>

<p>Whether you choose climate change or homelessness or AI, makes quite a
difference in your delivered impact. One can save about a few hundreds
of thousands, the other can save about 100‚Äôs of people, and the last
one potentially, millions.</p>

<p>OK! But what about the probability of actually making an impact at
said field, Isn‚Äôt that an important factor too? If we work in ‚Äòmaking
AI safe‚Äô it is estimated below that the impact is about 6.3 million
people. But what are the odds of me being hired by MIRI because I
don‚Äôt have the skill. In my case, I would think 1%. In that case 1% of
6.3 million is 63000.</p>

<p>In this case atleast, personal fit doesn‚Äôt seem to make a big
difference. So it looks like ‚Äòthe issue‚Äô could indeed be a very
important factor! Oops!</p>

<p>But none of this actually means anything, unless I factor this with
say my capabilities or my probability of succeeding.</p>

<p>Moreover, what I am trying to say is the expected impact of my work
100% depends on the expected impact of my work only. Just because I
work in CC or AIsafety doesn‚Äôt automatically catapult me. There are
several other factors like personal fit (i.e., my probability of
success). Just looking at one seems to be foolishness as with poor
personal fit, you could suck so bad that it might not be worth your starting!</p>

<p>To conclude, the expected impact of my work, is dependent, I don‚Äôt
know how to say if it is the most important factor. As one factor
without the other seems pointless!</p>

<p><em>Based on my example, 80khours seems to have been seriously
exposed. Now what! Can you comment on this?</em></p>

<p><em>To be re-written</em></p>

<blockquote>
  <p>It‚Äôs harder to have (a big impact on commonly supported causes)[1]
because (work in most areas has diminishing marginal returns)[2]. In
other words, if (an area already receives plenty of attention)[3],
then (there will usually already be people working on the most
promising interventions)[4].</p>
</blockquote>

<p><strong>Claim</strong>: It‚Äôs harder to have [1].</p>

<p><strong>Question</strong>: Is it harder to have [1]?</p>

<p><strong>Example</strong>: If you look at making AI safe, there are 100 people
working on it, barely common. If we assume that the current chances of
making AI safe is 0.1%, then we are able to save say 7 million people
effectively. 80khours claims if an additional 100 people work on it,
the chances of making it is 1% more, aka 70 million lives could be
saved. Per person added we seem to have about 6.3million in effective
people saved on average. Talk about BIG!</p>

<p>If you look at Climate Change, there is a 10% chance of 7 billion
people being affected. i.e., effectively 700 million are in
danger. Assuming that there are only 700 people working on CC‚Äîwhich
is probably not accurate at all considering the 10 billion
<span>$</span> in funding‚Äîthe effective possible people saved is 1
million (highly conservative estimate).</p>

<p><strong>Claim</strong>: Its‚Äô harder to have [1], because of [2].</p>

<p><em>because!</em></p>

<p><strong>Claim</strong>: If [3], then [4].</p>

<p><strong>Question</strong>: If [3], then <em>__</em>?</p>

<p><strong>Example</strong>:</p>

<p>For [3], we think of Climate Change(CC), it receives 10b
<span>$</span> in funding.</p>

<p>For [4], we think of the Paris Climate Agreement whose whole aim is to
keep temperature increase ‚Äúwell below‚Äù 2 degrees enforced by 194
‚Äústates‚Äù spanning 88% of the Green house emissions.</p>

<blockquote>
  <p>Although we‚Äôd like to see more people working on many global
problems, we think (additional people)[1] can have the most impact
by focusing on the (issues that are most neglected)[2] relative to
(the magnitude of the stakes)[3] and (the number of promising
opportunities to make progress)[4].</p>
</blockquote>

<p><strong>Claim</strong>: [1] can have the most impact by working on [2], in relation
to [3].</p>

<p><strong>Question</strong>: Can [1], have the most impact by working on [2], in
relation to [3]?</p>

<p><strong>Split</strong>:</p>

<p>For [2], we think of <code class="language-plaintext highlighter-rouge">making AI safe</code> with only 100 people working on
it. For comparison we also think of <code class="language-plaintext highlighter-rouge">institutional decision making</code>
with only 100 to 1k people working on it. Or for that matter we look
into <code class="language-plaintext highlighter-rouge">Global Priorities Research</code> which also seems to have similar
funding of 5m-10m <span>$</span> for it‚Äôs spending.</p>

<p>For [3],  we think of expected savings/person.</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>AI</th>
      <th>Decision making</th>
      <th>Priorities</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Total value of solving</td>
      <td>1%</td>
      <td>0.5%*</td>
      <td>0.5%*</td>
    </tr>
    <tr>
      <td>Doubling effort solves</td>
      <td>1%</td>
      <td>1%</td>
      <td>1%</td>
    </tr>
    <tr>
      <td>Neglectedness</td>
      <td>7</td>
      <td>8</td>
      <td>9</td>
    </tr>
    <tr>
      <td>num of additional people</td>
      <td>100</td>
      <td>500*</td>
      <td>500?</td>
    </tr>
    <tr>
      <td>expected people saved/person**</td>
      <td>7k</td>
      <td>700</td>
      <td>700</td>
    </tr>
  </tbody>
</table>

<p>/* - averaged value between upper and lower limit given in 80khours
? -  ‚Äúeducated‚Äù guess
/**- expected savings based on assumption that total damage would be</p>

<p><em>And without such clarity in numbers how the fuck do you even begin to
compare different interventions! I wish 80khours used same numbers
across everywhere and not vague terms like ‚Äú<a href="https://80000hours.org/problem-profiles/improving-institutional-decision-making/#what-can-you-do-in-this-area">improves value of future
by 1%</a>‚Äù</em></p>

<p><strong>Example</strong>:</p>

<p>In order to the make the most impact one would choose AI (7k people),
which seems to be based on ‚Äúmost neglected issue‚Äù and ‚Äúthe magnitude
of the stakes‚Äù.</p>

<p><strong>Claim</strong>: [1] can have the most impact by working on [2], by
focusing on ‚Äò[3] and [4]‚Äô.</p>

<p><strong>Question</strong>: Can [1], have the most impact by working on [2],
focusing on ‚Äò[3] and [4]‚Äô?</p>

<p><strong>Split</strong>:</p>

<p>For [2], we think of <code class="language-plaintext highlighter-rouge">making AI safe</code>, <code class="language-plaintext highlighter-rouge">institutional decision
making</code> and <code class="language-plaintext highlighter-rouge">global priorities research</code></p>

<p>For [3], we have seen above</p>

<p>For [4], number of opportunities, for me (I assume), which seem to be
the same for all issues below:</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>AI</th>
      <th>Decision Making</th>
      <th>Priorities</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Policy</td>
      <td>N</td>
      <td>N</td>
      <td>N</td>
    </tr>
    <tr>
      <td>Earning to give</td>
      <td>Y</td>
      <td>Y</td>
      <td>Y</td>
    </tr>
    <tr>
      <td>Research/PhD</td>
      <td>N</td>
      <td>N</td>
      <td>N</td>
    </tr>
    <tr>
      <td>Work at some grant giver</td>
      <td>N</td>
      <td>N</td>
      <td>N</td>
    </tr>
    <tr>
      <td>non-academic  research</td>
      <td>N</td>
      <td>N</td>
      <td>N</td>
    </tr>
    <tr>
      <td>complementary roles</td>
      <td>M</td>
      <td>-</td>
      <td>N</td>
    </tr>
  </tbody>
</table>

<p>Considering [3], and [4], it appears that [4] does not seem to add
value in the above case. It appears that the claim is true but [4],
doens‚Äôt seem to add any value.</p>

<p><em>Am feeling different, I am ready to work on this late even, usually
it was like start and get over with. When I start I feel like this is
my thing, This is pandians turf! I feel like its my basketball, I know
the smell, I am ready, I feel like a guy who PUA‚Äôs every single day! #
not afraid!</em></p>

<blockquote>
  <p>So, what are the most neglected and solvable issues that have the
biggest stakes for long-term welfare?</p>
</blockquote>

<h3 id="current-view">Current View</h3>

<blockquote>
  <p>In the 1950s, the (large-scale production of nuclear weapons)[1]
meant that, (for the first time, a few world leaders gained the
ability to kill hundreds of millions of people)[2] ‚Äî and possibly
many more if they triggered (a nuclear winter)[2a], which would make it
nearly impossible to grow crops for several years)[3]. Since then,
the possibility of (runaway climate change)[4] has joined the list of
(catastrophic risks facing humanity)[5].</p>
</blockquote>

<p><strong>Claim</strong>: [1] meant [2].</p>

<p><strong>Question</strong>: Does [1] mean [2]?</p>

<p><strong>Example</strong>:</p>

<p>For [1], we think of the sudden rise in capacity of the US nuclear
weapons from <a href="https://en.wikipedia.org/wiki/Historical_nuclear_weapons_stockpiles_and_nuclear_tests_by_country">2 to 299 from 1945 to 1950</a>.</p>

<p>For [2]: <a href="https://www.un.org/press/en/2009/gadis3385.doc.htm">One nuclear weapon can destroy hundreds of thousands of
people in a ‚Äúmajor city‚Äù</a>. With 299 weapons the could kill up to 2
million people.</p>

<p><em>I am unable to find the number of people possible to wound with one
nuclear weapon in 1950‚Äôs.</em></p>

<p><strong>Claim</strong>: [2a] results in many more deaths</p>

<p><strong>Question</strong>: Does [2a]  result in many more deaths</p>

<p><strong>Example</strong>:</p>

<p>The smallest nuclear powers today have 50 Hiroshima-sized nuclear
weapons, which when used could cause the temperature of earth to rise
several degrees over large areas in North America and Eurasia, including most of
the grain-growing regions for more than a year. Resulting in famines
and starvation and possible death.</p>

<p><a href="https://www.sciencedaily.com/releases/2006/12/061211090729.htm">Source</a>.</p>

<p><em>Yes not many numbers are given but I am unable to go deeper, as this
already took an hr!</em></p>

<p><em>As far as I am reading there seem to be no nuclear winters in the past+</em></p>

<hr />

<p><strong>Claim</strong>: Since 1950 [4] has joined [5].</p>

<p><strong>Question</strong>: Since when did [4], join [5]?</p>

<p><strong>Example</strong>: By late 1960‚Äôs the number of scientific papers published
on the Climate Change leapt from roughly 3 to 20 papers
year. ‚Äî<a href="https://www.historyextra.com/period/modern/climate-change-warnings-history/">source</a></p>

<blockquote>
  <p>During the next century we may develop (new transformative
technologies, such as advanced artificial intelligence and
bioengineering)[1], that could bring about a (radically better future)[2] ‚Äî
but may also (pose grave risks)[3].</p>
</blockquote>

<p><strong>Claim</strong>: We may develop [1].</p>

<p><strong>Question</strong>: Are we going to develop [1]?</p>

<p><strong>Example</strong>: An AI system named AlphgGo, in the course of the year, it
became the best player in the world with a 60-win streak. Why this is
impressive(transformative) is because it is not possible to win the
game with brute force, but only with strategic intuition, which the AI
developed. ‚Äî <a href="https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/">Source</a></p>

<p><strong>Claim</strong>: [1] can cause [2].</p>

<p><strong>Question</strong>: Can [1], cause [2]? or How many people can AI save?</p>

<p><strong>Example</strong>: Assuming everything can be cured, and that it is just a
matter of time and understanding, AI could cure Cancer which killed
9.6 million people in 2017.</p>

<p><strong>Claim</strong>: [1] can cause [3].</p>

<p><strong>Question</strong>: Can [1], cause [3]?</p>

<p><strong>Example</strong>: In an autonomous assignment to reduce the incidence of
 Cancer, an AI decides to kill human beings before they can grow old
 to develop cancer as this was the only way to drive cancer rates to
 0.</p>

<blockquote>
  <p>Previously, we focused on improving (near-term global health)[1],
and we still think it‚Äôs (an important cause)[2]. However, over the
past eight years, we‚Äôve come to realise that (these technological
developments)[5] mean that the (actions of the present generation)[3]
may put the (entire future of civilization)[4] at stake.</p>
</blockquote>

<p><del>Claim: In the past 80khours focussed on [1].</del></p>

<p><strong>Claim</strong>: [1], is still [2].</p>

<p><strong>Question</strong>: Is [1], important?</p>

<p><strong>Example</strong>: Every year around 10 million people in poorer countries
die of curable illnesses that can be very cheaply prevented (100
<span>$</span> to 1000 <span>$</span> per year) including malaria and
HIV.</p>

<p><strong>Claim</strong>: [5] implies, [3] may result in [4].</p>

<p><strong>Question</strong>:</p>

<p><strong>Example</strong>: Technologies such as AI could produce grave issues, if
not enough effort is put into it (for example, in the control
problem). In the future if AI, is much more powerful and intelligent
than a human brain, it could choose to simply eliminate rival
intellects aka existential risk.</p>

<blockquote>
  <p>In combination with our (growing confidence in longtermism)[1], this
has persuaded us that the most important challenge of the next
century is likely to be to (reduce ‚Äòexistential risk‚Äô ‚Äî events
that would drastically damage the long-term future of humanity.)[2]</p>
</blockquote>

<p><strong>Claim</strong>: [2] is the most important challenge.</p>

<p><strong>Question</strong>: Is [2] the most important challenge?</p>

<p><strong>Example</strong>: AI could potentially wipe out the entire population and
the future generations, if not ‚Äúenough work‚Äù is done on the ‚Äòcontrol
problem‚Äô.</p>

<!-- 13 in 3 hrs [7] claims -->

<hr />

<blockquote>
  <p>There are several types of (existential risk)[1]. Currently, we‚Äôre
most concerned by the risk of (global catastrophes)[2] that might
lead to (billions of deaths)[3] and (threaten to permanently end
civilization)[4]. There are several reasons we think it‚Äôs
overwhelmingly (important to address these risks)[5].</p>
</blockquote>

<p><strong>Claim</strong>: There are several types of [1].</p>

<p><strong>Question</strong>: What are the several types of [1]?</p>

<p><strong>Example</strong>: <em>I don‚Äôt know what they are, can‚Äôt find something online
related this as well</em></p>

<p><strong>Claim</strong>: 80khours is currently, concerned by [2] that might lead to
[3] and [4].</p>

<p><strong>Question</strong>: Is 80k hours concerned by [2]? that might lead to [3]
and [4].</p>

<p><strong>Example</strong>: 80khours scores the issues due to climate change as
14/16, as compared to ‚Äúimproving health in poor countries‚Äù scored at 13/16.</p>

<p><del>Claim**: [2] might lead to [3] and [4].</del></p>

<p><strong>Claim</strong>: There are several reasons for [5].</p>

<p><strong>Question</strong>: Are there several reasons for [5]?</p>

<p><strong>Example</strong>: These are written below‚Ä¶</p>

<blockquote>
  <p>First, because of the (power of the new technologies)[1] noted above,
we think that the (probability of this kind of catastrophe occurring
in our lifetime)[2] is too big to ignore.</p>
</blockquote>

<p><strong>Claim</strong>: [2] is too big to ignore because of [1].</p>

<p><em>because</em></p>

<p><strong>Claim</strong>: [2] is too big to ignore.</p>

<p><strong>Question</strong>: Is [2], too big to ignore?</p>

<p><strong>Example</strong>:</p>

<p>For [2], we think of a <a href="https://en.wikipedia.org/wiki/Global_catastrophic_risk">5% chance</a> of a super intelligent AI ending
in human extinction i.e., a loss of billions of lives multiplied by 5%
resulting in expected loss of 70m people (not counting the future
generations)</p>

<blockquote>
  <p>Second, it seems like (such an event)[3] would be among the (worst
things that could happen)[4]. This is especially true if one takes a
(longtermist perspective)[5], because (extinction)[6] would also mean
the (loss of the potential welfare of all future generations)[7].</p>
</blockquote>

<p><strong>Claim</strong>: [3] would be [4].</p>

<p><strong>Question</strong>: Would [3] be [4]?</p>

<p><strong>Example</strong>: As seen in the previous example, a loss of 70m
(not taking into account the future generations) is expected.</p>

<p><strong>Claim</strong>: It is especially true if you take into account [5].</p>

<p><strong>Example</strong>: With a 5% chance and a 11 billion population, we already
have an expected loss of 110m people. If we consider future
generations, this number goes upwards only based on the expected
population and the time they have on earth. For example if we assume a
population of 20b by 2200 without the catastrophe, then we shall have
an expected loss of 9b and 110m people.</p>

<!-- 12 in 1 hr 20 mins adn 6 claims -->

<hr />

<blockquote>
  <p>Third, (some of these risks)[1] are highly neglected. For instance,
the fields of (AI safety and catastrophic biorisk)[0] receive the
attention of perhaps only 100 dedicated researchers and
policymakers, compared to the (billions or trillions of dollars)[2]
that (go into more familiar priorities)[3], such as (international
development)[4], (poverty relief in rich countries)[5], (education,
and technological development)[6]. This makes them perhaps more than
(a factor of 1000 more neglected)[7].</p>
</blockquote>

<p><strong>Claim</strong>: [1] are highly neglected.</p>

<p><strong>Example</strong>: AI safety receives the attention of 100 dedicated
researchers, for a probability of extinction of <a href="https://en.wikipedia.org/wiki/Global_catastrophic_risk">5% by 2100</a>.</p>

<p><strong>Claim</strong>: [2] is spent on [3].</p>

<p><strong>Question</strong>: The US spent 600 billion$ on military in 2015</p>

<p><strong>Claim</strong>: [2] is spent on [4]</p>

<p><strong>Example</strong>: US spends 5.7 billion in foreign aid for Afganistan</p>

<p><strong>Claim</strong>: [2] is spent on [5]</p>

<p><strong>Example</strong>: <a href="https://www.google.com/search?q=number+of+people+receiving+poverty+aid+in+US&amp;oq=number+of+people+receiving+poverty+aid+in+US&amp;aqs=chrome..69i57.9752j0j7&amp;sourceid=chrome&amp;ie=UTF-8">45m</a> people are in poverty receiving aid from US and
living in the US. <a href="http://federalsafetynet.com/poverty-and-spending-over-the-years.html">18k</a> <span>$</span>/person is being spent by the
US government which is in total 810 billion <span>$</span>.</p>

<p><strong>Claim</strong>: [2] is spent on [6].</p>

<p><strong>Example</strong>: ‚ÄúTotal expenditures for public elementary and secondary
schools in the United States in 2014‚Äì15 amounted to $668
billion‚Äù. ‚Äî <a href="https://www.google.com/search?ei=axIAXaK-Ho6ckgXiybHwAw&amp;q=us+spending+on+education&amp;oq=us+spending+on+education&amp;gs_l=psy-ab.3..0l10.52410.55401..55575...0.0..0.69.1181.24......0....1..gws-wiz.......0i71j35i39j0i67j0i20i263.b7rFg-obFDg">Source</a></p>

<p><strong>Claim</strong>: This makes [1] a factor of 1000 more neglected.</p>

<p><strong>Question</strong>: How neglected does this make [1]?</p>

<p><strong>Example</strong>: 10m <span>$</span> is spent on AI safety risk whereas 668b
<span>$</span>  spent on education. 668b/10m=66000</p>

<blockquote>
  <p>This neglect suggests that a (comparatively small number of
additional people working on these risks)[1] could significantly
reduce them. We suggest specific ways to help in the next section.</p>
</blockquote>

<p><strong>Example</strong>:</p>

<p>If we double the number of people working in AI safety, we can reduce
the risk by 1%. Which amounts to about 70 millions effective lives for
an extra 100 people.</p>

<blockquote>
  <p>This said, we remain (uncertain about this picture)[1]. (Many of the
‚Äòcrucial considerations‚Äô that led us to our current priorities)[2]
were only (recently identified and written about)[3]. We may yet
learn of (other ways to increase the probability of a positive
long-term future and reduce the chance of widespread future
suffering)[4], that seem (more promising to address than the
existential risks we currently focus on)[6].</p>
</blockquote>

<p><strong>Claim</strong>: 80khours is [1].</p>

<p><strong>Question</strong>: Is 80khours [1]?</p>

<p><strong>Example</strong>: 80khours <a href="https://80000hours.org/problem-profiles/global-priorities-research/">emphsizes on working on ‚Äúglobal priorities
research‚Äù</a>, to identify what we should work on AI safety more or
climate change more (for example).</p>

<p><strong>Claim</strong>: [2] were only [3].</p>

<p><strong>Question</strong>: Was [2], [3]?</p>

<p><strong>Example</strong>: 80khours wrote <a href="https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/#what-can-you-do-to-help">this article</a> on AI safety on April
2015.</p>

<p><strong>Claim</strong>: there may be [4] than [6].</p>

<p><strong>Example</strong>: It could turn out that focusing on green energy
excessively is the way to go to for the future rather than identifying
or predicting the temperature rise, as it will always have a ton of
uncertainty.</p>

<p><em>I can only give a hypothetical example here! right?</em></p>

<blockquote>
  <p>For these reasons, we also work to support those creating the (new
academic field of global priorities research)[1], which (draws on
economics, philosophy and other disciplines)[2] to work out (what‚Äôs most
crucial for the long-term future)[3].</p>
</blockquote>

<p><strong>Claim</strong>: 80khours supports GPR.</p>

<p><strong>Example</strong>: 80khours provides its research on which of the issues are
most relevant.</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>AI Safety</th>
      <th>Nuclear security</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>scale       (16)</td>
      <td>15</td>
      <td>15</td>
    </tr>
    <tr>
      <td>neglectedness(12)</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <td>solvability (8)</td>
      <td>4</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<p><strong>Claim</strong>: GPR ‚Äúdraws‚Äù on economics, philosophy and other disciplines
to determine [3].</p>

<p><strong>Split</strong>:</p>

<p>For GPR, we think of 80khours suggesting new people to work on AI
safety instead of Climate Change, which will result in saving many
more lives over the long-term-future.</p>

<p>For economics, we think of 80khours intention to look at <a href="https://80000hours.org/articles/problem-framework/#introducing-how-we-define-the-factors">factors like
scale, neglectedness and solvability</a>.</p>

<table>
  <thead>
    <tr>
      <th>factors</th>
      <th>AI Safety</th>
      <th>Climate Change</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>scale       (16)</td>
      <td>15</td>
      <td>14</td>
    </tr>
    <tr>
      <td>Neglectedness(12)</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Solvability (8)</td>
      <td>4</td>
      <td>4</td>
    </tr>
  </tbody>
</table>

<p>For Philosophy we think of, 80khours and their focus to look at
long-term-future instead of near-term-future as the former will save a
lot more number of people.</p>

<p>For other disciplines, we think of 80khours looking into Climate
Change and AI to understand the values of the different factors</p>

<p><strong>Example</strong>: 80khours suggests that working on AI safety is better
than working on Climate Change based on the fact that the
neglectedness is very high, and the amount of people going to die as a
result would be large.</p>

<hr />

<!-- We start work here 15 jun -->

<h1 id="200-starts-here">200 starts here!</h1>
<h2 id="work-starts-here-finish-40-50-phrases-today">Work starts here. Finish 40-50 phrases today</h2>

<blockquote>
  <p>In addition, we encourage people to (work on ‚Äòcapacity-building‚Äô
measures)[1] that will (help humanity manage future challenges, whatever
those turn out to be)[2]. These measures could involve (improving
institutional decision making and building the ‚Äòeffective altruism‚Äô
community.)[3]</p>
</blockquote>

<p><strong>Claim</strong>: It is good for people to [1] which will [2].</p>

<p><strong>Question</strong>: Is it good for people to [1] which will [2]?</p>

<p><strong>Split</strong>:</p>

<p>For [1], we think of <a href="https://uk.linkedin.com/in/nielbowerman">Niel Bowerman</a>, who is seen to work in different
organizations (CEA) in the role of fund-raising and growing organizations,</p>

<p>For [2] we think of, Niel Bowerman being able to jump right in and work
in addressing the talent gap for AI safety.</p>

<p>For good, we think of working in AI safety because it has the
potential to take down the entire world and yet has only 100 people
working on it.</p>

<!-- **97k per year** -->
<!-- (There is an estimate of [97k$ per year](https://80000hours.org/2016/08/reflections-from-a-givewell-employee/) in impact (or donations), -->
<!-- for someone working at GiveWell. We assume EAO's have similar impact.) -->

<!-- *I could give an example showing a statement where 80khours encourages -->
<!-- people as it's the "want" that is in question here. Does 80k want or -->
<!-- not. OR was I expected to just look at "it is good to work [1] that -->
<!-- will [2]?" Can you comment?* -->

<!-- [1] hr for [2] claims and [2] phrases -->

<hr />

<p><strong>Claim</strong>: [1] could involve [3].</p>

<p><strong>Question</strong>: Does [1] involve [3]?</p>

<p><strong>Split</strong>:</p>

<p>For [1], we think of Niel Bowerman working for 80khours in the role of
bringing in more people into AI safety</p>

<p>For building the EA community, we think of the same example as in [1].</p>

<p>For institutional decision making, <em>I don‚Äôt have an example</em></p>

<blockquote>
  <p>Some other issues we‚Äôve focused on in the past include (ending
factory farming)[1] and improving (health in poor
countries)[2]. They seem especially promising if you don‚Äôt think
(people can or should focus on the long-term effects of their
actions)[3].</p>
</blockquote>

<p><strong>Claim</strong>: 80khours focused on [1] and [2] in the past.</p>

<p><strong>Question</strong>: Has 80khours focused on [1] and [2] in the past?</p>

<p><strong>Example</strong>: 80khours wrote <a href="https://80000hours.org/problem-profiles/">articles</a> on supporting factory
farming and global poverty since 2009. But recently they call AI
safety and other existential risks as top-problems and not FF and GP.</p>

<p><strong>Claim</strong>: [1] seem promising if you don‚Äôt think [3].</p>

<p><strong>Question</strong>: Is [1], promising if you don‚Äôt think [3]?</p>

<p><strong>Example</strong>:</p>

<p>50 billion animals die each year. 1k people are working on
it. ‚ÄúExpected value with intense efforts for the future of humanity‚Äù
is <a href="https://80000hours.org/problem-profiles/factory-farming/#think-you-should-work-on-something-else">0.05% (average)</a>, i.e., <code class="language-plaintext highlighter-rouge">0.0005*7 billion human lives</code> i.e.,
3.5m expected human lives. Assuming that doubling the effort leads to
reducing the problem by 1%, we have,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3.5*E6 expected people lives * 1% / 1000 people
= 35 expected people lives in total
</code></pre></div></div>

<p>Contrast this, to working in Data Science at Google in the US, I
expect <a href="/deliberate-practice.html">400 lives to be saved</a>.</p>

<p>So, does not look promising!</p>

<p><strong>Claim</strong>: [2] seems promising if you don‚Äôt think of [3].</p>

<p><strong>Question</strong>: Is [2], promising if your don‚Äôt think [3]?</p>

<p><strong>Example</strong>:</p>

<p>If one works in GiveWell, he can probably have an impact of <a href="https://80000hours.org/2016/08/reflections-from-a-givewell-employee/">97k$ per
year</a>. This implies he can save <code class="language-plaintext highlighter-rouge">97k$/4k$*30=727</code> lives in total of
30 years. Contrast this to working in Data Science in Google in the
US, about <a href="/deliberate-practice.html">400 lives</a> can be saved over a 30 years.</p>

<!-- [4] phrases [3] claims [1] hr -->

<blockquote>
  <p>There are (many issues)[1] we haven‚Äôt been able to look into yet, so
we expect there are other (high-impact areas we haven‚Äôt
listed)[3]. We have a (list of candidates)[4] on our (problem
profile page)[5], and we‚Äôd be excited for (people to explore some of
these as well as other areas that could have a large effect on the
long-term future.)[6] (These areas)[6a] can be (particularly worth
pursuing)[7] if you‚Äôre (especially motivated by one of them)[8]. We
cover this more in the section on ‚Äòpersonal fit‚Äô below.</p>
</blockquote>

<p><strong>Claim</strong>: There are [1], that 80khours has not looked into yet.</p>

<p><strong>Question</strong>: Are there [1], that 80khours has not looked into yet?</p>

<p><strong>Example</strong>: <a href="https://80000hours.org/problem-profiles/">Criminal Justice Reform, medical research into how to
slow aging etc‚Ä¶</a></p>

<p><strong>Claim</strong>: There could be other [3].</p>

<p><strong>Question</strong>: Could there be other [3]?</p>

<p><em>In this case, I could give a hypothetical example or an example from
the past? can you help with what‚Äôs good here? and why?</em></p>

<p><strong>Example from Past</strong>: Until a few years back 80khours thought that the best
places to work on were ‚Äúreducing near-term life risks aka reducing
global health risks‚Äù but when they explored that there were global
catastrophic risks that could kill the entire planet and future
generations, they have now changed their stance on where people should
be working considering the impact.</p>

<p><strong>Example Hypothetical</strong>: If medical research into ‚Äòhow to slow aging‚Äô seems
largely promising (95% chance of making it with 10b <span>$</span>
with a 100 people extra), in delivering a mechanism that doubles the
human life expectancy, it could be beneficial to work on it as it
could save <code class="language-plaintext highlighter-rouge">95% * 7b expected people lives/100 = 66m expected people
lives per person working on it</code></p>

<p><strong>Claim</strong>: 80khours has [4] on [5].</p>

<p><strong>Example</strong>: They have ‚Äúindividual cognition‚Äù and many others in this
page: https://80000hours.org/problem-profiles/</p>

<p><strong>Claim</strong>: It‚Äôs a good idea for [6].</p>

<p><strong>Question</strong>: Is it a good idea for [6]?</p>

<p><strong>Example</strong>:</p>

<p>Working in DS gives an impact over 30 years of life of:</p>

<ul>
  <li>75% chance of working in US starting with 150k$ for 30 years
starting at 35 years</li>
  <li>growth of 5% average until 50 and then 2% average growth till 65</li>
  <li>10% increase every 5 years</li>
  <li>Donating 35% of salary</li>
</ul>

<p>Results in saving 530 people. <em>Previously I said 400, now I have an
updated calculation.</em></p>

<p>Instead if I get into ‚Äúpromoting effective altruism‚Äù and work on my
‚Äúpeople convincing skills‚Äù and convert only 10 people who would not have
donated to donate similar amounts as in a DS career, then it appears
that it could result in saving 5300 people. Of course this needs to be
multiplied by the probability of this actually happening which could
be as low as 10% to match the success of a career in DS saving 530
people.</p>

<p><strong>Claim</strong>: These areas can be [7], if you‚Äôre [8].</p>

<p><strong>Question</strong>: Is [6a], [7], if you are [8].</p>

<p><strong>Split</strong>:</p>

<p>For [6a] we think of, working in promoting EA as in the above example.</p>

<p>For [8], we think of a personal fit of more than 50%</p>

<p>For [7], we think of an impact of <code class="language-plaintext highlighter-rouge">5300*50%=2650 lives</code> which is
better than working a DS job resulting in 530 net people.</p>

<!--[8] phrases and [5] claims -->

<hr />

<h3 id="which-careers-effectively-contribute-to-solving-these-problems">Which careers effectively contribute to solving these problems</h3>

<blockquote>
  <p>The (most effective careers)[1] are those that address the (most pressing
bottlenecks to progress)[2] on (the most pressing global problems)[3].</p>
</blockquote>

<p><strong>Claim</strong>: [1] are those that address [2] on [3].</p>

<p><strong>Question</strong>: Is [1], [2] on [3]?</p>

<p><strong>Split</strong>:</p>

<p>For [1], we think of a career in AI safety, say as a computer science
researcher in MIRI, with an impact of 57k people(derived below) saved per additional
person. Contrast this to the 530 people to be saved over a career in
DS.</p>

<p><em>It seems to be making sense finally why an STM thought donating to
MIRI was better than donating to GiveWell.</em></p>

<p>For [2], we think of the control problem in AI</p>

<p>For [3], we think of AI safety</p>

<p><strong>Derivation for 57k</strong>
|                                          | AI safety | Climate Change |
|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî‚Äî-|
| Possible Deaths at the end of 2100       | 21b       | 20% x 21b      |
| % of chance  (middle of given range)     | 5.5%      | 5.25%          |
| people involved                          | 100       | 1000 (guess)   |
| Double effort =&gt; X% reduction in risk    | 1%        | 50%*           |
| Multiply everything above                | 57,750    | 55,125         |
| Money involved  (minimum) <span>$</span> | 10m       | 10b            |
| Dividing by above                        | 5.7E-3    | 5.5E-6         |</p>

<p>*here double effort is assumed to mean ‚Äúmajor effort‚Äù cited in their
article</p>

<blockquote>
  <p>For the (same reasons)[1] we think it‚Äôs an advantage to work on (neglected
problems)[2], we also think it‚Äôs an advantage to take (neglected
approaches to those problems)[3]. We discuss some of these approaches in
this section.</p>
</blockquote>

<p><strong>Claim</strong>: Due to [1], it is good to work on [2].</p>

<p><em>because</em></p>

<p><strong>Claim</strong>: It is advantageous to work on [2].</p>

<p><strong>Question</strong>: Is it advantageous to work on [2]?</p>

<p><strong>Example</strong>:</p>

<p>As shown above, we see that the lives saved per person per dollar is
much better for AI safety, aka a factor of 1000 better than working in
Cimate Change which is not ‚Äúso neglected‚Äù(aka, 10b <span>$</span> in funding)</p>

<p><strong>Claim</strong>: It is good to work on [3].</p>

<p><strong>Question</strong>: Why is it advantageous to work on [3]?</p>

<p><strong>Example</strong>:</p>

<p>MIRI sends out a mail on Christmas saying that they didn‚Äôt meet their
funding goals by a few 100k <span>$</span>. Let alone adding another 100 people to solve
the most important problem (complete anhilation by 2100 with a 1-10%
chance) by 1 more percent, what about keeping the people currently
involved and trying to grow the movement?</p>

<p>For [3], we think of ‚Äòadding more people‚Äô being the most neglected
approach as there are only 100 people working on it currently and
adding another 100 will only reduce the problem by 1%.</p>

<p>For advantageous, we think of 57k lives (as above) for every
additional person added to AI safety (on average).</p>

<!-- [5] phrases [3]-[4] claims -->

<h3 id="last-two-hours">Last two hours</h3>

<hr />

<blockquote>
  <p>Given our take on (the world‚Äôs most pressing problems)[1] and the
(most pressing bottlenecks these issues face)[2], we think the
following (five broad categories of career)[3] are a good place to
(start generating ideas)[4] if (you have the flexibility to consider a
new career path)[5].</p>
</blockquote>

<p><strong>Claim</strong>: Given [1] and [2], it appears that following [3] is a good
place for [4].</p>

<p><em>I am not sure how to give an example Given [1] and [2]. So I skip
this for now.</em></p>

<p><strong>Claim</strong>: It appears that following [3], is a good place for [4].</p>

<p><strong>Question</strong>: Is following [3], a good place for [4], if [5]?</p>

<p><strong>Split</strong>:</p>

<p>For [3], we think of a career in researching Climate Change</p>

<p>For [4], we think of Niel Bowerman meeting ‚ÄòGiving What we can‚Äô which
led him to go into Earning to Give in Finance, and then slowly
transitioning from there to FHI and then into AI policy with 80khours.</p>

<p>For [5], we think of Niel being able to move to finance for earning to
give.</p>

<p><strong>Example</strong>: Niel Bowerman started his career in researching climate
change where he realized that he should probably earn to give, and
moved into a career path in that direction and in the end landed with
AI policy at 80000 hours. As we have seen in the past AI safety¬†¬ª
Climate Change aka ‚Äúgood‚Äù.</p>

<h4 id="research">research</h4>

<blockquote>
  <p>(Many of the top problem areas we focus on)[1] are mainly (constrained by
a need for additional research)[2], and we‚Äôve argued that (research)[3] seems
like a high-impact path in general)[4].</p>
</blockquote>

<p><strong>Claim</strong>: [1] are mainly [2].</p>

<p><strong>Question</strong>: Is [1], mainly [2]?</p>

<p><strong>Example</strong>: There are 100 people working on AI safety, an additional
100 people will reduce the risk by 1%.</p>

<p><strong>Claim</strong>: [3] seems like [4].</p>

<p><strong>Example</strong>: Working in MIRI as a researcher could save 57k lives and
has a bang-for-the-buck as compared to Climate change (about 1000
times better).</p>

<p><em>I don‚Äôt know what general means so I skip it!</em></p>

<blockquote>
  <p>(Following this path)[8] usually means (pursuing graduate study in a
relevant area where you have good personal fit)[5], then aiming to do
(research relevant to a top problem area)[6], or else (supporting
other researchers who are doing this)[7].</p>
</blockquote>

<p><strong>Claim</strong>: [8] usually means, [5].</p>

<p><strong>Example</strong>: <a href="https://intelligence.org/team/">&gt;50% of them at MIRI</a> seem to have a graduate degree
or a PhD.</p>

<p><strong>Claim</strong>: [8] usually means [6] or [7].</p>

<p><strong>Example</strong>: For me top problem area is Climate change or AI
safety. None of the team of MIRI seem to have worked or done any
research in Climate change or AI safety before joining MIRI, or even
supporting them in some way.</p>

<p>I hereby confirm [8] doesn‚Äôt seem to mean [6] or [7].</p>

<!-- 13 in [2] hrs with [6] claims -->

<hr />

<!--16th jun Sunday-->

<blockquote>
  <p>(Research)[1] is the (most difficult to enter of the five
categories)[2], but it has (big potential upsides)[3], and in (some
disciplines)[4], going to (graduate school)[5] gives you (useful
career capital for the other four categories)[6]. This is one reason
why if (you might be a good fit for a research career)[7], it‚Äôs
often a good path to start with though we still usually (recommend
exploring other options for 1-2 years before starting a PhD)[8]
unless (you‚Äôre highly confident you want to spend your career doing
research in a particular area)[9]).</p>
</blockquote>

<p><strong>Claim</strong>: [1] is [2].</p>

<p><strong>Example</strong>: I am positive MIRI does not want me with my current skill
set. I probably need to work atleast 5 years (magic number), before I can come
to the level of their research. Whereas I could already earn-to-give
to MIRI as small as the amount may be.</p>

<p><strong>Claim</strong>: [1] has [3].</p>

<p><strong>Example</strong>: An additional worker in places like MIRI has an impact of
57k people. This is by far the highest I have ever seen in terms of
impact. If you look at earning to give for the most money making job I
know, aka Investment Banking, you could save <a href="http://agent18.github.io/Summary-before-applying-to-80k.html">3771 lives</a> at max (not
including the personal fit).</p>

<p><strong>Claim</strong>: In [4], going to [5], gives you [6].</p>

<p><strong>Example</strong>: Jesse Liptrap from MIRI, finished his PhD in Math and was
able to work as SWE in Google (allowing him to earn-to-give). He
currently works at MIRI.</p>

<p><strong>Claim</strong>: If [7], it might be good to start directly with [1].</p>

<p><strong>Split</strong>:</p>

<p>For [7], we think of Jesse Liptrap having atleast <a href="https://scholar.google.co.in/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=jesse+liptrap&amp;btnG=">3 papers</a> on his
name</p>

<p>For ‚Äòit might be good to [1]‚Äô, we think of Jesse having finished his PhD
being able to work in Google (with the possibility of earning to give)
and in the end still able to come back to research.</p>

<p><strong>Claim</strong>: It is better to do [8] unless [9].</p>

<p><em>I guess the point of 80k is: To explore and try other things before
joining PhD, as once you finish your PhD and leave academia to
explore, coming back is hard. I was unable to find real life examples
of ‚Äúhow hard it is‚Äù or who these people were.</em></p>

<blockquote>
  <p>After your (PhD)[1], it‚Äôs hard to (re-enter academia if you leave)[2], so at
this stage if (you‚Äôre still in doubt)[3] it‚Äôs often best to (continue
within academia)[4] (although this is less true in (certain disciplines,
like machine learning, where much of the most cutting-edge research
is done in industry)[5]). Eventually, however, it may well be best to do
(research in non-profits, corporations, governments and think tanks
instead of academia)[6], since (this can sometimes let you focus more on
the most practically relevant issues and might suit you better)[7].</p>
</blockquote>

<p><strong>Claim</strong>: After [1], its hard to [2]</p>

<p><em>Was not able to find an example online for someone who came back to
academia and how ‚Äúhard‚Äù it was for him</em></p>

<p><em>Skipped the whole para, it is taking a lot of time to find examples
(an hour or more)</em></p>

<p><strong>Claim</strong>: if [3], better to [4]</p>

<p><strong>Claim</strong>: if [3], better to [4], unless [5].</p>

<p><strong>Claim</strong>: It is better to work in [6], since [7]</p>

<p><strong>Claim</strong>: it is better to work in [6].</p>

<blockquote>
  <p>You can also (support the work of other researchers)[1] in a
(complementary role, such as a project manager, executive assistant,
fundraiser or operations)[2]. We‚Äôve argued (these roles)[3] are often
neglected, and therefore especially high-impact. It‚Äôs often useful
to have (graduate training in the relevant area)[4] before taking these
roles.</p>
</blockquote>

<p><strong>Claim</strong>: It is good to [1] in [2].</p>

<p><strong>Example</strong>: As discussed earlier, AI safety is really quite neglected
with 100 people working on it with 10m <span>$</span>. Neil Bowerman
from 80khours is trying to add people required to fill the ‚Äútalent
gaps‚Äù. If Neil is able to add 10 more people and even claim 1% of
their total impact that would be 570 lives saved just for his work in
a few years. Contrast that to a DS job which saves 400 people</p>

<p><em>I think it is important to contrast it with something otherwise it is
hard for someone to understand if it is good or bad. Agree: to always
contrast?</em></p>

<p><strong>Claim</strong>: [3] is often neglected</p>

<p><strong>Example</strong>: As of 2017 only 100 people are working. Adding another
hudred people would reduce the risk by only 1%. The risk associated is
a 5% chance of world extinction by 2100.</p>

<p><strong>Claim</strong>: [3] is high impact</p>

<p><strong>Example</strong>: As shown above 1 extra person in the field of AI can on
average save 57k people. If Neil is able to add 10 more people and
even claim 1% of their total impact that would be 570 lives saved
just for his work in a few years.</p>

<p><strong>Claim</strong>: [3] is neglected and hence it is high impact.</p>

<p><strong>Example</strong>: AI is neglected whereas Climate Change is not. A person
working in AI seems to have 1000 times more impact than a person
working for Climate Change.</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>AI safety</th>
      <th>Climate Change</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Possible Deaths at the end of 2100</td>
      <td>21b</td>
      <td>20% x 21b</td>
    </tr>
    <tr>
      <td>% of chance  (middle of given range)</td>
      <td>5.5%</td>
      <td>5.25%</td>
    </tr>
    <tr>
      <td>people involved</td>
      <td>100</td>
      <td>1000 (guess)</td>
    </tr>
    <tr>
      <td>Double effort =&gt; X% reduction in risk</td>
      <td>1%</td>
      <td>50%*</td>
    </tr>
    <tr>
      <td>Multiply everything above</td>
      <td>57,750</td>
      <td>55,125</td>
    </tr>
    <tr>
      <td>Money involved  (minimum) <span>$</span></td>
      <td>10m</td>
      <td>10b</td>
    </tr>
    <tr>
      <td>People saved per $ per person (Dividing by above)</td>
      <td>5.7E-3</td>
      <td>5.5E-6</td>
    </tr>
  </tbody>
</table>

<p><strong>Claim</strong>: It is useful to have [4] before [3].</p>

<p><strong>Split</strong>:</p>

<p>For [4] before [3]: Neil Bowerman has a PhD (equivalent) in Physics,
where he worked on existential risks of extreme climate change with a
focus on providing emission targets.</p>

<p>Also Sean O hEigeartaigh, from CSER has a PhD in Genome Evolution, he
is also known to increase the number of people at FHI and secure
rougly 3m <span>$</span> in funding. Now he is completely in
operations such as grantwriting, fundraising, long-term planning
etc‚Ä¶</p>

<p><em>Not sure how ‚Äúuseful‚Äù [4] is before [3]</em></p>

<blockquote>
  <p>(Some especially relevant areas to study)[1] include (not in order
and not an exhaustive list): (machine learning, neuroscience,
statistics, economics / international relations / security studies /
political science / public policy, synthetic biology /
bioengineering / genetic engineering, China studies, and decision
psychology)[2]. (See more on the question of what to study.)</p>
</blockquote>

<p><strong>Claim</strong>: [1] is [2].</p>

<p><em>Not sure how to satisfy the claim‚Äôs ‚Äúrelevance‚Äù with an example. I
can imagine how it looks though: A did Machine Learning PhD and it
helped because of X in top problem. I am unable to connect B and the top problem
with an example. aka the same inability to answer the previous claim‚Äôs
‚Äúusefulness‚Äù</em></p>

<!--15 roughly phrases and [10] claims lookied into with [3] hrs time   -->

<hr />

<h4 id="working-at-effective-non-profits">Working at effective non-profits</h4>

<blockquote>
  <p>Although we suspect (many non-profits)[1] don‚Äôt have (much impact)[2],
there are still (many great non-profits)[3] addressing (pressing global
issues)[4], and they‚Äôre sometimes constrained by a (lack of talent)[5], which
can make them a (high-impact option)[6].</p>
</blockquote>

<p><strong>Claim</strong>: [1] don‚Äôt have [2].</p>

<p><strong>Example</strong>: Many non-profits like Grameen Foundation fail to show
data of their success and in some cases such as the ‚ÄòVillage phone
program‚Äô seem to have been evaluated as having no impact on the
trading activity which it was supposed to boost.‚ÄîGiveWell</p>

<p><strong>Claim</strong>: [3] addresses [4].</p>

<p><strong>Example</strong>: MIRI addresses research regarding AI safety</p>

<p><strong>Claim</strong>: [3] constrained by [5].</p>

<p><strong>Example</strong>:</p>

<p>For [3], we think of MIRI.</p>

<p>For [5], we think of the Open Philanthropy project ready to pay a mean
value of 3m <span>$</span>, to add a person immediately to places like
MIRI, OpenAI. When the salary for a MIRI engineer would be 200k$
max I assume.</p>

<p><strong>Claim</strong>: [3] constrained by [5], is [6].</p>

<p><strong>Example</strong>: Every additional person added to AI safety(MIRI, OpenAI)
will have on average an impact of 57k lives.</p>

<blockquote>
  <p>One major advantage of (non-profits)[1] is that (they can tackle)[1a] the
(issues that get most neglected by other actors)[2], such as
(addressing market failures)[3], (carrying out research that doesn‚Äôt
earn academic prestige)[4], or doing (political advocacy on behalf
of disempowered groups such as animals or future generations)[5].</p>
</blockquote>

<p><strong>Claim</strong>: [1] can tackle [2] such as [4].</p>

<p><strong>Split</strong>:</p>

<p>For [1], we think of GiveWell</p>

<p>For [2], we think of not knowing where to donate our money as we have
no idea of the effectiveness of the charity.</p>

<p>For [4], we think of <a href="https://blog.givewell.org/2009/12/28/celebrated-charities-that-we-dont-recommend/">a post by GiveWell</a>, where they tear down
some of the popular non-profits like Grameen and expose how much they
suck.</p>

<p>For [1a], we think of GiveWell being able to move 110m <span>$</span>
in 2015 to organizations it deemed effective.</p>

<p><strong>Claim</strong>: [1] can tackle [2], such as [3].</p>

<p><strong>Split</strong>:</p>

<p>For [1], we think of 80khours</p>

<p>For [2], we think of AI Safety with only 100 people working on it for
a <a href="https://en.wikipedia.org/wiki/Global_catastrophic_risk">5% chance of human extinction by the end of this century</a>.</p>

<p>For [3], we think of 80khours addressing lack of people in AI safety
with Neil Bowerman.</p>

<p>For [1a], we think of 80khours deploying Neil Bowerman to identify and
fill up the talent gaps and create talent pipelines to ensure there
are more people working on AI safety.</p>

<p><strong>Claim</strong>: [1] can tackle [2], such as [5].</p>

<p><strong>Split</strong>:</p>

<p>For [1], we think of Animal Equality</p>

<p>For [2], we think of 50 billion animals being killed every year ‚Äúmost
of them‚Äù experience ‚Äúextreme levels of suffering‚Äù (like castration
without anesthesia or antibiotics)‚Äî<a href="https://80000hours.org/problem-profiles/factory-farming/">Source</a></p>

<p>For [5], We think of Animal Equality advocating for animal rights for
animals in US, India etc..</p>

<p>For [1a], we think of Animal Equality <a href="https://animalcharityevaluators.org/charity-review/animal-equality/">saving 3k to 8k</a> animals for
every 1k <span>$</span> of donations.</p>

<!-- end of day [2]. jun16 -->

<hr />

<!-- Start of day [4] jun 18 -->

<blockquote>
  <p>(To focus on this category)[0], start by making a list of (non-profits)[1]
that address (the top problem areas)[2], (have a large scale solution to
that problem)[3], and (are well run)[4]. Then, (consider any job where you
might have great personal fit)[5].</p>
</blockquote>

<p><strong>Claim</strong>: Make a list of [1] with [2], [3] and [4]; and then do [5]
for [0].</p>

<p>For list of [1], with [2], [3], [4], we think of</p>

<ul>
  <li>
    <p>MIRI working on AI safety, is working on solving the control
problem with research, and has <a href="https://intelligence.org/2018/11/26/miris-2018-fundraiser/">enough funding for this year</a> for
their 15 staff members</p>
  </li>
  <li>
    <p>80khours works on Global Priorities Research, they provide research
for all people to read to help them make ‚Äúgood choices‚Äù in their
career, and has enough funding for this year.</p>
  </li>
</ul>

<p>For 5, we think of:</p>

<ul>
  <li>Let‚Äôs say I have a personal fit of 1% for MIRI and 1% for 80khours.</li>
</ul>

<p>For 0, we think of maximum value of [personal fit multiplied by impact]:</p>

<ul>
  <li>
    <p>Every additional person to MIRI has an impact of 57k people. With a
1% personal fit, I would be at 570 people saved.</p>
  </li>
  <li>
    <p>By working in 80khours similar to the position of Neil Bowerman, if
I add 50 people to AI safety and assume a 1% impact from them, and
a personal fit of 1%, we have <code class="language-plaintext highlighter-rouge">57000*50*1%*1%=285 people</code></p>
  </li>
</ul>

<p>Just looking at personal fit seems to be not enough, we should also
look at impact multiplied by it.</p>

<blockquote>
  <p>The (top non-profits in an area)[5] are often (very difficult to enter)[6],
but you can always (expand your search to consider a wider range of
organizations)[7]. (These roles)[8] also cover a (wide variety of skills,
including outreach, management, operations, research, and others.)[9]</p>
</blockquote>

<p><strong>Claim</strong>: [5] is often [6].</p>

<p><strong>Example</strong>: If you look at the people working in MIRI, a research
fellow is expected to have published research in computer science,
logic and mathematics. This is extremely hard for me due to my lack of
background, and it sounds like 5 years of full time work before I reach
that level.</p>

<p><strong>Claim</strong>: [7] is a solution to [5] being [6].</p>

<p><strong>Example</strong>: It looks like <a href="https://80000hours.org/job-board/?role=finance-analyst___reckY4tLfT2UY1SFW">80khours means to look at non-top
non-profits such as those working on ‚Äòhealth in poor countries‚Äô or
‚Äòanimal rights‚Äô</a>. I would imagine this should take less than 5
years of part time work, to get into GiveWell.</p>

<p><strong>Claim</strong>: [8] covers [9].</p>

<p><strong>Example</strong>: Working at GiveWell would mean doing research on
effectiveness of interventions and writing blogs.</p>

<blockquote>
  <p>We list some (organizations to consider)[10] on (our job board)[11], which
includes (some top picks)[12] as well as (an expanded list at the
bottom)[13]. Read more about working at effective non-profits in our full
career review (which is unfortunately somewhat out of date).</p>
</blockquote>

<p><strong>Claim</strong>: [10] is listed in [11].</p>

<p><strong>Example</strong>: MIRI is on the <a href="https://80000hours.org/job-board/ai-ml-safety-research/">job board</a>.</p>

<p><strong>Claim</strong>:  [11] include [12] as well as [13].</p>

<p><strong>Example</strong>: Job board includes MIRI, as well as GiveDirectly.</p>

<h4 id="apply-an-unusual-strength-to-a-needed-niche">Apply an unusual strength to a needed niche</h4>

<blockquote>
  <p>If you already have a strong existing skill set, is there a way to
apply that to one of the key problems?</p>
</blockquote>

<blockquote>
  <p>If (there‚Äôs any option)[13] in which you (might excel)[14], it‚Äôs usually worth
considering, both for the (potential impact)[15] and especially for the
(career capital)[16]; (excellence in one field)[17] can often give you
(opportunities in others)[18].</p>
</blockquote>

<p><strong>Claim</strong>: if [13] in which you [14], it is worth considering for
[15].</p>

<p><strong>Example</strong>:</p>

<p>If Messi (soccer player worth 400m <span>$</span>) works in ML and say
somehow joins MIRI, he can save 57k people. If Messi instead donates
5m <span>$</span> and satisfies MIRI‚Äôs budget, he is essentially
sponsoring 15 people who have on average 57k people impact. If Messi
assumes a 20% of the total impact of MIRI, this comes to about
<code class="language-plaintext highlighter-rouge">15*57000*0.2=171k</code> people.</p>

<p><strong>Claim</strong>:</p>

<blockquote>
  <p>This is even more likely if you‚Äôre (part of a community that‚Äôs
coordinating or working in a small field)[119]. (Communities)[20] tend to need a
(small number of experts)[21] covering each of their (main bases)[22].</p>
</blockquote>

<p><em>I gave up at this point! too painful, barely going forward!
Quantifying impacts and giving examples is quite slow and really hard
(1 claim per 45mins). So I stop here.</em></p>

<!-- end of day [4] jun 18 part 1 -->

<hr />

<h2 id="ai">AI</h2>

<hr />

<!-- beginning of jun 19 -->

<blockquote>
  <p>There is no doubting the (force of the arguments)[1] the problem is a
(research challenge worthy of the next generation‚Äôs best mathematical
talent)[2]. (Human civilization)[3] is at stake.</p>
</blockquote>

<p><strong>Claim</strong>: There is no doubting [1].</p>

<p><strong>Question</strong>: Why is there no doubting [1]?</p>

<p><strong>Split</strong>: For [1], we think of, 5% chance for human extinction due to
AI by 2100.</p>

<p><strong>Example</strong>: The fate of Gorillas currently depends on the actions of
humans. Similarly the fate of humanity may come to depend more on the
actions of machines than our own.</p>

<p><em>This is reasoning and not an example I think, your thoughts? or I
should just give a hypothetical example?</em></p>

<p>Imagine Russia has an autonomous weapon system, that works without
human intervention. If the weapon detects a threat it is going to
engage and bomb the hell out of who ever it thinks did this. If the AI
makes a mistake at any time, it still continues to bomb the hell out
of who ever it thinks did it, resulting in war.</p>

<p><strong>Claim</strong>: Problem is [2].</p>

<p><strong>Example</strong>: MIRI was founded in 2000. And in 2017 80khours says that
adding another 100 people will only solve 1% of the problem.</p>

<p><strong>Claim</strong>: [3] is at stake.</p>

<p><strong>Example</strong>:</p>

<p>The fate of Gorillas currently depends on the actions of humans. They
are currently endangered. Similarly the fate of humanity may come to
depend on the actions of machines than our own.</p>

<blockquote>
  <p>Around 1800, (civilization)[4] underwent (one of the most profound shifts
in human history: the industrial revolution)[5].</p>
</blockquote>

<p><strong>Claim</strong>: Around 1800, [4], underwent [5].</p>

<p><strong>Example</strong>: Around 1800, inventions such as the steam engine fueled
transportation using horses or a boat went on to railroads, steam
boats and automobiles.</p>

<blockquote>
  <p>(This)[6] wasn‚Äôt the (first such event)[7] ‚Äì (the agricultural
revolution)[] had upended (human lives 12,000 years earlier)[].</p>
</blockquote>

<p><strong>Claim</strong>: [6] wasn‚Äôt [7].</p>

<p><strong>Example</strong>: The agricultural revolution 12000 years earlier, allowed
humans to produce enough food for themselves. This shows up only in the
1700s with the population rise from 5.5m to 9 million in Britain. It
does not show up earlier due to diseases ad warfare apparently.</p>

<blockquote>
  <p>(A growing number of experts)[8] believe that (a third revolution will
occur during the 21st century, through the invention of machines
with intelligence which far surpasses our own)[9]. These range from
(Stephen Hawking to Stuart Russell, the author of the best-selling AI
textbook, AI: A Modern Approach)[10].</p>
</blockquote>

<p><strong>Claim</strong>: [8] believe [9].</p>

<p><strong>Example</strong>: Stephen Hawking says here that ‚Äúfull development of an
AI‚Äù will spell the end of the world.</p>

<p><em>i guess this is not an example!</em></p>

<p><strong>Claim</strong>: [10] are part of [8].</p>

<p><strong>Example</strong>: An Open letter was signed by Stephen hawking, Stuart
Russel and many others in 2015 stating concerns over the issues with
AI.</p>

<blockquote>
  <p>(Rapid progress in machine learning)[1] has (raised the prospect that
algorithms will one day be able to do most or all of the mental
tasks currently performed by humans)[2]. (This)[3] could ultimately lead to
(machines that are much better at these tasks than humans)[4].</p>
</blockquote>

<p><strong>Claim</strong>: [1] has [2].</p>

<p><strong>Example</strong>: In 2000, ‚Äúroomba‚Äù could autonomously vacuum the floor by
avoiding obstacles. Today, AlphaGo AI, can beat the greatest Go
players with just a year of learning.</p>

<p><strong>Claim</strong>: [3]/[1] could lead to [4].</p>

<p><strong>Example</strong>: Today, AlphaGo AI, can beat the greatest Go players with
just a year of learning.</p>

<blockquote>
  <p>(These advances)[5] could lead to (extremely positive developments,
presenting solutions to now-intractable global problems)[6], but they
also pose (severe risks)[7]. (Humanity‚Äôs superior intelligence)[8] is
pretty much the sole reason that (it is the dominant species on the
planet)[9]. If (machines surpass humans in intelligence)[10], then just
as the fate of gorillas currently depends on the actions of humans,
the (fate of humanity may come to depend more on the actions of
machines than our own)[1].</p>
</blockquote>

<p><strong>Claim</strong>: [5]/[3]/[1] could lead to [6].</p>

<p><strong>Example</strong>:</p>

<p>AlphaGo, identified superior ways of playing GO which were previously
considered rubbish by humans for thousands of years. Computers seem
like they can go beyond what humans can see with years and years of
work, within just a year. Similarly, it could be possible to cure
cancer and other diseases.</p>

<p><em>how to do I give an example for ‚Äúcould lead to‚Äù? I don‚Äôt think I have
given one above!‚Äù</em></p>

<p><strong>Claim</strong>: [5] also poses [7].</p>

<p><strong>Example</strong>: With making of autonomous Weapons or autonomous combat
Bots, the risk of cyber attack by an adversary or malfunction, could
result in attack on people or escalate conflicts by killing the
unintended.</p>

<p><strong>Claim</strong>: [8] is [9].</p>

<p><strong>Example</strong>: Humans are capable of making tools like spheres to be
able to protect themselves from large predators, whilst traveling
always as a group of people. Whereas Zebras even though they travel in
large packs, have no way of resisting a few lions targeting 100
zebras. There will be casualty.</p>

<p><strong>Claim</strong>: if [10] then [1].</p>

<p><em>I have no idea how to answer this claim, how do I give an example
that will inform if A then B.</em></p>

<hr />

<!-- 20 phrases [13] claims [3] hrs!  -->

<blockquote>
  <p>For a (technical explanation of the risks from the perspective of
computer scientists)[1a], see these papers (concrete problems in AI,
long-term challenges ensuring the safety of AI)[2].</p>
</blockquote>

<p><strong>Claim</strong>: [1a] is found in [2].</p>

<p><strong>Example</strong>:</p>

<p>For [1a] in [2] we think of, ‚ÄúImagine that an agent discovers a buffer
overflow in its reward function: it may then use this to get extremely
high reward in an unintended way. From the agent‚Äôs point of view, this
is not a bug, but simply how the environment works, and is thus a
valid strategy like any other for achieving reward. For example, if
our cleaning robot is set up to earn reward for not seeing any messes,
it might simply close its eyes rather than ever cleaning anything
up. Or if the robot is rewarded for cleaning messes, it may
intentionally create work so it can earn more reward.‚Äù‚Äî<a href="http://web.archive.org/web/20170406223825/https://arxiv.org/pdf/1606.06565v2.pdf">Source</a></p>

<blockquote>
  <p>(This)[3] might be the (most important transition of the next century)[4] ‚Äì
either ushering in an (unprecedented era of wealth and progress)[5], or
(heralding disaster)[6]. But it‚Äôs also (an area that‚Äôs highly neglected)[7]:
while (billions)[8] are (spent making AI more powerful)[8a], we estimate (fewer
than 100 people)[9] in the world are working on (how to make AI safe)[10].</p>
</blockquote>

<p><strong>Claim</strong>: [3] might be [4] either going into [5] or [6].</p>

<p><strong>Split</strong>:</p>

<p>For [3], we think of a world where machines are more intelligent than
human beings.</p>

<p><strong>Example</strong>:</p>

<p>There is a chance that we could go into extinction (for example, as a
result of autonomous warbots being compromised leading into war) and
if not it could be curing diseases/problems like Cancer or global
warming. The outcomes are on both extremes.</p>

<p><strong>Claim</strong>: AI safety is [7].</p>

<p><strong>Example</strong>: Only 100 people are working with 10m <span>$</span> in
funding. Contrast this to the funding obtained by one single
organization working on curing Malaria: AMF for this year got 40m
<span>$</span>.</p>

<p><strong>Claim</strong>: [8] is spent on [8a]</p>

<p><strong>Example</strong>: It appears that billions of dollars are going to be spent
on making virtual assistants, chatbots, recognize images, process
human speech, identify anamolies in CT scans, identify cracks in jet
engine blades etc‚Ä¶ NOT ON AI SAFETY.‚Äî<a href="http://web.archive.org/web/20190427051757/https://blogs.wsj.com/cio/2017/01/11/artificial-intelligence-looms-larger-in-the-corporate-world/">source</a></p>

<p><strong>Claim</strong>: [9] working on [10].</p>

<p><strong>Example</strong>: There seem to be <a href="https://medium.com/datadriveninvestor/12-organizations-saving-humanity-from-the-dark-side-of-ai-bce8c9da1ea5">12 organizations</a> working on the
problem of AI safety. All seem to be non-profits of a small scale so I
would imagine 15 people max per organization roughly amounts to 180
people (approximately in the ballpark).</p>

<blockquote>
  <p>(This problem)[1] is an (unusual one)[2], and it took us a (long
time)[3] to (really understand it)[4]. Does it (sound weird)[5]?
Definitely. When (we first encountered these ideas in 2009)[5a] we
(were skeptical)[6]. But (like many others)[7], (the more we read the
more concerned we became)[8]. We‚Äôve also come to believe the
(technical challenge)[9] can probably be (overcome if humanity puts
in the effort)[10].</p>
</blockquote>

<p><strong>Claim</strong>: [1] is [2].</p>

<p><strong>Example</strong>: I have never heard about this on the news/media like
Climate Change. I didn‚Äôt even look up twice despite an STM donating 4k
<span>$</span>..</p>

<p><strong>Claim</strong>: It took 80khours [3] to [4].</p>

<p><strong>Example</strong>: 80khours seems to have articles related to improving
global poverty since 2011, but regarding AI articles are made only
since 2017 despite encountering it in 2009.</p>

<p><strong>Claim</strong>: It sounds weird.</p>

<p><strong>Example</strong>: <em>No idea how to answer this, probably not important as well.</em></p>

<p><strong>Claim</strong>: When [5a] we [6].</p>

<p><strong>Example</strong>: 80khours didn‚Äôt bother to publish an article until 2017
from 2009 to 2017 on AI safety</p>

<p><strong>Claim</strong>: Many others understood the risks by [8].</p>

<p><strong>Example</strong>:</p>

<p>I have seen the TED talk before in 2017 December, only now am I truly
warming up to AI safety (possibly because I made the world class
assumption that all EAO‚Äôs have exactly the same impact as GiveWell). I
never really saw the need to give money to MIRI until a week
back. Recently I started off with the ‚Äúkey ideas post‚Äù by 80khours and
started taking apart the phrases when I realized the impact (57k
people per extra person working on it). Additionally it helped to see
several scientists giving a voice for AI safety
<a href="https://web.archive.org/web/20170311011724/https://futureoflife.org/ai-open-letter">here</a>. Furthermore, it helped to make concrete the risks such as
with the autonomous weapons potential to destabilize nations.</p>

<p><strong>Claim</strong>: [9] can be [10].</p>

<p><strong>Example</strong>: <a href="https://news.un.org/en/story/2019/03/1035381">Here</a>, UN has requested a ban on development of
autonomous weapons. If all countries come to an agreement on this, it
could potentially save us from extinction as a result of autonomous
weapons.</p>

<blockquote>
  <p>(Working on a newly recognized problem)[1] means that (you risk throwing
yourself at an issue that never materializes)[2] or (is solved easily)[3] ‚Äì
but (it)[] also means that you may have a (bigger impact by pioneering an
area others have yet to properly appreciate)[4], just like (many of the
highest impact people in history have done)[5].</p>
</blockquote>

<p><strong>Claim</strong>: [1] means [2].</p>

<p><strong>Example</strong>: ‚ÄúEarlier this year, the U.S. defense think-tank Rand
Corporation warned in a study that the use of AI in military
applications could give rise to a nuclear war by 2040.‚Äù‚Äî<a href="https://www.cnbc.com/2018/08/01/five-of-the-scariest-predictions-for-ai.html">Source</a></p>

<p>Seems like the claim could be wrong.</p>

<p><strong>Claim</strong>: [1] means [3].</p>

<p><strong>Example</strong>: <em>I am not sure what they are getting at, they are
basically suggesting all possible scenarios aka, you will see AI
materialize or you wont see it materialize! Sounds useless to me.</em></p>

<p><strong>Claim</strong>: You may have [4].</p>

<p><strong>Example</strong>: There are only 100 people working in AI safety with a
calculated 57k people to be saved if one additional person works on
it, on average.</p>

<p><strong>Claim</strong>: you may have [4], just like [5].</p>

<p><strong>Split</strong>:</p>

<p>For [4], we think of working in AI safety and saving 57k people.</p>

<p>For [5], Gandhiji, seems to have brought India Independence, by
pioneering in the are of Non-violence, which others were yet to
properly appreciate. (unable to estimate the impact aka, number of
lives saved)</p>

<h2 id="tio-summary">TIO Summary</h2>

<!-- start of 17 june! -->

<blockquote>
  <p>(Talent)[1] we imagine is something that (people)[2] are born
with. (Talent)[3] certainly seems to be (overrated)[4] especially when (it
refuses to show itself even after many many years into the lives of
exceptional musicians.)[5a]</p>
</blockquote>

<p><strong>Claim</strong>: [2] is born with [1] to become GREAT.</p>

<p><strong>Question</strong>: Is [2], born with [1] to become GREAT?</p>

<p><strong>Example</strong>: Jerry Rice, known as the greatest receiver in history‚Äî
whose stats in total touchdown receptions are 50% higher than the
runner up‚Äîwas signed to the San Francisco 49ers after 15 teams
passed him over.</p>

<p>Claim appears to be false.</p>

<p><strong>Claim</strong>: [3] is [4] since [5].</p>

<p><strong>Example</strong>: Jerry Rice, known as the greatest receiver in history‚Äî
whose stats in total touchdown receptions are 50% higher than the
runner up‚Äîwas signed to the San Francisco 49ers after 15 teams
passed him over. AKA, It doens‚Äôt look like his ‚Äúin-born-talent‚Äù didn‚Äôt want to show
itself 15-20 years later.</p>

<blockquote>
  <p>In a study of outstanding American pianists, for example, you could
not have predicted their eventual high level of achievement even
after they‚Äôd been training intensively for six years;</p>
</blockquote>

<blockquote>
  <p>A standard argument that comes at any such (number of studies)[1]
presented is, (‚ÄúBut what about Mozart, and what about Tiger Woods?‚Äù)[2]</p>
</blockquote>

<p><strong>Claim</strong>: People say [2] when [1] is presented.</p>

<p><strong>Example</strong>: <em>I am unable to provide an example for this</em></p>

<blockquote>
  <p>There seems to be an (explanation)[5] for these so called
(anomalies)[6]. In both the case of (Mozart and Tiger woods their
fathers)[7] seem to be starting them off (quite early in their
lives)[8] and have spent quite some time building the (skill into
their children)[9]. In the case of (Mozart his father)[10] was a
(highly accomplished pedagogue)[11] and in the case of (Tiger Woods,
his father)[12] played (golf quite well)[13] and was (extremely
passionate about it)[14] and (was also a teacher)[15].</p>
</blockquote>

<p><strong>Claim</strong>: There is a [5], for [6].</p>

<p><strong>Example</strong>:</p>

<p>For [5], we think of Tiger‚Äôs father who was top 10% of gold players
himself and was a teacher and dedicated his life to teach Tiger Woods
from the age of 7 months.</p>

<p>For [6], we think of Tiger Woods with the most number of PGA tour wins
(and still playing), where as 99% of people who gold don‚Äôt even play
professionally, let alone win a title.</p>

<p><strong>Claim</strong>: [7] started their children at [8].</p>

<p><strong>Example</strong>: Tiger‚Äôs father started him off at 7 months.</p>

<p><strong>Claim</strong>: [7] have spent quite some time building [9]</p>

<p><strong>Example</strong>: Tiger‚Äôs father started Tiger off with a metal club and a
putter at 7 months. By the age of 2 they are at the golf course
playing and practicing regularly. By age 4, he is learning from a
professional coach.</p>

<p><strong>Claim</strong>: [10] was [11]</p>

<p><strong>Example</strong>: Wolfgang‚Äôs father wrote a book on violin instruction that
remained influential for decades. <em>I don‚Äôt thing this is a good enough
example.</em></p>

<p><strong>Claim</strong>: [12] played [13].</p>

<p><strong>Example</strong>: Tiger Woods father was among the top 10% of the players
with a couple of years of starting it.</p>

<p><strong>Claim</strong>: [12] was [14].</p>

<p><strong>Example</strong>: Tiger Woods father was among the top 10% of the players
with a couple of years of starting it. He wanted to teach his son asap.</p>

<p><strong>Claim</strong>: [12] was [15].</p>

<p><strong>Example</strong>: He coached Little League teams and took them to state
tournaments in baseball.</p>

<!-- 18 and [10] claims [1].[5] hrs-->

<blockquote>
  <p>(The question about talent)[1] is answered (in the fact that
Mozart‚Äôs first piece regarded today as a masterpiece was composed
when he was 21.)[2] Although it is (an early age)[3], it must be
taken into account that (the boy)[4] has been in preparation since
(very very young)[5]. In an attempt to compare (how Mozart fares
with his current contemporaries)[5a], Scientists created a ‚Äò(precocity
index)[6]‚Äô. This roughly measures (how much better someone is
compared to the average)[7]. (Mozart)[8] scored a (130 percent
on the precocity index)[9] whereas (his current contemporaries)[10]
scored (thirty to five-hundred percent)[10a]. (This)[11] is probably
due to the (improved methods in teaching and learning)[12].</p>
</blockquote>

<p><strong>Claim</strong>: Talent is inborn due to 2.</p>

<p><em>because</em></p>

<p><strong>Claim</strong>: 21 is [3].</p>

<p><strong>Example</strong>:</p>

<p>‚ÄúGeorge Grove, the founding editor of ‚ÄúGrove‚Äôs Dictionary of Music and
Musicians‚Äù has called Mendelssohn‚Äôs ‚ÄúMidsummer Night‚Äôs Dream‚Äù
overture, Op. 21 ‚Äúthe greatest marvel of early maturity that the world
has ever seen in music.‚Äù This work was completed by Mendelssohn on
August 6, 1826 when Mendelssohn was 17 years and 6 months old.‚Äù‚Äî<a href="http://themusicalvoice.net/?p=408">Source</a></p>

<p>For people known as GREATs‚Äô, 21 doesn‚Äôt seem to be very early.</p>

<p><strong>Claim</strong>: [4] has been in preparation since [5].</p>

<p><strong>Example</strong>: Mozart‚Äôs dad started him on a program of intensive
training at the age of three.</p>

<p><strong>Claim</strong>: Scientists created [6] for [5a]?</p>

<p><strong>Example</strong>: It looks <a href="https://scholar.google.co.in/scholar?q=precocity+index&amp;hl=en&amp;as_sdt=0%2C5&amp;as_ylo=1980&amp;as_yhi=2000">here</a>, that precocity index is used much
before the paper that is <a href="https://scholar.google.com/scholar?cluster=7258691968061327584&amp;hl=en&amp;oi=scholarr">cited above</a>, about the precocity index
of musicians.</p>

<p>It seems like it was not created for comparing Mozart to his
contemporaries.</p>

<p><strong>Claim</strong>: [6] measures [7], ‚Äúroughly‚Äù.</p>

<p><strong>Example</strong>: Mozart has a precocity index of 130%, which is nothing
but based on a ‚Äúsimple formula‚Äù:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-X/(Y-X)
</code></pre></div></div>

<p>X- 	Number of years of preparation before publicly playing a piece for
average person
Y- 	Number of years of preparation before publicly playing a piece for Mozart</p>

<p><strong>Claim</strong>: [8] scored [9].</p>

<p><strong>Example</strong>: <em>All this probably requires is a citation?</em></p>

<p><strong>Claim</strong>: [10] scored [10a]</p>

<p><strong>Example</strong>: <em>All this probably requires is a citation?</em></p>

<p><strong>Claim</strong>: [11] is probably due to [12].</p>

<p><em>because</em></p>

<blockquote>
  <p>In Tiger‚Äôs case (his father)[13] never really claimed any (inborn
talent)[14], but he thought that the (boy seemed to grasp things)[15] rather
quickly. And (both of them)[1] state (Hard Work for the Success of
Tiger)[2].</p>
</blockquote>

<p><strong>Claim</strong>: [13] never claimed [14].</p>

<p><strong>Example</strong>: A quick google search of ‚Äúinborn talent tiger woods‚Äù does
not come up with any news articles or media where [13] states [14].</p>

<p><strong>Claim</strong>: [15] was rather quick.</p>

<p><em>I don‚Äôt know how I can find an example for that.</em></p>

<p><strong>Claim</strong>: [1] state [2].</p>

<p><strong>Example</strong>:</p>

<p>‚ÄúPeople don‚Äôt understand that when I grew up, I was never the most
talented. I was never the biggest. I was never the fastest. I
certainly was never the strongest. The only thing I had was my work
ethic, and that‚Äôs been what has gotten me this far.‚Äî<a href="https://www.brainyquote.com/quotes/tiger_woods_465111">Tiger
Woods</a>‚Äù</p>

<!-- 17 and [11] claims in [1].[5] hrs -->

<blockquote>
  <p>If you look at (Jack Welsh, CEO General Electric)[1], one of the
(twentieth century‚Äôs manager of the century)[2] apparently showed no
(inclination towards business until his mid-twenties)[3]. He started
working in (chemical development operation at GE around that
time)[3a]. And until that point there seems to be (nothing)[4] indicating the
(business tycoon that he was going to become)[5]. Talent Waar ben jij?</p>
</blockquote>

<p><strong>Claim</strong>: [1] is [2].</p>

<p><strong>Example</strong>:</p>

<p>‚ÄúJack Welch is a celebrated, legendary CEO. In his two decades at the
helm of General Electric, he grew revenues to $130 billion from $25
billion and profit to $15 billion from $1.5 billion.‚Äù‚Äî<a href="https://www.cnbc.com/2017/11/17/former-ge-ceo-jack-welch-how-to-be-a-great-leader.html">Source</a></p>

<p><strong>Claim</strong>: [1] showed no [3].</p>

<p><strong>Example</strong>: By the age of 25 he seems to have finished his masters
and PhD in Chemical engineering. He was even looking for jobs as a
faculty in universities like West Virginia before he joined GE.</p>

<p><strong>Claim</strong>: [1] was working in [3a].</p>

<p><strong>Example</strong>: <em>Its a question of fact. I guess I just cite a source.</em>
https://en.wikipedia.org/wiki/Jack_Welch</p>

<p><strong>Claim</strong>: Until his mid-twenties, there was [4], indicating the [5].</p>

<p><strong>Example</strong>:</p>

<p>By the age of 25 he seems to have finished his masters and PhD in
Chemical engineering. He was even looking for jobs as a faculty in
universities like West Virginia before he joined GE.</p>

<blockquote>
  <p>If talent existed and (refused to show itself even after so many
years of life)[6], it beckons if (inate ability)[7] (talent) even exists.</p>
</blockquote>

<p><strong>Claim</strong>: If [7] exists and it [6], then [7] doesn‚Äôt exist</p>

<p><strong>Example</strong>: <em>unable to give examples for this if-then/proof-type
statements</em></p>

<blockquote>
  <p>Maybe (talent)[8] seems like it doesn‚Äôt exist, but surely (intelligence)[9]
and (memory power)[10] should have a high influence. Spoiler Alert! (Nope)[11].</p>
</blockquote>

<p><strong>Claim</strong>: [8] seems to not exist</p>

<p><strong>Example</strong>: By the age of 25, Jack Welch, the ‚Äòmanager of the
century‚Äô didn‚Äôt even begin doing anything related to business and was
considering working as a faculty in universities before he joined GE
in Chemical Engineering.</p>

<p><strong>Claim</strong>: [9] has high influence on Greatness/Success</p>

<p><strong>Example</strong>: In a study of 45 thousand salesmen, whose IQ was pitted
against their Sales ratings, it appears that intelligence showed a
correlation of 0.04 with objective sales, whereas Achievement
(Striving for competence in ones work) showed a correlation of 0.4
with objective sales.</p>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.459.1742&amp;rep=rep1&amp;type=pdf">Source</a></p>

<p>So, Absolutely NOT!</p>

<p><strong>Claim</strong>: [10] has high influence on Greatness/Success</p>

<p><strong>Example</strong>:</p>

<p>‚ÄúA study with highly skilled chess players and non-experts in chess was
done where all were shown real chess game positions of 25 pieces for
5-10 seconds. The chess masters were able to recall the position of
every single piece, whereas the non-experts were able to recall 4 or 5
pieces. As expected. This was followed up with random placement of
chess pieces and the same 5-10 seconds to remember each piece. The
chess masters and the non-experts pretty much ended up with the same
results.‚Äù‚Äî <a href="/deliberate-practice.html">from Agent18‚Äôs blog</a></p>

<p>So, Absolutely Not!</p>

<blockquote>
  <p>A study was conducted in the business realm. (Salesmen)[12] were an
(attractive subject for this study)[13] as it is rather clear to
measure (output/success)[14]. (More number of sales)[14a] implies
(more success)[14b]. (The study)[15] was the largest of its kind
containing (data of several dozen studies amounting to 45k
individuals)[16]. Because of such a large number the (endless
sources of noise)[17] are expected to be drowned. (The bosses)[18]
gave (good indication of the IQ of the person with their
ratings)[19], and with the help of (sales they actually made)[20],
(the results)[21] were compiled.</p>
</blockquote>

<p><strong>Claim</strong>: [12] are [13].</p>

<p><strong>Example</strong>: As a Design Engineer the contribution in terms of numbers
(<span>$</span> contributed to my company) is highly unclear. I guess
as a result we have vague criteria for determining our impact such as
‚Äúhow I did my work in a year rated from 1-3‚Äù and ‚Äúwhat I did rated
from 1-3‚Äù. One day I work on a verification procedure, another day I
work on some stage design which takes 2 years to make, whose value is
not yet known. Where as in Sales, its OK/NOK. You either sold 5 bulbs
or you didn‚Äôt.</p>

<p><strong>Claim</strong>: [12] are [13] as it is clear to measure [14].</p>

<p><strong>Example</strong>: You either sold 5 bulbs or you didn‚Äôt.</p>

<p><strong>Claim</strong>: [14a] implies [14b]</p>

<p><strong>Example</strong>: If you sold ‚Äòn‚Äô tables for X <span>$</span>. If you sold
‚Äò2n‚Äô tables then you make 2X <span>$</span>.</p>

<p><strong>Claim</strong>: [15] was the largest of its kind</p>

<p><strong>Example</strong>: There are other papers with sample sizes ranging from
11 to 16k. This study had almost 46k large sample. (It was actually a
combination of several samples from different papers.)</p>

<p><strong>Claim</strong>: [15] contained [16].</p>

<p><strong>Example</strong>: [15] contained samples from studies with sample sizes
from 11 to 16k.</p>

<p><strong>Claim</strong>: With 45k samples, [17] is expected to be drowned</p>

<p><strong>Example</strong>: <em>This is a hard one, need to spend a lot of time on
understanding randomness and come up with examples! Skip for now! I
have no idea of examples for 17 in the context of the sales people,
nor do I know why it is drowned or have an example for it.</em></p>

<p><strong>Claim</strong>: [18] have [19].</p>

<p><strong>Example</strong>: The bosses rated their staff on their performance and it
turns out that they have 0.4 correlation with IQ. The bosses ratings
were also correlated with Achievement but with 0.2 correlation. It
looks like bosses have a better vision on IQ than anything else.</p>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.459.1742&amp;rep=rep1&amp;type=pdf">Source</a></p>

<p><strong>Claim</strong>: [20] was used as a outcome</p>

<p><strong>Example</strong>: ‚ÄúInterest appears to be a strong predictor of sales (0.3
correlation)‚Äù.</p>

<blockquote>
  <p>(Intelligence)[22] was (virtually useless in predicting how well a
salesperson would perform)[23]. Whatever it is that makes (a sales
ace)[24], it seems to be something other than (brainpower)[25].</p>
</blockquote>

<p><strong>Claim</strong>: [22] was [23].</p>

<p><strong>Example</strong>: 0.04 correlation between General Cognitive ability and
objective sales</p>

<p><strong>Claim</strong>: [25] is not useful for [24]</p>

<p><strong>Example</strong>: 0.04 correlation between General Cognitive ability and
objective sales</p>

<!-- 25 phrases and 18 claims in [2].[7] hrs -->

<!-- jun 18 the work tbd!-->

<blockquote>
  <p>(Another investigation on real world performance)[1] was with
(betting of horses)[2]. (The goal)[3] was to forecast (post-time
odds)[4]. Based on (this)[5] the (classification of experts and
non-experts)[6] was done. (Both groups)[7] seem to have (not much
differences)[8] in terms of (experience at the track)[9], (years of
formal education)[10], and (even the IQ averages and
variation)[11]. Further investigation suggested that (IQ‚Äôs)[12] didn‚Äôt
help (predict if someone was going to be good or bad at this)[13]. (A
person with IQ of 85 (‚Äúdull normal‚Äù))[14] was able to (pick out the
top horse in 10/10 races)[15]. And (a non-expert with IQ 118)[16] (picked up
the top horse for 3/10 cases)[17]. There are a (dozen factors)[18] that go into
deciding the (outcome of the game)[18], like (how the horse fared in the
last game)[19], (track condition)[20] etc‚Ä¶ Apparently the (low-IQ-experts)[21] used
(far complex models that took a wide consideration of multiple
variables)[22] unlike (the high-IQ-non-experts)[23].</p>
</blockquote>

<p><em>To work this out, it looks like I need the original paper. But I can‚Äôt
find a readable copy of it. So I skip this for now.</em></p>

<blockquote>
  <p>And this doesn‚Äôt stop here. (The same traits)[1] are observed with (Chess,
GO and even scrabble)[2]. ‚ÄúScrabble users show below average results on
tests of verbal ability.‚Äù, And some Chess grand masters have IQ that
are below Normal. All in all,</p>
</blockquote>

<p><strong>Claim</strong>: [1] is observed with [2].</p>

<p><strong>Example</strong>: ‚ÄúScrabble users show below average results on tests of
verbal ability.‚Äù</p>

<blockquote>
  <p>(IQ)[2] seems to be a (decent predictor of performance)[3] on an
(unfamiliar task)[4], but (once a person has been at it for a few
years)[5], (IQ)[6] predicts (little or nothing about performance)[7].</p>
</blockquote>

<p><strong>Claim</strong>: [2] seems to be [3] on [4].</p>

<p><strong>Example</strong>: <em>I don‚Äôt have an example</em></p>

<p><strong>Claim</strong>: [2] seems to not be [3] on [5].</p>

<p><strong>Example</strong>: Chess grand masters have IQ that are below normal.</p>

<p><strong>Claim</strong>: [2], predicts [7].</p>

<p><strong>Example</strong>: Chess grand masters have IQ that are below normal.</p>

<!-- 7 -->

<h3 id="but-what-about-memory">but what about memory?</h3>

<blockquote>
  <p>The Czech master Richard Reti once played twenty-nine blindfolded
games simultaneously. Miguel Najdorf, a Polish-Argentinean grand
master, played forty-five blindfolded games simultaneously in Sao
Paulo in 1947;</p>
</blockquote>

<blockquote>
  <p>Surely (this)[1] is a (sign of the Divine)[2], right? Surprise,
surprise! A study with highly skilled chess players and non-experts
in chess was done where all were shown real chess game positions of
25 pieces for 5-10 seconds. The chess masters were able to recall
the position of every single piece, whereas the non-experts were
able to recall 4 or 5 pieces. As expected. This was followed up with
random placement of chess pieces and the same 5-10 seconds to
remember each piece. The chess masters and the non-experts pretty
much ended up with the same results.</p>
</blockquote>

<p><strong>Claim</strong>: [1] is not [2].</p>

<p><strong>Example</strong>: Despite Chess players seeming to have great memory
(Richard Reti playing 29 blind-folded games), they still suck as bad
as non-experts as they can only recall 4 or 5 pieces when the chess
pieces are placed at random.</p>

<blockquote>
  <p>(The chess masters)[3] did not (have incredible memories)[4]. What
they had was an (incredible ability to remember real chess
positions)[5].</p>
</blockquote>

<p><strong>Claim</strong>: [3] did not have [4].</p>

<p><strong>Example</strong>: When chess experts were asked to recall pieces placed in
random on a chess board, they sucked as much as the non-experts.</p>

<p><strong>Claim</strong>: [3] had [5].</p>

<p><strong>Example</strong>:</p>

<p>‚ÄúA study with highly skilled chess players and non-experts in chess
was done where all were shown real chess game positions of 25 pieces
for 5-10 seconds. The chess masters were able to recall the position
of every single piece, whereas the non-experts were able to recall 4
or 5 pieces. As expected. This was followed up with random placement
of chess pieces and the same 5-10 seconds to remember each
piece. The chess masters and the non-experts pretty much ended up
with the same results.‚Äù</p>

<blockquote>
  <p>(Experts remembered about 5-9 chunks of information at a time on the
chess board)[6] that allowed them to (recall the positions of the
pieces)[7]. The same was observed with GO and Gomuku even.</p>
</blockquote>

<p><strong>Claim</strong>: [6] allowed them to do [7].</p>

<p><strong>Example</strong>: Experts could recall only 5-9 pieces when the chess coins
were placed at random. But were able to recall the entire board for
real chess positions.</p>

<blockquote>
  <p>(Many decades of research)[8] have shown that (average short-term
memory)[9] holds (only about seven items)[10]. (The capacity of
short-term memory)[11] doesn‚Äôt seem to vary much from person to
person; virtually (everyone‚Äôs short-term memory)[12] falls in the
range of (five to nine items)[13].</p>
</blockquote>

<p><strong>Claim</strong>: [8] has shown [9] holds [10].</p>

<p><strong>Example</strong>: The <a href="http://psychclassics.yorku.ca/Miller/">main article</a> cited 29k times was written in 1956 and
it is still being cited to this day i.e., 7 decades.</p>

<p><strong>Claim</strong>: [9] holds [10].</p>

<p><strong>Example</strong>: People who do not have years of experience in music, are
able to identify 7+-2 tones with a number corresponding to 1 tone.</p>

<p><strong>Claim</strong>: [11] does not vary much from person to person</p>

<p><strong>Example</strong>: experts and non experts in chess were able to identify
5-9 randomly placed pieces.</p>

<p><strong>Claim</strong>: [12] falls in the range of [13].</p>

<p><strong>Example</strong>: People who do not have years of experience in music, are
able to identify 7+-2 tones with 1 number corresponding to 1 tone.</p>

<!--13 -->

<hr />

<!-- end of jun 18th -->

<blockquote>
  <p>As reflected later in the book (TIO, Chap 6), (remembering 49 games
at once)[14] is still a (ginormous feat)[15] (not possible with this
short term memory)[16]. More on this later.</p>
</blockquote>

<blockquote>
  <p>Up until now it might seem that (we)[17] are just unstoppable forces
who can all become (legends)[18]. But certainly there are
(limitations)[19]. There are (physical limitations to
achievement)[20] such as Death and diseases, (limitations related to
age)[21], (personal dimensions)[22] etc‚Ä¶ It appears that other than
(physical limitations)[23], there is not really (clearly understood
or proven non-physical inate abilities inhibiting our potential to
success)[24].</p>
</blockquote>

<!-- begint juni 19 -->

<h2 id="the-goal">the goal</h2>

<p>The goal could be to understand where I could work, what type of work
I could do.</p>

<p>https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/#top</p>

<p>Devour the article!</p>

<p>https://80000hours.org/job-board/ai-ml-safety-research/</p>

<p>http://www.paulgraham.com/selfindulgence.html</p>

<p>how about some stuff from here:</p>

<p>http://agent18.github.io/deliberate-practice.html morgen freeman!</p>

<p>http://pradeep90.github.io/Deep-Thinking.html</p>

<p>Peter Singers morality essay</p>

<h2 id="k-fold">k-fold</h2>

<blockquote>
  <p>If (K is small)[1] in a (K-fold cross validation)[2] is the (bias in the
estimate of out-of-sample (test set) accuracy)[3] smaller or bigger? If
(K is small)[4] is the( variance in the estimate of out-of-sample (test
set) accuracy smaller or bigger.)[5] Is K large or small in leave one
out cross validation?</p>
</blockquote>

<p>For [1], we think of k=3</p>

<p>For [2], we think of the following:</p>

<ul>
  <li>
    <p>Divide data set into 3 parts.</p>
  </li>
  <li>
    <p>Take the first part as test and the rest as training</p>
  </li>
  <li>
    <p>perform say linear regression with all variables and obtain coefficients</p>
  </li>
  <li>
    <p>Compute Accuracy on test dataset</p>
  </li>
  <li>
    <p>Do this for every part and compute average and variance!</p>
  </li>
</ul>

<p>Need to show actual code of example!</p>

<p>For 3,</p>

<p>smaller or bigger than what?</p>

<h2 id="lets-do-pg-article-on-saving-money-instead-of-earning-more-and-what">lets do pg article on saving money instead of earning more and what</h2>
<p>he means</p>

<h2 id="from-21-june">From 21 june</h2>

<blockquote>
  <p>By (donating to the most effective organizations in an area)[1], just about
(anyone in a well paid job)[2] can have a (substantial impact)[3].</p>
</blockquote>

<p><strong>Claim</strong>: [2] can have [3] by [1].</p>

<p>For [1], we think of donating to AMF (which is certified by GiveWell
as the most effective charity)</p>

<p>For [2], We think of being the top 1% in the world in terms of
earnings(a salary allowing a 10% donation with ease of 4k$).</p>

<p>For [3], we think of saving one life with 4k$ of donation. This will
come to about 200 lives with increase in salaries over 30 years.</p>

<p>By donating to AMF (certified by GiveWell as most effective
charity),</p>

<blockquote>
  <p>You may be able to (take this a step further)[4] and ‚Äòearn to give‚Äô by
(aiming to earn more than you would have done otherwise and to donate
some of this surplus effectively)[5].</p>
</blockquote>

<blockquote>
  <p>Not (everyone)[6] wants (to make a dramatic career change)[7], or is
(well-suited to the narrow range of jobs that have the most impact
on the most pressing global problems)[8]. However, by donating,
(anyone)[9] can (support these top priorities, ‚Äòconvert‚Äô their
labour into labour working on the most pressing issues, and have a
much bigger impact)[10].</p>
</blockquote>

<h3 id="summary">Summary</h3>

<blockquote>
  <p>(Many experts)[9] believe that there is a (significant chance that
humanity will develop machines more intelligent than ourselves
during the 21st century)[10]. (This)[] could lead to (large, rapid
improvements in human welfare)[11], but there are (good reasons)[1]
to think that (it could also lead to disastrous outcomes)[2]. The
problem of (how one might design a highly intelligent machine to
pursue realistic human goals safely)[3] is (very poorly
understood)[4]. If (AI research continues to advance without enough
work going into the research problem of controlling such
machines)[5], (catastrophic accidents)[6] are much more likely to
occur. Despite (growing recognition of this challenge)[7],
(fewer than 100 people worldwide)[8] are directly working on the
problem.</p>
</blockquote>

<p><strong>Claim</strong>: The problem of [3] is poorly understood.</p>

<p><strong>Example</strong>:</p>

<p>For [3], we think of using Machine Learning algorithms that do not
show the probability values i.e., ‚Äúairplane‚Äù, instead of ‚Äú99%
airplane and 1% cat‚Äù. This aims to not allow hackers to train for
adversary examples as shown <a href="http://web.archive.org/web/20170305071334/https://openai.com/blog/adversarial-example-research/">here</a>.</p>

<p>For [4], we think of the <a href="https://arxiv.org/pdf/1602.02697.pdf">black-box adversary</a>, which is able to
trick Google and Amazon ML models, such that they get it‚Äôs adversarial
examples wrong by 96% and 88%.</p>

<p><strong>Claim</strong>: If [5], then [6] are more likely.</p>

<p><strong>Example</strong>: The blackbox adversary is able to trick Google and Amazon
ML models such that they get 96% and 88% of supplied adversarial
examples wrong.</p>

<p><strong>Claim</strong>: Despite [7], [8] are directly working on this problem.</p>

<p><strong>Example</strong>:</p>

<p>For [7], MIRI was founded in 2000 and OpenAI was started in 2015</p>

<p>For [8], we think of the 15 organizations that work on AI safety with
say 12 people each resulting in roughly 180 people worldwide.</p>

<p><em>[8] is directly working on a problem. Is easy to give example. How
does ‚Äúdespite [7]‚Ä¶‚Äù be given an example.</em></p>

<blockquote>
  <p>The (arguments for working on this problem area)[1] are complex, and what
follows is only (a brief summary)[2].</p>
</blockquote>

<p>??? come back</p>

<blockquote>
  <p>Superintelligence: Paths, Strategies, Dangers, by
Oxford Professor Nick Bostrom. The Artificial Intelligence
Revolution, a post by Tim Urban at Wait But Why, is shorter and also
good (and also see this response).</p>
</blockquote>

<blockquote>
  <p>When Tim Urban started investigating his article on this topic, he
expected to finish it in a few days. Instead he spent weeks reading
everything he could, because, he says, ‚Äúit hit me pretty quickly
that what‚Äôs happening in the world of AI is not just an important
topic, but by far the most important topic for our future.‚Äù</p>
</blockquote>

<p><em>skipping this as this is some personal account. There are claims
like, ‚Äúhe started doing this‚Ä¶‚Äù, ‚Äúhe realized it takes more time‚Ä¶‚Äù,
I am not sure it is useful to break my head over trying to give
examples for these.</em>‚Äô</p>

<blockquote>
  <p>In October 2015 an AI system named AlphaGo shocked the world by
defeating a professional at the ancient Chinese board game of Go for
the first time. A mere five months later, a second shock followed:
AlphaGo had bested one of the world‚Äôs top Go professionals, winning
4 matches out of 5. Seven months later, the same program had further
improved, crushing the world‚Äôs top players in a 60-win streak. In
the span of a year, AI had advanced from being too weak to win a
single match against the worst human professionals, to being
impossible for even the best players in the world to defeat.</p>
</blockquote>

<p><em>no claims here other than claims of fact. So I skip this!</em></p>

<blockquote>
  <p>(This)[1] was shocking because (Go)[2] is considered (far harder for a
machine to play than Chess)[3]. (The number of possible moves in
Go)[4] is (vast)[5], so it‚Äôs not possible to (work out the best move
through ‚Äúbrute force‚Äù.)[6] Rather, the game requires (strategic
intuition)[7]. (Some experts)[8] thought it would take at least (a decade
for Go to be conquered)[9].</p>
</blockquote>

<p><strong>Claim</strong>: [1] was shocking</p>

<p><strong>Example</strong>: ‚ÄúSome experts thought it would take at least a decade‚Äù,
but it took less than a year.</p>

<p><em>Is this an accepted example</em></p>

<p><strong>Claim</strong>: [1] was shocking because [2] is [3].</p>

<p><strong>Claim</strong>: [2] is [3].</p>

<p><strong>Example</strong>: Chess starts with 16 different positions playable at
start, where as Go starts with 361 positions at the start.</p>

<p><strong>Claim</strong>: [4] is [5],</p>

<p><strong>Example</strong>: 361 positions at the start as compared to 16 positions
for chess.</p>

<p><strong>Claim</strong>: [4] is [5], so it is not possible to [6].</p>

<p><strong>Example</strong>: ‚ÄúThe search space in Go is vast ‚Äì more than a googol
times larger than chess (a number greater than there are atoms in the
universe!). As a result, traditional ‚Äúbrute force‚Äù AI methods ‚Äì which
construct a search tree over all possible sequences of moves ‚Äì don‚Äôt
have a chance in Go. ‚Äú‚Äî<a href="https://ai.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html">Source</a>.</p>

<p><strong>Claim</strong>: The game requires [7].</p>

<p><strong>Example</strong>:</p>

<ul>
  <li>
    <p>due to search space being more than googol, it is not possible to
use brute force.</p>
  </li>
  <li>
    <p>It uses neural networks, and machine learning from over 30 million
moves from the past, and it eventually became the best Go player in
the world.</p>
  </li>
</ul>

<p><strong>Claim</strong>: [8] thought it would take at least a decade, but it took
less than a year to be the best.</p>

<p><strong>Example</strong>: <em>I only quotes, no one says who said it. so SKIP!</em></p>

<blockquote>
  <p>Since then, (AlphaGo)[10] has discovered that (certain ways of playing Go
that humans had dismissed as foolish for thousands of years were
actually superior.)[11] Ke Jie, the top ranked go player in the world,
has been astonished: ‚Äúafter (humanity)[14] spent (thousands of years
improving our tactics)[15],‚Äù he said, ‚Äúcomputers tell us that humans are
completely wrong. I would go as far as to say not a single (human)[13] has
touched (the edge of the truth of Go)[12].‚Äù9</p>
</blockquote>

<p><em>Here I need to dive deep into Go, Although not useful, it could still
help taking down an unknown topic aka via research.</em></p>

<p><strong>Claim</strong>: [10] has discovered [11].</p>

<p><strong>Example</strong>: ‚ÄúMaster made moves that seemed foolish but inevitably led
to victory this week over the world‚Äôs reigning Go champion, Ke Jie of
China‚Äù‚Äî <a href="https://www.wsj.com/articles/ai-program-vanquishes-human-players-of-go-in-china-1483601561">source</a>.</p>

<p><strong>Claim</strong>: [14] has spent [15].</p>

<p><strong>Example</strong>:</p>

<p><strong>Claim</strong>: [13] has touched [12].</p>

<p><strong>Example</strong>: A game being played for 2000 years</p>

<p>don‚Äôt know what it means,</p>

<blockquote>
  <p>(The advances above)[1] became possible due to (progress in an AI
technique called ‚Äúdeep learning‚Äù)[2]. In the (past)[3], we had to
give computers (detailed instructions for every task)[4]. Today, we
have (programs that teach themselves how to achieve a goal)[5] ‚Äì for
example, a program was able to learn how to play Atari games based
only on reward feedback from the score. This has been made possible
by (improved algorithms)[6], (faster processors)[7], (bigger data
sets)[8], and (huge investments by companies like Google)[9]. It has
led to (amazing advances far faster than expected)[10].</p>
</blockquote>

<p><strong>Claim</strong>: [1] became possible due to [2].</p>

<p><strong>Example</strong>: Winning reliably took AlphaGo training with 30 million moves from
games played by experts until predictability went to 57%. And
thousands of games between it‚Äôs neural networks, to improve itself
gradually over time.</p>

<p>I dont‚Äô think it answers it!</p>

<p><strong>Claim</strong>: In [3], we gave computers [4].</p>

<p><strong>Example</strong>: In video games, bots are written based on rules, if the
opponent has reached position X, bot A will charge at him and shoot at
X frequency with it‚Äôs aim at 50%.</p>

<p>Whereas today, AlphaZero can learn and play chess like a ‚Äúmaster‚Äù in
4 hours.</p>

<p><strong>Claim</strong>: We have [5], today.</p>

<p><strong>Example</strong>: AlphaGoZero is able to learn the entire game of GO
without any human intervention to the level of AlphaGo which is the
world‚Äôs best Go player currently.</p>

<p><strong>Claim</strong>: We have [5], today because of [6].</p>

<p><strong>Example</strong>: <em>I don‚Äôt know how to give an example for this? I don‚Äôt
know where to find such an example. I expect the example to look like:
We used alg A and B and B had superior performance‚Ä¶ Alg A allowed
had so much performance and this drawback, Alg b in 2019 doesn‚Äôt have
those.</em></p>

<p><strong>Claim</strong>: We have [5], today because of [7].</p>

<p><strong>Example</strong>: <em>same as above</em></p>

<p><strong>Claim</strong>: We have [5], today because of [8].</p>

<p><strong>Example</strong>: <em>because</em> and <em>same as above</em></p>

<p><strong>Claim</strong>: We have [5], today because of [9].</p>

<p><strong>Claim</strong>: We have [5], today because of [10].</p>

<p><strong>Claim</strong>: [6],[7],[8],[9] has led to [8].</p>

<p><em>skip</em></p>

<h2 id="plan-change">Plan change</h2>

<ul>
  <li>
    <p>Will only deal with top claims that are useful for me. Wtf does that
mean?</p>

    <p>I am not going to look into lines like:</p>

    <p>‚Äúthis has been made possible by imrpoved algorithms, faster
processors‚Äù. I‚Äôd rather take this at face value, I don‚Äôt see how
they are helping me. Contrast this to ‚ÄúIt has led to Amazing
advances far faster than expected‚Äù, I care about this, because it
gives me concreteness in understanding how ‚Äúfast the problem is
progressing‚Äù ok Great! I think it‚Äôs still a feeling, atleast we have
two examples that ‚Äúdifferentiate them‚Äù‚Ä¶</p>

    <p>Let‚Äôs make all the claims and skip them or go further. Sounds good!</p>
  </li>
  <li>
    <p>When to skip:</p>

    <ul>
      <li><em>because</em> <em>due</em></li>
      <li><em>too much work for a useless non-useful stuff</em></li>
      <li><em>too easy stuff</em></li>
      <li><em>claims of fact</em></li>
    </ul>
  </li>
</ul>

<p>And that my friend is your feedback, the feedback of truly
understanding, not doing random stuff and reexplaining the same joke
over and over again.</p>

<p>Don‚Äôt seem to like browsing on the internet hoping for some results to
match.</p>

<p>If you can‚Äôt open the source move on! Don‚Äôt waste time, thinking you
are DPing!</p>

<h3 id="summary-1">Summary</h3>

<p>Write all claims, skip useless stuff, focus on useful stuff that you
are confused about that you feel you need to learn more about.</p>

<h2 id="plan-in-action">plan in action</h2>

<blockquote>
  <p>But (those)[1] are just (games)[2]. Is general (machine
intelligence)[3] still far away? Maybe, but maybe not. It is really
hard to (predict the future of technology)[4], and (lots of past
attempts)[5] have been (completely off the mark)[6]. However, (the best
available surveys of experts)[7] assign (a significant probability to the
development of powerful AI within our lifetimes)[8].</p>
</blockquote>

<p><strong>Claim</strong>: [1] are just [2].</p>

<p><em>skip</em></p>

<p><strong>Claim</strong>: [3] is still far away</p>

<p><strong>Claim</strong>: Maybe but maybe not</p>

<p><em>skip</em> <em>I feel like puking at these statements that just seem to
simply waste your time. Is this bad writing? what are the dimensions
bro? exactly.</em></p>

<p><strong>Claim</strong>: It is really hard to [4].</p>

<p><strong>Example</strong>: ‚ÄúSome experts thought it would take at least a decade for
Go to be conquered‚Äù, but it has already arrived and it is the best
player in the world. ‚Äî <a href="https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/">source</a></p>

<p><strong>Claim</strong>: [5] about [4], has been [6].</p>

<p><strong>Example</strong>: <em>Can‚Äôt open the financial times source</em>, so don‚Äôt know by
how far they missed the mark.</p>

<p><strong>Claim</strong>: [7], assigns [8].</p>

<p><strong>Example</strong>: Of the 29 people who answered the <a href="https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/">survey</a>, more than
half thought that there was a greater than 50% chance of high-level
machine intelligence by 2050. and 10% chance of it happening by 2024.</p>

<blockquote>
  <p>(One survey of the 100 most-cited living computer science
researchers, of whom 29 responded)[], found that (more than half
thought there was a greater than 50% chance of ‚Äúhigh-level machine
intelligence‚Äù ‚Äì one that can carry out most human professions at
least as well as a typical human ‚Äì being created by 2050, and a
greater than 10% chance of it happening by 2024)[] (see figure
below).2 [10]</p>
</blockquote>

<p><em>skip. as they are questions of fact</em></p>

<h3 id="impacts">Impacts</h3>

<blockquote>
  <p>If the (experts are right)[1], (an AI system that reaches and then
exceeds human capabilities)[2] could have (very large impacts)[3],
both (positive)[4] and (negative)[5]. If (AI matures in fields such as
mathematical or scientific research)[6], (these systems could make rapid
progress in curing diseases or engineering robots to serve human
needs.)[7]</p>
</blockquote>

<p><strong>Claim</strong>: <del>if [1]</del>, [2] could have [3].</p>

<p><em>skip</em> <em>as it is covered in the next claims</em></p>

<p><strong>Claim</strong>: [2] could have [3] which is +ve</p>

<p><strong>Example</strong>: Humans can miss the signs of <a href="https://www.forbes.com/sites/charlestowersclark/2019/04/30/the-cutting-edge-of-ai-cancer-detection/#5cae67037336">cancer 20-30% of the
cases</a> (fuck me!) of cancer. We seem to be talking about 3.6m
people who are probably going to die. (I assume that if you leave
cancer undetected you are going to be fucked). If AI is able to detect
with high accuracy, it could lead to attempting to save these 3.6m
lives.</p>

<p><em>But I don‚Äôt think this is what they are talking about in 6 and 7. I
currently have only a hypothetical example but I think I am still
missing a bit of detail.</em></p>

<p><strong>Claim</strong>: [2] could have [3] which is -ve</p>

<p><strong>Example</strong>:</p>

<p>*‚ÄúThe owners of a pharmaceutical company use machine learning algorithms to rapidly generate and evaluate new organic compounds.</p>

<p>As the algorithms improve in capability, it becomes increasingly impractical to keep humans involved in the algorithms‚Äô work ‚Äì and the humans‚Äô ideas are usually worse anyway. As a result, the system is granted more and more autonomy in designing and running experiments on new compounds.</p>

<p>Eventually the algorithms are assigned the goal of ‚Äúreducing the incidence of cancer,‚Äù and offer up a compound that initial tests show is highly effective at preventing cancer. Several years pass, and the drug comes into universal usage as a cancer preventative‚Ä¶</p>

<p>‚Ä¶until one day, years down the line, a molecular clock embedded in the compound causes it to produce a potent toxin that suddenly kills anyone with trace amounts of the substance in their bodies.</p>

<p>It turns out the algorithm had found that the compound that was most effective at driving cancer rates to 0 was one that killed humans before they could grow old enough to develop cancer. The system also predicted that its drug would only achieve this goal if it were widely used, so it combined the toxin with a helpful drug that would incentivize the drug‚Äôs widespread adoption.‚Äù*</p>

<p>18m people die of Cancer every year.</p>

<p><strong>Claim</strong>: If [6], then [7].</p>

<p><strong>Example</strong>:</p>

<blockquote>
  <p>On the other hand, (many people)[1] worry about the (disruptive
social effects of this kind of machine intelligence)[2], and in
particular its (capacity to take over jobs previously done by less
skilled workers)[3]. If the (economy is unable to create new jobs for
these people quickly enough)[4], there will be (widespread
unemployment and falling wages)[5].11 (These outcomes)[6] could be
avoided through (government policy)[7], but doing so would likely
require (significant planning)[8].</p>
</blockquote>

<p><strong>Claim</strong>: [1] worry about [2].</p>

<p><strong>Example</strong>: 80khours is trying to place many people in <a href="https://80000hours.org/articles/ai-policy-guide/">AI
policy</a> to ‚Ä¶ ???</p>

<p><strong>Claim</strong>: [2] is [3].</p>

<p><strong>Example</strong>: Google assistant is already able to make appointments for
you by speaking like a human.</p>

<p><strong>Claim</strong>: [1] worry about [3].</p>

<p><strong>Example</strong>: 80khours hours???</p>

<p><strong>Claim</strong>: If [4], there will be [5].</p>

<p><strong>Example</strong>: ???</p>

<p><strong>Claim</strong>: [6] could be avoided through [7], but would require [8].</p>

<p><strong>Example</strong>: ???</p>

<h2 id="rant">Rant</h2>

<p><strong>Lot of OB, moving walking, not deepworking</strong>. Feeling bored! and
dragging myself to get success. There used to be times in the past
when I would look at the time and it ouwld be [2] hrs. Nowadays [1] hr
is already hard. and the idea of being done with this shit is very
soothing.</p>

<p>Reading deepwork? or write an article about mathivanan? also I don‚Äôt
know wha I am doing anymore as I just seem to be spending time</p>

<p>What is rewarding, when I find a great example which I feel like I
understand something that an STM would respect, but that takes a lot
of time or reading and googling articles.</p>

<ul>
  <li>
    <p>need to read articles</p>
  </li>
  <li>
    <p>getting lost to find what exaxtly I would like to find</p>
    <h2 id="statistics">Statistics</h2>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>Date</th>
      <th>phrases/hr</th>
      <th>claims/hr</th>
      <th>actual claims/hr</th>
      <th>Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>17-05-2019</td>
      <td>12</td>
      <td>7</td>
      <td>-</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>18-05-2019</td>
      <td>10</td>
      <td>4</td>
      <td>-</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>19-05-2019</td>
      <td>¬†</td>
      <td>1</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>20-05-2019</td>
      <td>¬†</td>
      <td>1</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>21-05-2019</td>
      <td>3</td>
      <td>2</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>22-05-2019</td>
      <td>5</td>
      <td>3</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>23-05-2019</td>
      <td>2</td>
      <td>2</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>24-05-2019</td>
      <td>4</td>
      <td>2</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>25-05-2019</td>
      <td>-</td>
      <td>-</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>26-05-2019</td>
      <td>10</td>
      <td>7</td>
      <td>¬†</td>
      <td>Good, did proper one hr</td>
    </tr>
    <tr>
      <td>27-05-2019</td>
      <td>2</td>
      <td>1</td>
      <td>¬†</td>
      <td>Quite hard, was work</td>
    </tr>
    <tr>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>on the next phrase</td>
    </tr>
    <tr>
      <td>28-05-2019</td>
      <td>3</td>
      <td>1</td>
      <td>¬†</td>
      <td>T‚Äôwas hard!</td>
    </tr>
    <tr>
      <td>29-05-2019</td>
      <td>¬†</td>
      <td>5</td>
      <td>0</td>
      <td>0 worked out!</td>
    </tr>
    <tr>
      <td>30-05-2019</td>
      <td>¬†</td>
      <td>0</td>
      <td>¬†</td>
      <td><strong>failed</strong></td>
    </tr>
    <tr>
      <td>31-05-2019</td>
      <td>¬†</td>
      <td>0</td>
      <td>¬†</td>
      <td>tried hard, had to read</td>
    </tr>
    <tr>
      <td>01-06-2019</td>
      <td>2</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>02-06-2019</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>03-06-2019</td>
      <td>3</td>
      <td>2</td>
      <td>¬†</td>
      <td>ok! last example fine</td>
    </tr>
    <tr>
      <td>04-06-2019</td>
      <td>4</td>
      <td>2</td>
      <td>¬†</td>
      <td>good day! repeat 1!</td>
    </tr>
    <tr>
      <td>05-06-2019</td>
      <td>3</td>
      <td>1</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>06-06-2019</td>
      <td>3</td>
      <td>1</td>
      <td>¬†</td>
      <td>repeated the same!</td>
    </tr>
    <tr>
      <td>07-07-2019</td>
      <td>2</td>
      <td>1</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>08-07-2019</td>
      <td>2</td>
      <td>1</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>09-07-2019</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
      <td><strong>failed</strong></td>
    </tr>
    <tr>
      <td>10-07-2019</td>
      <td>30</td>
      <td>17</td>
      <td>¬†</td>
      <td>[5] hrs</td>
    </tr>
    <tr>
      <td>11-07-2019</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>12-07-2019</td>
      <td>3</td>
      <td>2</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>13-07-2019</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
      <td><strong>failed</strong></td>
    </tr>
    <tr>
      <td>14-07-2019</td>
      <td>3</td>
      <td>2</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>15-07-2019</td>
      <td>32(0.5/m)</td>
      <td>20</td>
      <td>¬†</td>
      <td>[6] hrs!</td>
    </tr>
    <tr>
      <td>16-07-2019</td>
      <td>27(1/m)</td>
      <td>16</td>
      <td>¬†</td>
      <td>6</td>
    </tr>
    <tr>
      <td>17-07-2019</td>
      <td>60(1.3/m</td>
      <td>39</td>
      <td>¬†</td>
      <td>[6] hrs but my article</td>
    </tr>
    <tr>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>on DP</td>
    </tr>
    <tr>
      <td>18-08-2019</td>
      <td>15 +20</td>
      <td>[7]+[14]</td>
      <td>¬†</td>
      <td>[3].5hrs+ [2].5</td>
    </tr>
    <tr>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>80khours art + mijn</td>
    </tr>
    <tr>
      <td>19-09-2019</td>
      <td>20+25</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>80khours AI   [5].5</td>
    </tr>
    <tr>
      <td>20-09-2019</td>
      <td>1</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>[1] hr  AI</td>
    </tr>
    <tr>
      <td>21-09-2019</td>
      <td>4</td>
      <td>2</td>
      <td>¬†</td>
      <td>[2] hrs</td>
    </tr>
    <tr>
      <td>22-09-2019</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>Did [2] hrs</td>
    </tr>
    <tr>
      <td>23-09-2019</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>Did [3]-[4] hrs</td>
    </tr>
    <tr>
      <td>New plan</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>24-09-2019</td>
      <td>8-5</td>
      <td>6-3</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
  </tbody>
</table>

<p>I am dreaming most of the time! I don‚Äôt have a deadline or some focus! I think. I am
rarely able to do this. I am thinking about the life in India! This
should be painful not boring! And I think it is boring and the very
second the clock ticks 58 to 60 mins pandian is out!</p>

<p>Need to finish 10 phrases today period!</p>

<h3 id="letter-to-an-stm">Letter to an STM</h3>

<p>Thalaiva,</p>

<p>If You were me, how would you be spending your time? On what exactly would you be spending your time? Would you just keep trying to clock hours after hours of pure DP on Concrete Thinking? Would you also work on DS?</p>

<p>Why am I looking at DS?</p>

<p>Based on 80khours, I came to the conclusion of working on DS, because it will give me more money(1.5 times in the US) than an engineer, and I could move to the US (for cryonics and more money than here). Everyone from different backgrounds are able to do DS so it should be easy to move. I personally know many people who have moved to DS without too much difficulty after a masters in TU Delft. The route I envision is to start DS work within this year and move to some big DS company in a few years (2-3years)  and do a lot of ‚Äúcritical thinking‚Äùand make my way to some EAO like say GiveWell within the next 5-8 years and really start saving large numbers of people.</p>

<p>Should I be working on DP for CT completely instead?</p>

<p>Why I ask this is because I am not sure of the consequence of working on this (DP for concrete Thinking), i.e., I don‚Äôt have an example where this makes a ‚Äúdifference‚Äù in my life. I don‚Äôt know how to compare DS and DP for CT. But I will take your word for it and slog my ass off atleast for the coming 4 weeks (just the beginning)(&gt;4hrs per day average of DP guaranteed). Also, over the last few weeks there has been a dip in my amount of hours clocked, so I SUCK and I don‚Äôt want to SUCK. 4.15 hours on a good week and 2.9 hrs/day last week (I start half hour after I have had dinner, I take long breaks scrolling FB etc‚Ä¶, including weekends.). (I count work on DS, and DP for concrete thinking together in the above.)</p>

<p>I need your thoughts on this. I am not sure how this 1 hr per day od DP for CT is helping, as I barely get shit done in 1 hr (5 claims, sometimes 1 claim, as things take time to puzzle out.). I stop before I get in the groove. If DS should not be my focus right now, I am more than willing to stop with course 8/10 (20 more hrs of work) and not get closure and not be able to save face when people ask why I am not done with these courses yet.</p>

<p>I don‚Äôt want to do 1 hr if it is the most important thing to focus on. I don‚Äôt want to do random phrases from texts and move on to others as it gets hard for me. I want to take a full blown essay (80khours key ideas) or chapter from some book on regression and tear it apart over how many ever days it takes full time. Why? that way I guess I get some work done with Large repetitions. Also I can gather some statistics of work done per hour on similar work and compare over the course of the exercise.</p>

<p>So,</p>

<p>Thalaiva, What would you do if you were me? Let‚Äôs go big or go home!</p>

<p>And last but not the least, Can you please do this for me? It will help me big time to compare and improve. Can you take this section on Longtermism (5 small paragraphs) and detail it out for me as if you were submitting it for correction by including the phrases the claims and the examples. I am having several questions as I have shown my take, I would like to see you do it and carry that ‚Äúattitude‚Äù throughout the entire essay. It feels like this is one type of essay vagueness I need to handle. And for most parts I am wondering what depth I should go in etc‚Ä¶ as discussed below.</p>

<p>I know you always say send me 200 phrases! Name your price for this, just this one time.</p>

<p>Thank You for everything. Cheers!</p>

<h3 id="feedback-checklist">Feedback checklist</h3>

<p>Feedback checklist:</p>

<ol>
  <li>
    <p>Could it be that this claim has no any example at all? For example, ‚Äúcivilization is at stake‚Äù.</p>
  </li>
  <li>
    <p>Could this claim be false? Remember the ‚Äúthere is no doubting‚Äù example.</p>
  </li>
  <li>
    <p>Does this claim say anything about ‚Äúbest‚Äù (need to compare against the entire set) or ‚Äúmost‚Äù (need to show it‚Äôs the majority in the set) or ‚Äúno‚Äù (need to show that nothing in the set matches)?</p>
  </li>
  <li>
    <p>Did you stick to examples that are in the chapter itself? That way you don‚Äôt have to search online for too long.</p>
  </li>
  <li>
    <p>Did you use a running example for a technical phrase? There will be lots of new phrases in the book, like ‚Äúconvergent instrumental value‚Äù and ‚Äúorthogonality thesis‚Äù. Whenever you see them, you should recall whatever running example you‚Äôve used.</p>
  </li>
  <li>
    <p>If this is an ‚Äúif-then‚Äù claim, did you either get a concrete example or mark it as having no example?</p>
  </li>
</ol>

<p>Short names: none; false; best; chapter; running; if-then.</p>

<p>Please refer to the checklist after every claim analysis to ensure
you‚Äôre not making old mistakes. If you want to add to the checklist
based on mistakes found in past feedback, that‚Äôs great.</p>

<h3 id="mission">Mission</h3>

<p>Mission #9: Your mission, should you choose to accept it, is to
concretely analyze the key claims in the book Superintelligence by
Nick Bostrom (the book mentioned in the Elon Musk tweet above). He‚Äôs a
PhD at Oxford who‚Äôs been writing about AI safety along with guys like
Eliezer for nearly two decades. The book has detailed arguments and
examples about all the topics like possible paths to
‚Äúsuperintelligence‚Äù (whatever that means), types of
‚Äúsuperintelligence‚Äù, the control problem, etc.</p>

<p>No need to write ‚ÄúQuestion: ‚Äú - doesn‚Äôt seem to have changed your answers.</p>

<p>Don‚Äôt have to go sentence by sentence; look at one key claim for each
section, usually the one in the first few paragraphs, or one for each
paragraph if you feel it‚Äôs an important section. For example:</p>

<blockquote>
  <p>CHAPTER 2 Paths to superintelligence</p>

  <p>Machines are currently far inferior to humans in general
intelligence. Yet one day (we have suggested) they will be
superintelligent. How do we get from here to there? This chapter
explores several conceivable technological paths. We look at
artificial intelligence, whole brain emulation, biological
cognition, and human-machine interfaces, as well as networks and
organizations. We evaluate their different degrees of plausibility
as pathways to superintelligence. The existence of multiple paths
increases the probability that the destination can be reached via at
least one of them.</p>
</blockquote>

<p>The key claim is ‚ÄúHow do we get from here to there? Answer: Artificial
intelligence, whole brain emulation, ‚Ä¶‚Äù</p>

<h3 id="claim">Claim</h3>

<p>I am possibly going to enjo the experience much much more than the
last few days of fighting to complete the [2] hours. It seems like
this is a signal for an panindian pandian to do something else or do
it differently.</p>

<p>Without Examples I am nothing!</p>

<h2 id="questions-to-an-stm">Questions to an STM</h2>

<p>How to identify claims of importance?</p>

<p>What is the goal here?</p>

<p>People are making so many claims (as listed in black in the book)?</p>

<p>How do you go about it?
2</p>
:ET