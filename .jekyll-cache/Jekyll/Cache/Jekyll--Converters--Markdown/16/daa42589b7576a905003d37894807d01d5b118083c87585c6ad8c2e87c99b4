I"C„<h2 id="resources">Resources</h2>

<p>2018 Lesson 1 wiki: https://forums.fast.ai/t/wiki-thread-lesson-1/6825</p>

<p>course 2018: https://course18.fast.ai/lessonsml1/lesson1.html</p>

<p>2020 setup help: https://forums.fast.ai/t/setup-help/65529</p>

<p>Ubuntu setup help used:</p>

<ul>
  <li>
    <p>https://forums.fast.ai/t/pytorch-installation-in-conda-environment-failing/6703/19</p>
  </li>
  <li>
    <p>https://forums.fast.ai/t/platform-local-server-ubuntu/65851</p>
  </li>
</ul>

<p>updates 2020:
https://forums.fast.ai/t/official-part-1-2020-updates-and-resources-thread/63376</p>

<blockquote>
  <p>^^ Has all the code pictures and text</p>
</blockquote>

<p>2020 book:https://github.com/fastai/fastbook/blob/master/01_intro.ipynb</p>

<p>2020 software fastai repo: https://github.com/fastai/fastai</p>

<blockquote>
  <p>^^ this is fastai v2 written from scratch</p>
</blockquote>

<p>2020 course nbs: https://github.com/fastai/fastbook/tree/master/clean</p>

<p>2020 readme page fastai: https://course.fast.ai/#How-do-I-get-started?</p>

<p>2020 lesson 1 wiki: https://forums.fast.ai/t/lesson-1-official-topic/65626</p>

<p>forums: https://forums.fast.ai/</p>

<p>Help with setup: https://forums.fast.ai/t/setup-help/65529</p>

<p>Questions and answers: <a href="https://forums.fast.ai/t/fastbook-questionnaire-solutions-megathread/76955">FASTai questions and answers for each
lesson</a></p>

<p>Fastai blogs: <a href="https://forums.fast.ai/t/fastai2-blog-posts-projects-and-tutorials/65827">fastaiblogs in the fastforum</a></p>

<p><strong>Old</strong></p>

<p>fastiai2 - https://github.com/fastai/fastai2 
coursev4 - https://github.com/fastai/course-v4 
fastbook - https://github.com/fastai/fastbook</p>

<h2 id="collection-of-example-projects">Collection of example projects</h2>

<h2 id="lesson-1-resources">Lesson 1 Resources</h2>

<p><strong>Basics</strong></p>

<p><a href="https://course.fast.ai/videos/?lesson=1">Lesson 1 video course</a></p>

<p><a href="https://forums.fast.ai/t/lesson-1-official-topic/65626">Lesson 1 Official topic 2020</a></p>

<p><a href="https://github.com/fastai/fastbook/blob/master/01_intro.ipynb">fastbook chapter 1</a></p>

<p><a href="https://n9lqa5lx.gradient.paperspace.com/notebooks/fastbook/clean/01_intro-Copy1.ipynb">MY Paperspace lesson 1 nb</a></p>

<p><a href="https://forums.fast.ai/t/fastbook-chapter-1-questionnaire-solutions-wiki/65647">Lesson 1 hw questions and solutions</a></p>

<p><strong>Extra</strong></p>

<p><a href="https://forums.fast.ai/t/podcast-writeup-summaries-things-jeremy-says-to-do-qs/66194">Podcast + Writeups: Summaries + Things Jeremy Says to do +
Qs</a></p>

<p><a href="https://forums.fast.ai/t/did-you-do-the-homework/66034">to do types</a></p>

<h2 id="lesson-1-notes">Lesson 1 notes</h2>

<p><strong>Basic training loop</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#alt The basic training loop
</span><span class="n">gv</span><span class="p">(</span><span class="s">'''ordering=in
model[shape=box3d width=1 height=0.7]
inputs-&gt;model-&gt;results; weights-&gt;model; results-&gt;performance
performance-&gt;weights[constraint=false label=update]'''</span><span class="p">)</span>
</code></pre></div></div>

<p>You basically have <code class="language-plaintext highlighter-rouge">inputs-&gt;model-&gt;results</code>. In DL case you have a
system that can learn and update something.</p>

<p>I.e, <code class="language-plaintext highlighter-rouge">inputs-&gt;model-&gt;results-&gt;performance</code>, so some metrics and
updating some weights</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>inputs-&gt;model-&gt;results-&gt;Performance
Weights-&gt;model
Performance--&gt; weights
</code></pre></div></div>

<p>For example in the below work:</p>

<p>Inputs: Images, how many times you look at the image (epoch), number of 
Model: CNN
results: trained model to find cat and dog with high probability
Performance: error_rate/accuracy of validation data set
Weights: tuning <code class="language-plaintext highlighter-rouge">parameter</code> values in the cnn model</p>

<p><strong>What does this do?</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># CLICK ME
</span><span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="p">.</span><span class="n">PETS</span><span class="p">)</span><span class="o">/</span><span class="s">'images'</span>

<span class="k">def</span> <span class="nf">is_cat</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">isupper</span><span class="p">()</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">ImageDataLoaders</span><span class="p">.</span><span class="n">from_name_func</span><span class="p">(</span>
    <span class="n">path</span><span class="p">,</span> <span class="n">get_image_files</span><span class="p">(</span><span class="n">path</span><span class="p">),</span> <span class="n">valid_pct</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
    <span class="n">label_func</span><span class="o">=</span><span class="n">is_cat</span><span class="p">,</span> <span class="n">item_tfms</span><span class="o">=</span><span class="n">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">))</span>

<span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet34</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">error_rate</span><span class="p">)</span>
<span class="n">learn</span><span class="p">.</span><span class="n">fine_tune</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>ImageDataLoaders</strong></p>

<p>There are various different classes for different kinds of deep
learning datasets and problems‚Äîhere we‚Äôre using ImageDataLoaders. <em>The
first part of the class name will generally be the type of data you
have, such as image, or text</em>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dls = ImageDataLoaders.from_name_func(
path, get_image_files(path), valid_pct=0.2, seed=42,
	label_func=is_cat, item_tfms=Resize(224))
</code></pre></div></div>

<p><strong>Convolution Neural Networks</strong></p>

<p>The fifth line of the code training our image recognizer tells fastai
to create a convolution neural network (CNN) and specifies what
architecture to use (i.e. what kind of model to create), what data we
want to train it on, and what metric to use.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>learn = cnn_learner(dls, resnet34, metrics=error_rate)
</code></pre></div></div>

<p>There are many different architectures in fastai, which we will
introduce in this book (as well as discussing how to create your
own). Most of the time, however, <strong>picking an architecture isn‚Äôt a
very important part of the deep learning process</strong>. It‚Äôs something
that academics love to talk about, but in practice it is unlikely to
be something you need to spend much time on. <strong>There are some standard
architectures that work most of the time</strong>, and in this case we‚Äôre
using one called <strong>ResNet</strong> that we‚Äôll be talking a lot about during
the book; it is both <strong>fast and accurate for many datasets and
problems</strong>. The <strong>34 in resnet34 refers to the number of layers</strong> in
this variant of the architecture (other options are 18, 50, 101, and
152). Models using architectures with more layers take longer to
train, and are more prone to overfitting (i.e. you can‚Äôt train them
for as many epochs before the accuracy on the validation set starts
getting worse). On the other hand, when using more data, they can be
quite a bit more accurate.</p>

<p>cnn_learner also has a parameter <em>pretrained</em>, which defaults to True
(so it‚Äôs used in this case, even though we haven‚Äôt specified it),
which sets the weights in your model to values that have already been
trained by experts to recognize a thousand different categories across
1.3 million photos (using the famous ImageNet dataset). A model that
has weights that have already been trained on some other dataset is
called a pretrained model.</p>

<p><strong>fine_tune</strong></p>

<p><code class="language-plaintext highlighter-rouge">cnn_learner</code> only describes the ‚Äúarchitecture‚Äù,</p>

<p>A transfer learning technique where the parameters of a pretrained
model are updated by training for additional epochs using a different
task to that used for pretraining.</p>

<p>So you train for additional epochs (number of times you look at the
each image)</p>

<p>I dont‚Äô get this part:</p>

<p><strong>Don‚Äôt get this part (Later!)</strong></p>

<p>When you use the <code class="language-plaintext highlighter-rouge">fine_tune</code> method, fastai will use these tricks for you. There are a few parameters you can set (which we‚Äôll discuss later), but in the default form shown here, it does two steps:</p>

<ol>
  <li>Use one epoch to fit just those parts of the model necessary to get the new random head to work correctly with your dataset.</li>
  <li>Use the number of epochs requested when calling the method to fit the entire model, updating the weights of the later layers (especially the head) faster than the earlier layers (which, as we‚Äôll see, generally don‚Äôt require many changes from the pretrained weights).</li>
</ol>

<p>The <em>head</em> of a model is the part that is newly added to be specific to the new dataset. An <em>epoch</em> is one complete pass through the dataset. After calling <code class="language-plaintext highlighter-rouge">fit</code>, the results after each epoch are printed, showing the epoch number, the training and validation set losses (the ‚Äúmeasure of performance‚Äù used for training the model), and any <em>metrics</em> you‚Äôve requested (error rate, in this case).</p>

<p><strong>Note on resnet34</strong></p>

<p>The 34 in resnet34 refers to the number of layers in this variant of
the architecture (other options are <strong>18, 50, 101, and 152</strong>). Models
using architectures with more layers take longer to train, and are
more prone to overfitting (i.e. you can‚Äôt train them for as many
epochs before the accuracy on the validation set starts getting
worse). On the other hand, when using more data, they can be quite a
bit more accurate.</p>

<h2 id="lesson-2-hw-questions-and-answers">Lesson 2 HW questions and answers</h2>

<p><a href="https://forums.fast.ai/t/fastbook-chapter-1-questionnaire-solutions-wiki/65647">Answers</a></p>

<ol>
  <li>
    <p>Do you need these for deep learning?</p>

    <ul>
      <li>Lots of math F</li>
      <li>Lots of data T</li>
      <li>Lots of expensive computers F</li>
      <li>A PhD F</li>
    </ul>
  </li>
  <li>
    <p>Name five areas where deep learning is now the best in the world.</p>

    <ul>
      <li>
        <p>Natural language processing (NLP):: Answering questions; speech
recognition; summarizing documents; classifying documents; finding
names, dates, etc. in documents; searching for articles mentioning a
concept</p>
      </li>
      <li>
        <p>Computer vision:: Satellite and drone imagery interpretation
  (e.g., for disaster resilience); face recognition; image
  captioning; reading traffic signs; locating pedestrians and
  vehicles in autonomous vehicles</p>
      </li>
      <li>
        <p>Medicine:: Finding anomalies in radiology images, including CT,
MRI, and X-ray images; counting features in pathology slides;
measuring features in ultrasounds; diagnosing diabetic
retinopathy</p>
      </li>
      <li>
        <p>Biology:: Folding proteins; classifying proteins; many genomics
tasks, such as tumor-normal sequencing and classifying
clinically actionable genetic mutations; cell classification;
analyzing protein/protein interactions</p>
      </li>
      <li>
        <p>Image generation:: Colorizing images; increasing image
resolution; removing noise from images; converting images to art
in the style of famous artists</p>
      </li>
      <li>
        <p>Recommendation systems:: Web search; product recommendations;
home page layout</p>
      </li>
      <li>
        <p>Playing games:: Chess, Go, most Atari video games, and many
real-time strategy games</p>
      </li>
      <li>
        <p>Robotics:: Handling objects that are challenging to locate
(e.g., transparent, shiny, lacking texture) or hard to pick up</p>
      </li>
      <li>
        <p>Other applications:: Financial and logistical forecasting, text
  to speech, and much more‚Ä¶</p>
      </li>
    </ul>
  </li>
  <li>
    <p>What was the name of the first device that was based on the
principle of the artificial neuron?</p>

    <p>Mark I Perceptron</p>
  </li>
  <li>
    <p>Based on the book of the same name, what are the requirements for
parallel distributed processing (PDP)?</p>

    <ol>
      <li>A set of <em>processing units</em></li>
      <li>A <em>state of activation</em></li>
      <li>An <em>output function</em> for each unit</li>
      <li>A <em>pattern of connectivity</em> among units</li>
      <li>A <em>propagation rule</em> for propagating patterns of activities
through the network of connectivities</li>
      <li>An <em>activation rule</em> for combining the inputs impinging on a
unit with the current state of that unit to produce an output
for the unit</li>
      <li>A <em>learning rule</em> whereby patterns of connectivity are modified by experience</li>
      <li>An <em>environment</em> within which the system must operate</li>
    </ol>
  </li>
  <li>
    <p>What were the two theoretical misunderstandings that held back the
field of neural networks?</p>

    <ul>
      <li>NN could not handle functions like XOR</li>
      <li>NN is slow and too big to be useful</li>
    </ul>
  </li>
  <li>
    <p>What is a GPU?</p>

    <p>GPUS can run NN 100 times faster</p>

    <p>Graphics Processing Unit</p>
  </li>
  <li>
    <p>Open a notebook and execute a cell containing: <code class="language-plaintext highlighter-rouge">1+1</code>. What happens?</p>

    <p>2</p>
  </li>
  <li>
    <p>Follow through each cell of the stripped version of the notebook
for this chapter. Before executing each cell, guess what will
happen.</p>

    <p>Documentation can be reached by <code class="language-plaintext highlighter-rouge">doc(PILImage.create)</code> for
example.7</p>

    <p>didn‚Äôt understand how to understand PILImage‚Ä¶ but moving
on‚Ä¶ Let‚Äôs try some motherfucking stuff.</p>
  </li>
  <li>
    <p>Complete the Jupyter Notebook online <a href="https://github.com/fastai/fastbook/blob/master/app_jupyter.ipynb">appendix</a>.</p>

    <p>done.</p>
  </li>
  <li>
    <p>Why is it hard to use a traditional computer program to recognize
images in a photo?</p>

    <blockquote>
      <p>This is very difficult because cats, dogs, or other objects,
have a wide variety of shapes, textures, colors, and other
features, and it is close to impossible to manually encode this
in a traditional computer program.</p>
    </blockquote>

    <p>No simple heuristics that you can come up with for the whole
general problem.</p>
  </li>
  <li>
    <p>What did Samuel mean by ‚Äúweight assignment‚Äù?</p>

    <p>how much each of the million parameters weigh!</p>

    <p>‚Äúweight assignment‚Äù refers to the current values of the model parameters</p>
  </li>
  <li>
    <p>What term do we normally use in deep learning for what Samuel
called ‚Äúweights‚Äù?</p>

    <p>parameters</p>
  </li>
  <li>
    <p>Draw a picture that summarizes Samuel‚Äôs view of a machine learning
model.</p>

    <p>inputs + parameters ‚Äì&gt; architechutre ‚Äì&gt; predictions</p>

    <p>predictions + labels ‚Äì&gt; loss</p>

    <p>loss ‚Äì&gt; update</p>
  </li>
  <li>
    <p>Why is it hard to understand why a deep learning model makes a
particular prediction?</p>

    <p>because we are using an general function that can solve in theory everything</p>
  </li>
  <li>
    <p>What is the name of the theorem that shows that a neural network
can solve any mathematical problem to any level of accuracy?</p>

    <p>universal approximation theorem</p>
  </li>
  <li>
    <p>What do you need in order to train a model?</p>

    <p>metric
input
program
initial weights? pre-trained model</p>
  </li>
  <li>
    <p>How could a feedback loop impact the rollout of a predictive
policing model?</p>

    <p>You end up predicting with biases. You start with predicting
arrests based on the data that already exists. You are not
‚Äúpredicting crime‚Äù, but where ‚Äúarrests‚Äù are going to be made.</p>

    <p>Arrests are different from if crime took place‚Ä¶ Arrests are a
proxy. You are only as good as your proxy.</p>
  </li>
  <li>
    <p>Do we always have to use 224√ó224-pixel images with the cat
recognition model?</p>

    <p>Nope we can use what ever the hell we want.</p>
  </li>
  <li>
    <p>What is the difference between classification and regression?</p>

    <blockquote>
      <p>Note: Classification and Regression: classification and regression
have very specific meanings in machine learning. These are the two
main types of model that we will be investigating in this book. A
classification model is one which attempts to predict a class, or
category. That is, it‚Äôs predicting from a number of discrete
possibilities, such as ‚Äúdog‚Äù or ‚Äúcat.‚Äù A regression model is one
which attempts to predict one or more numeric quantities, such as a
temperature or a location. Sometimes people use the word regression
to refer to a particular kind of model called a linear regression
model; this is a bad practice, and we won‚Äôt be using that
terminology in this book!</p>
    </blockquote>
  </li>
  <li>
    <p>What is a validation set? What is a test set? Why do we need them?</p>

    <p>Validation set to validate the model
Test set to check at the very end to remain intellectually honest.</p>

    <p>Validation set is used to test the hyperparameters by us. So we
see that the training was poor based on the validation set. Then
we just go to varying the hyperparameters such as
architecture. Although the algorithm doesn‚Äôt see it see it, it
actually sees it through us.</p>

    <p>We want to prevent the computer from memorizing.</p>

    <blockquote>
      <p>The solution to this conundrum is to introduce another level of even
more highly reserved data, the test set. Just as we hold back the
validation data from the training process, we must hold back the
test set data even from ourselves. It cannot be used to improve the
model; it can only be used to evaluate the model at the very end of
our efforts. In effect, we define a hierarchy of cuts of our data,
based on how fully we want to hide it from training and modeling
processes: training data is fully exposed, the validation data is
less exposed, and test data is totally hidden. This hierarchy
parallels the different kinds of modeling and evaluation processes
themselves‚Äîthe automatic training process with back propagation, the
more manual process of trying different hyper-parameters between
training sessions, and the assessment of our final result.</p>
    </blockquote>

    <blockquote>
      <p>Having two levels of ‚Äúreserved data‚Äù‚Äîa validation set and a test set,
with one level representing data that you are virtually hiding from
yourself‚Äîmay seem a bit extreme. But the reason it is often necessary
is because models tend to gravitate toward the simplest way to do good
predictions (memorization), and we as fallible humans tend to
gravitate toward fooling ourselves about how well our models are
performing. The discipline of the test set helps us keep ourselves
intellectually honest. That doesn‚Äôt mean we always need a separate
test set‚Äîif you have very little data, you may need to just have a
validation set‚Äîbut generally it‚Äôs best to use one if at all possible.</p>
    </blockquote>
  </li>
  <li>
    <p>What will fastai do if you don‚Äôt provide a validation set?</p>

    <p>20% default</p>
  </li>
  <li>
    <p>Can we always use a random sample for a validation set? Why or why
not?</p>

    <p>No, in time series you might want to use a time time frame in the
past to predict a time frame in the future.</p>

    <p>why?  not representative of most business use cases (where you are
using historical data to build a model for use in the future)</p>
  </li>
  <li>
    <p>What is overfitting? Provide an example.</p>

    <p>Overfitting refers to when the model fits too closely to a limited
set of data but does not generalize well to unseen data.</p>

    <p>When you fit both your validations and training set you are bound
to overfit</p>

    <p>When function is not continuous anymore resulting in spurious values</p>
  </li>
  <li>
    <p>What is a metric? How does it differ from ‚Äúloss‚Äù?</p>

    <p>The concept of a metric may remind you of loss, but there is an
important distinction. The entire purpose of loss is to define a
‚Äúmeasure of performance‚Äù that the training system can use to
update weights automatically. In other words, a good choice for
loss is a choice that is easy for stochastic gradient descent to
use. But a metric is defined for human consumption, so a good
metric is one that is easy for you to understand, and that hews as
closely as possible to what you want the model to do. At times,
you might decide that the loss function is a suitable metric, but
that is not necessarily the case.</p>

    <p>Sound exactly the same to me. I don‚Äôt have an example for when
loss and metric are different. I don‚Äôt know how it would look.</p>

    <p>??</p>
  </li>
  <li>
    <p>How can pretrained models help?</p>

    <blockquote>
      <p>Pretrained models have been trained on other problems that may
be quite similar to the current task. For example, pretrained
image recognition models were often trained on the ImageNet
dataset, which has 1000 classes focused on a lot of different
types of visual objects. Pretrained models are useful because
they have already learned how to handle a lot of simple features
like edge and color detection. However, since the model was
trained for a different task than already used, this model
cannot be used as is.</p>
    </blockquote>
  </li>
  <li>
    <p>What is the ‚Äúhead‚Äù of a model?</p>

    <p>The part that you remove from a pretrained model and then train
your self. The last part.</p>
  </li>
  <li>
    <p>What kinds of features do the early layers of a CNN find? How
about the later layers?</p>

    <blockquote>
      <p>Earlier layers learn simple features like diagonal, horizontal,
and vertical edges. Later layers learn more advanced features
like car wheels, flower petals, and even outlines of animals.</p>
    </blockquote>
  </li>
  <li>
    <p>Are image models only useful for photos?</p>

    <p>no‚Ä¶ sound examples exist or anything that you can convert to image.</p>
  </li>
  <li>
    <p>What is an ‚Äúarchitecture‚Äù?</p>

    <p>template for the mathematical functions</p>
  </li>
  <li>
    <p>What is segmentation?</p>

    <p>Identifying every individual pixel.</p>
  </li>
  <li>
    <p>What is <code class="language-plaintext highlighter-rouge">y_range</code> used for? When do we need it?</p>

    <p>target range‚Ä¶ think of movie reviews.</p>
  </li>
  <li>
    <p>What are ‚Äúhyperparameters‚Äù?</p>

    <p>Knobs we want to tune which are based on the validation set. Sort
of like parameters that will modify the parameters and
simulation.</p>

    <p>network architecture, learning rates, data augmentation
strategies, and other factors</p>

    <blockquote>
      <p>Training models requires various other parameters that define
how the model is trained. For example, we need to define how
long we train for, or what learning rate (how fast the model
parameters are allowed to change) is used. These sorts of
parameters are hyperparameters.</p>
    </blockquote>
  </li>
  <li>
    <p>What‚Äôs the best way to avoid failures when using AI in an
organization?</p>

    <p>Have a training and a validation and a test set.</p>
  </li>
</ol>

<h2 id="experiment-time">Experiment time?</h2>

<p>Experiment time - Most important step according alumni of course
	Signup for bing &amp; extract images
Understand data block api
Build model to perform classification
Interpret confusion matrix
Analyse the results by plotting losses
Exporting model &amp; Inference on any other image of your choice</p>

<h2 id="lesson-2-resources">Lesson 2 Resources</h2>

<p><a href="https://course.fast.ai/videos/?lesson=2">Lesson 2 video course</a></p>

<p><a href="https://course.fast.ai/videos/?lesson=3">Lesson 3 video course</a></p>

<p><a href="https://forums.fast.ai/t/lesson-2-official-topic/66294">Lesson 2 Official Topic 2020</a></p>

<p><a href="https://github.com/fastai/fastbook/blob/master/02_production.ipynb">Fastbook chapter 2</a></p>

<p><a href="https://nth44aap.gradient.paperspace.com/notebooks/fastbook/clean/02_production.ipynb">My paperspace lesson 2 nb</a></p>

<p><a href="https://forums.fast.ai/t/fastbook-chapter-2-questionnaire-solutions-wiki/66392">Lesson 2 hw questions and solutions</a></p>

<p><a href="https://github.com/fastai/bear_voila">Bear_classifier</a></p>

<p><a href="https://www.taniarascia.com/how-to-create-and-use-bash-scripts/">How to make a bash file</a></p>

<p><a href="https://mybinder.org/v2/gh/tkravichandran/First-DL-Classifier/main?urlpath=voila%2Frender%2Fobama_classifier_min_binder.ipynb">my binder app</a></p>

<p><a href="https://forums.fast.ai/t/share-your-v2-projects-here/65757/3">Share your V2 projects here</a></p>

<p><a href="https://forums.fast.ai/t/blog-posts-projects-and-articles/1736">Blogs, projects and other</a></p>

<h3 id="lesson-resources">Lesson resources</h3>

<ul>
  <li><a href="https://youtu.be/BvHmRx14HQ8">Lesson video</a> (private)</li>
  <li><a href="https://youtu.be/BoDwXwZXsDI">Covid-19 masks video</a> (public)</li>
  <li><a href="https://github.com/fastai/fastbook/blob/master/02_production.ipynb">fastbook chapter 2</a></li>
  <li><a href="https://forums.fast.ai/t/fastbook-chapter-2-questionnaire-solutions-wiki/66392">fastbook Chapter 2 questionnaire solutions</a> - feel free to contribute!</li>
  <li><a href="https://forums.fast.ai/t/lesson-2-non-beginner-discussion/66511">Non-beginner discussion</a></li>
  <li><a href="https://forums.fast.ai/t/part-1-2020-weekly-beginner-only-review-and-q-a/66203"> Part 1 (2020) - Weekly Beginner Only Review and Q&amp;A</a></li>
  <li><a href="https://github.com/LauraLangdon/anki">Anki deck for Lessons 1 and 2</a></li>
</ul>

<h3 id="links-from-lesson">Links from lesson</h3>

<ul>
  <li><a href="https://docs.google.com/document/d/1HLrm0pqBN_5bdyysOeoOBX4pt4oFDBhsC_jpblXpNtQ/edit?usp=sharing">Masks research</a></li>
  <li><a href="https://docs.google.com/document/d/1EWpWmyjzM4sNBF-7jp_1Y9a-pqiRg0wakGXy7kj11RA/edit">masks4all</a> - from Czech Republic</li>
  <li><a href="https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108">Publication ‚ÄúThe ASA Statement on <em>p</em> -Values‚Äù</a> &amp; <a href="https://www.fharrell.com/post/nhst-never/">article ‚Äú
Null Hypothesis Significance Testing Never Worked‚Äù</a></li>
  <li><a href="https://www.oreilly.com/radar/drivetrain-approach-data-products/">Designing great data products</a></li>
  <li><a href="https://azure.microsoft.com/en-us/services/cognitive-services/bing-image-search-api/">Bing Image Search API</a></li>
</ul>

<h3 id="deployment-guides">Deployment guides</h3>

<ul>
  <li>
    <p>Deploying the Bear Detector app with Flask + Gunicorn + Domain name
with SSL certificate, plus Dockerizing the app:
https://github.com/fastai/course-v3/blob/master/docs/deployment_flask_gunicorn_docker.md
(just sent pull request to correct missing /docs in link to image
oops)</p>
  </li>
  <li>
    <p>Deployment Platform: <a href="https://seeme.ai">SeeMe.ai - the AI marketplace</a>: Easily
deploy and share your Fastai (v1 and v2) models on the web, mobile,
Python SDK, ‚Ä¶. We have added support for <code class="language-plaintext highlighter-rouge">fastai2</code> and will
continue to support it throughout its development to a stable
version.  We created a <a href="https://github.com/zerotosingularity/seeme-quick-guides/blob/master/seeme-quick-guide-fastai-v2.ipynb"><code class="language-plaintext highlighter-rouge">fastai2</code> deployment quick guide</a>, taking
you through all the steps of training, deploying, using, and sharing
your model with your friends and the world.  Additional links:
<a href="https://forums.fast.ai/t/deployment-platform-seeme-ai/66270/">Deployment platform: Seeme.ai - wiki topic</a> | <a href="https://github.com/zerotosingularity/seeme-quick-guides/blob/master/seeme-quick-guide-fastai-v1.ipynb">Fastai v1 quick
guide</a></p>
  </li>
  <li>
    <p>Deployment Platform: <a href="https://aws.amazon.com/sagemaker/">Amazon SageMaker</a>. Follow the steps
outlined in the <a href="https://forums.fast.ai/t/deployment-platform-amazon-sagemaker/66439">following post</a> to train and deploy your fastai2
model with Amazon SageMaker.</p>
  </li>
  <li>
    <p>Deployment Platform: <a href="https://docs.microsoft.com/azure/azure-functions/">Azure Functions</a>. Follow the steps outlined
in this <a href="https://github.com/gopitk/fastai2deployazure">Github repo</a> to deploy a trained fastai2 model on Azure
cloud‚Äôs serverless infrastructure service ‚Äì the Azure
Functions. Welcome your feedback and questions on this <a href="https://forums.fast.ai/t/deployment-platform-azure-functions/67076">topic
thread</a>.</p>
  </li>
</ul>

<h3 id="deployment-examples">Deployment examples</h3>

<ul>
  <li><a href="https://github.com/muellerzr/fastai2-Starlette">Bear Detector web app</a>: a Starlette example for deployment in
fastai2</li>
  <li><a href="https://github.com/sebderhy/derotate">Derotate</a>: a code template for image-to-class and image-to-image
fastai2 deployments with web app, mobile app (in <a href="https://github.com/flutter/flutter">Flutter</a>), and
<a href="https://github.com/sebderhy/derotate/blob/master/test_api.ipynb">a notebook</a> showing how to call the model remotely as an API.</li>
  <li><a href="https://github.com/javismiles/bear-detector-flask-deploy">Bear detector web app deployed with flask</a>: example of the bear
detector app deployed with fast.ai v2, flask, gunicorn and nginx</li>
</ul>

<h3 id="other-useful-links">Other useful links</h3>

<ul>
  <li><a href="/t/podcast-writeup-summaries-things-jeremy-says-to-do-qs/66194">Lesson Summaries + Things Jeremy Says to do + Qs</a></li>
  <li><a href="https://www.notion.so/Lesson-2-a286cc6495784ccf9917668fd42cb2a0">Notes</a> by @Lankinen</li>
  <li><a href="https://link.springer.com/article/10.1007/s10654-016-0149-3">Statistical tests,  <em>P</em>  values, confidence intervals, and power: a guide to misinterpretations</a></li>
</ul>

<h2 id="lesson-2-notes">Lesson 2 notes</h2>

<p><a href="https://forums.fast.ai/t/getting-the-bing-image-search-key/67417">Getting bing images</a> for the key</p>

<p>To get key once done go <a href="https://azure.microsoft.com/en-us/try/cognitive-services/my-apis/?apiSlug=search-api-v7">here</a> or <a href="https://azure.microsoft.com/en-us/try/cognitive-services/?api=bing-image-search-api">generally</a></p>

<p>Metrics are printed out after we look at every single image ONCE (each
epoch each time you have ‚Äúlooked‚Äù at all the images).</p>

<p>overfitting example needed</p>

<p>loss and metric example needed</p>

<p>loss could be error_rate but in the classification problem, a small
change in parameters won‚Äôt result in change in classification from dog
to cat. so we need something else other than <code class="language-plaintext highlighter-rouge">error_rate</code>. and we
continue to use metrics to understand overfitting.</p>

<p><strong>Overfitting</strong></p>

<p>Overfitting is measured by looking at the METRICS that come out of the
validation set. The fastai program doesn‚Äôt see the validation set for
‚Äútraining‚Äù. training for data that hasn‚Äôt seen before is the objective</p>

<p>Don‚Äôt want to memorize</p>

<p>You still end up cheating/overfitting as you look at the metrics
(produced by the validation data) and change the <code class="language-plaintext highlighter-rouge">hyperparameters</code>.</p>

<p>Something about training loss validation loss and metric decrease and
how to identify if you are overtraining‚Ä¶ din‚Äôt quite get that <code class="language-plaintext highlighter-rouge">15:56</code>?</p>

<p><strong>Fine tuning unclear?</strong></p>

<p>is unclear what he means that first you use one epoch and then you use</p>

<blockquote>
  <p>When you use the fine_tune method, fastai will use these tricks for
you. There are a few parameters you can set (which we‚Äôll discuss
later), but in the default form shown here, it does two steps:</p>
</blockquote>

<blockquote>
  <p>Use one epoch to fit just those parts of the model necessary to get
the new random head to work correctly with your dataset.  Use the
number of epochs requested when calling the method to fit the entire
model, updating the weights of the later layers (especially the
head) faster than the earlier layers (which, as we‚Äôll see, generally
don‚Äôt require many changes from the pretrained weights).</p>
</blockquote>

<p><strong>transfer learning</strong></p>

<p>2012 imagenet winners using visualization Zeiler and Fergus</p>

<p><strong>Applications</strong></p>

<p>vision
tabular
text
recsys</p>

<p><strong>Where to look</strong></p>

<p>Modelzoo
pretrained model deep learning
imagenet</p>

<p>Medical images not there</p>

<p><strong>Read paper</strong></p>

<p>High temperature and High Humidity reduce the transmission of covid-19</p>

<p>check out blog post on how to do machine learning how to ask the right
questions <code class="language-plaintext highlighter-rouge">1:08:00</code></p>

<p><strong>Brilliant</strong></p>

<p>Jeremy uses p-value to discredit a paper. Someone predicts a trend
between temperature and R (a measure of covid transmicsibility) by
plotting for 100 cities and shows the <strong>situation</strong> <code class="language-plaintext highlighter-rouge">52:00</code>.</p>

<p>Task being if this happened by chance? Of course there is the whole
causation correlation discussion. <em>But this chance thing is extremely
interesting.</em></p>

<p>Jeremy takes a similar std deviation and a mean and randomly plots 100
cities and see what sort of R and temp he gets. Unrealistically he
then ends up discrediting the paper. I think what would be nice is a
p-value of many such simulations and where the prediction stands among
a null hypothesis. :) GREAT.</p>

<p>Here he also effectively tests what happens when you look at only 100
cities.</p>

<p>But the american statistical association <strong>hates p-values</strong>. Be
<strong>Careful</strong> while using them? What was the conclusion? <code class="language-plaintext highlighter-rouge">57:59</code></p>

<p>One of the <strong>Claim</strong>: more data you use the p value becomes smaller!
so don‚Äôt use threshold?</p>

<p>Jeremy <strong>suggests</strong> that a multivariate model i.e., transmission
dependent on gdp, density of cities and temperature and then you have
a p-value of 0.01</p>

<p><strong>Check out work by Frank Harell about p-values</strong></p>

<p><strong>Bing Image Search API</strong></p>

<p>Gives info such as URLs of images from the internet which are labeled.</p>

<h2 id="error-upon-running-notebook">Error upon running notebook</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RuntimeError: DataLoader worker (pid 19862) is killed by signal:
Killed.
</code></pre></div></div>

<p>Seems to be a <a href="https://forums.fast.ai/t/lesson-2-official-topic/66294/467?u=thetj09">memory issue</a>. Somehow it switched to only CPU and
the GPU plan got removed. Started a new notebook container.</p>

<p>Somehow the system in gradient changed from P5000 to CPU. I don‚Äôt
understand how though.</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>GPU</th>
      <th>CPU</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Free-P5000</td>
      <td>16GB</td>
      <td>30GB</td>
    </tr>
    <tr>
      <td>Free-GPU</td>
      <td>8GB</td>
      <td>30GB</td>
    </tr>
    <tr>
      <td>Free-cpu</td>
      <td>0GB</td>
      <td>2gGB</td>
    </tr>
    <tr>
      <td>My PC</td>
      <td>4GB</td>
      <td>8GB</td>
    </tr>
  </tbody>
</table>

<p>Follow <a href="https://forums.fast.ai/t/lesson1-issues-with-gradient-instance/77813/3">this issue</a>, apparently it is not supposed to go to
‚ÄúFree-CPU‚Äù. What a cunt program. And there seems to be a guy who
potentially could help from paperspace.</p>

<h2 id="deployment-errors">Deployment errors</h2>

<h3 id="binder-does-not-work">binder does not work</h3>

<p><a href="https://github.com/tkravichandran/bear_voila">forked bear-viola</a> from the main hosters and it didn‚Äôt work with binder
and there was no way to see what the error was.</p>

<h3 id="heroku">Heroku</h3>

<p>With heroku I had to add a <code class="language-plaintext highlighter-rouge">requirement.txt</code>, <code class="language-plaintext highlighter-rouge">runtime.txt</code>
(specifying the right version of python for the ones in the
<code class="language-plaintext highlighter-rouge">requirement.txt</code>). <code class="language-plaintext highlighter-rouge">Procfile</code> allows to add <code class="language-plaintext highlighter-rouge">--debug</code> option which
shows errors.</p>

<p>So, <code class="language-plaintext highlighter-rouge">Procfile</code> <a href="https://github.com/tkravichandran/First-DL-Classifier/commit/a3f01b1081d08c8e335692adaf4d4c9284468a6e#diff-0a99231995da379e7aebabe76c9d849a23737a42c3b3a8994043e2aa80958424">with and without debug</a>:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>web: voila --port=$PORT --no-browser --enable_nbextensions=True obama_classifier_min_binder.ipynb
</code></pre></div></div>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>web: voila --debug --port=$PORT --no-browser --enable_nbextensions=True obama_classifier_min_binder.ipynb
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Requirement.txt</code> now has and seems to need only the following for
lesson 2. Note that <strong>Haroku only allows 950Mb slugs, so we use CPU
version of pytorch</strong> as a result:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
https://download.pytorch.org/whl/cpu/torch-1.6.0%2Bcpu-cp38-cp38-linux_x86_64.whl
https://download.pytorch.org/whl/cpu/torchvision-0.7.0%2Bcpu-cp38-cp38-linux_x86_64.whl
fastai==2.0.11
voila
ipywidgets
</code></pre></div></div>

<p>Lastly we need to specify the exact python version in <code class="language-plaintext highlighter-rouge">runtime.txt</code>
other wise we get the ` not the right wheels` error.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python-3.8.5
</code></pre></div></div>

<p>Most useful links were that of the <a href="https://course.fast.ai/deployment_heroku">old fast ai deployment on haroku
guide</a> and the link of someone for who this worked aka <a href="https://github.com/mesw/whatgame3">whatgame</a>.</p>

<p><strong>Useful links</strong></p>

<p><a href="https://forums.fast.ai/t/deploying-in-heroku-error-not-a-supported-wheel-on-this-platform/76196">Link 1</a></p>

<p><a href="https://forums.fast.ai/t/anyone-using-heroku-to-deploy-a-fastai2-model/79070">Link 2</a></p>

<p><a href="https://github.com/mahtabsyed/PyTorch-fastaiv2-bears-classification">Other github repos</a></p>

<p><a href="https://github.com/mesw/whatgame3">Other github repos from which I copied ideas</a></p>

<p><a href="https://github.com/tkravichandran/First-DL-Classifier">Successful deployment by agent</a></p>

<h2 id="writing-the-blog-lessons-fastai">Writing the blog lessons fastai</h2>

<p>I am thinking I should start with the basics of data‚Ä¶. how I clean,
how I do other things and then finish with the app. Basically take
people through the entire journey of development.</p>

<p>More info about <a href="https://github.com/fastai/fastpages#customizing-blog-posts-with-front-matter">fast yaml front matter is here</a></p>

<h2 id="lesson-2-hw-questions-and-answers-1">Lesson 2 HW questions and answers</h2>

<p><a href="https://quizlet.com/512143734/fastai-chapter-2-questionnaire-flash-cards/?x=1jqt">Flashcards of Lesson 2 questions and answers</a></p>

<ol>
  <li>
    <p>Provide an example of where the bear classification model might
work poorly in production, due to structural or style differences
in the training data.</p>

    <ul>
      <li>Say black and white images</li>
      <li>bear hiding behind tree</li>
      <li>bear and people perhaps?</li>
      <li>polar bear perhaps</li>
      <li>hand drawn bears</li>
      <li>night time images</li>
      <li>small in the picture bear?</li>
    </ul>
  </li>
  <li>
    <p>Where do text models currently have a major deficiency?</p>

    <blockquote>
      <p>Deep learning is also very good at generating
context-appropriate text, such as replies to social media posts,
and imitating a particular author‚Äôs style. It‚Äôs good at making
this content compelling to humans too‚Äîin fact, even more
compelling than human-generated text. However, deep learning is
currently not good at generating correct responses! We don‚Äôt
currently have a reliable way to, for instance, combine a
knowledge base of medical information with a deep learning model
for generating medically correct natural language
responses. This is very dangerous, because it is so easy to
create content that appears to a layman to be compelling, but
actually is entirely incorrect.</p>
    </blockquote>
  </li>
  <li>
    <p>What are possible negative societal implications of text generation
models?</p>

    <blockquote>
      <p>Another concern is that context-appropriate, highly compelling
responses on social media could be used at massive
scale‚Äîthousands of times greater than any troll farm previously
seen‚Äîto spread disinformation, create unrest, and encourage
conflict. (fake news)</p>
    </blockquote>
  </li>
  <li>
    <p>In situations where a model might make mistakes, and those mistakes
could be harmful, what is a good alternative to automating a
process?</p>

    <p>Regular interventions to check by human pros. E.g., medical
diagnoses</p>

    <p>For example, a machine learning model for identifying strokes in CT
scans can alert high priority cases for expedited review, while
other cases are still sent to radiologists for review</p>
  </li>
  <li>
    <p>What kind of tabular data is deep learning particularly good at?</p>

    <blockquote>
      <p>Deep learning is good at analyzing tabular data that includes
natural language, or high cardinality categorical columns
(i.e. Zip codes)</p>
    </blockquote>
  </li>
  <li>
    <p>What‚Äôs a key downside of directly using a deep learning model for
recommendation systems?</p>

    <blockquote>
      <p>Machine learning approaches for recommendation systems will
often only tell what products a user might like, and may not be
recommendations that would be helpful to the user. For example,
if a user is familiar with other books from the same author, it
isn‚Äôt helpful to recommend those products even though the user
bought the author‚Äôs book. Or, recommending products a user may
have already purchased.</p>
    </blockquote>

    <p>After deep learning the model knows what X has bought in the
 past. But that doesn‚Äôt mean X will buy that or similar stuff like
 (boxed set of Harry potter novels).</p>
  </li>
  <li>
    <p>What are the steps of the Drivetrain Approach?</p>

    <p>Objective ‚Äì&gt; Levers ‚Äì&gt; DATA ‚Äì&gt; Models</p>

    <p>Show most relevant search ‚Äì&gt; rankings of search results ‚Äì&gt; get
 data on linked sites ‚Äì&gt; build model</p>

    <p>Vague‚Ä¶ move on</p>
  </li>
  <li>
    <p>How do the steps of the Drivetrain Approach map to a recommendation
system?</p>

    <blockquote>
      <p>The objective of a recommendation engine is to drive additional
sales by surprising and delighting the customer with
recommendations of items they would not have purchased without
the recommendation. The lever is the ranking of the
recommendations. New data must be collected to generate
recommendations that will cause new sales . This will require
conducting many randomized experiments in order to collect data
about a wide range of recommendations for a wide range of
customers. This is a step that few organizations take; but
without it, you don‚Äôt have the information you need to actually
optimize recommendations based on your true objective (more
sales!)</p>
    </blockquote>

    <p>Vague‚Ä¶ Move on</p>
  </li>
  <li>
    <p>Create an image recognition model using data you curate, and deploy
it on the web.</p>

    <p>done.</p>
  </li>
  <li>
    <p>What is <code class="language-plaintext highlighter-rouge">DataLoaders</code>?</p>

    <blockquote>
      <p>DataLoaders is a thin class that just stores whatever DataLoader
objects you pass to it, and makes them available as train and
valid. Although it‚Äôs a very simple class, it‚Äôs very important in
fastai: it provides the data for your model.</p>
    </blockquote>
  </li>
  <li>
    <p>What four things do we need to tell fastai to create
<code class="language-plaintext highlighter-rouge">DataLoaders</code>?</p>

    <blockquote>
      <ul>
        <li>What kinds of data we are working with</li>
        <li>How to get the list of items</li>
        <li>How to label these items</li>
        <li>How to create the validation set</li>
      </ul>
    </blockquote>
  </li>
  <li>
    <p>What does the <code class="language-plaintext highlighter-rouge">splitter</code> parameter to <code class="language-plaintext highlighter-rouge">DataBlock</code> do?</p>

    <p>splits data to valid and train.</p>
  </li>
  <li>
    <p>How do we ensure a random split always gives the same validation
set?</p>

    <p>seed in Data loaders or split ahead of time.</p>
  </li>
  <li>
    <p>What letters are often used to signify the independent and
dependent variables?</p>

    <p>x for independent and y for dependent</p>

    <blockquote>
      <p>The independent variable is the thing we are using to make
predictions from, and the dependent variable is our target. In
this case, our independent variables are images, and our
dependent variables are the categories (type of bear) for each
image</p>
    </blockquote>
  </li>
  <li>
    <p>What‚Äôs the difference between the crop, pad, and squish resize
approaches? When might you choose one over the others?</p>

    <p>crops, pads adds pad, squish compresses image. random crop is
preferred as all the information is still available at each epoch.</p>
  </li>
  <li>
    <p>What is data augmentation? Why is it needed?</p>

    <p>To account for rotations in the image, change in contrast, black
and white‚Ä¶ better result on validity.</p>
  </li>
  <li>
    <p>What is the difference between <code class="language-plaintext highlighter-rouge">item_tfms</code> and <code class="language-plaintext highlighter-rouge">batch_tfms</code>?</p>

    <p>One by one and the other one are transforms to be performed on a batch.</p>
  </li>
  <li>
    <p>What is a confusion matrix?</p>

    <blockquote>
      <p>let‚Äôs see whether the mistakes the model is making are mainly
thinking that grizzlies are teddies (that would be bad for
safety!), or that grizzlies are black bears, or something else</p>
    </blockquote>

    <p>It is calculated (Of course) based on the valid set.</p>
  </li>
  <li>
    <p>What does <code class="language-plaintext highlighter-rouge">export</code> save?</p>

    <p>pickle file i.e., model readily usable</p>

    <blockquote>
      <p>export saves both the architecture, as well as the trained
parameters of the neural network architecture. It also saves
how the DataLoaders are defined.</p>
    </blockquote>
  </li>
  <li>
    <p>What is it called when we use a model for getting predictions,
instead of training?</p>

    <p>Inference</p>
  </li>
  <li>
    <p>What are IPython widgets?</p>

    <p>widgets such as upload button.</p>
  </li>
  <li>
    <p>When might you want to use CPU for deployment? When might GPU be
better?</p>

    <p>CPU for running on one image (deployment)</p>

    <p>GPU for running on multiple images (aka modelling)</p>
  </li>
  <li>
    <p>What are the downsides of deploying your app to a server, instead
of to a client (or edge) device such as a phone or PC?</p>

    <blockquote>
      <p>The application will require network connection, and there will
be extra network latency time when submitting input and
returning results. Additionally, sending private data to a
network server can lead to security concerns.</p>
    </blockquote>
  </li>
  <li>
    <p>What are three examples of problems that could occur when rolling
out a bear warning system in practice?</p>

    <ul>
      <li>false positives
        <ul>
          <li>unseen data</li>
        </ul>
      </li>
      <li>unknown regions (night time imaging not good)</li>
      <li>speed of prediction</li>
      <li>resolution of images</li>
    </ul>
  </li>
  <li>
    <p>What is ‚Äúout-of-domain data‚Äù?</p>

    <blockquote>
      <p>Data that is fundamentally different in some aspect compared to
the model‚Äôs training data. For example, an object detector that
was trained exclusively with outside daytime photos is given a
photo taken at night.</p>
    </blockquote>
  </li>
  <li>
    <p>What is ‚Äúdomain shift‚Äù?</p>

    <blockquote>
      <p>This is when the type of data changes gradually over time. For
example, an insurance company is using a deep learning model as
part of their pricing algorithm, but over time their customers
will be different, with the original training data not being
representative of current data, and the deep learning model
being applied on effectively out-of-domain data.</p>
    </blockquote>
  </li>
  <li>
    <p>What are the three steps in the deployment process?</p>

    <p>Manual process
limited scope
gradual expansion</p>
  </li>
</ol>

<h2 id="hacks-documentation">Hacks Documentation</h2>

<p>https://stackoverflow.com/a/64708015/5986651</p>

<p>All things should have <strong>module</strong> attributes usually.</p>

<p><strong>Find info on</strong></p>

<p>Use <code class="language-plaintext highlighter-rouge">function.__module__</code> to know where it comes from. Use <code class="language-plaintext highlighter-rouge">?</code> and
<code class="language-plaintext highlighter-rouge">??</code> to see the code. And <code class="language-plaintext highlighter-rouge">doc()</code> to see what is happening with the</p>

<p><code class="language-plaintext highlighter-rouge">first?</code>, <code class="language-plaintext highlighter-rouge">first??</code> pandain le‚Ä¶</p>

<blockquote>
  <p>In python, <code class="language-plaintext highlighter-rouge">__module__</code> is for class var, <code class="language-plaintext highlighter-rouge">requires_grad_</code> is a member
method of torch.Tensor(), it is not a class instance, so it has no
<strong>module</strong> attr.</p>
</blockquote>

<h2 id="other-lesson-2-resources">Other lesson 2 Resources</h2>

<ul>
  <li>
    <p><a href="https://youtu.be/BvHmRx14HQ8?t=4485">What Jeremy says</a></p>
  </li>
  <li>
    <p>Example <a href="https://forums.fast.ai/t/fastai2-blog-posts-projects-and-tutorials/65827">blogs, projects and tutorial</a></p>
  </li>
  <li>
    <p><a href="https://forums.fast.ai/t/official-project-group-thread/65817">Project Group thread</a></p>
  </li>
  <li>
    <p><a href="https://forums.fast.ai/t/official-project-group-thread/65817">Official 2020 Project group thread</a></p>
  </li>
  <li>
    <p>Lesson 1 TOP examples</p>
  </li>
</ul>

<h2 id="lesson-3-resources">Lesson 3 Resources</h2>

<p><a href="https://course.fast.ai/videos/?lesson=3">Lesson 3 video course</a></p>

<p><a href="https://forums.fast.ai/t/lesson-3-official-topic/67244">Lesson 3 Official Topic 2020</a></p>

<ul>
  <li><a href="https://github.com/fastai/fastbook/blob/master/02_production.ipynb">fastbook chapter 2 </a></li>
  <li>
    <p><a href="https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb">fastbook chapter 4 </a></p>
  </li>
  <li><a href="https://forums.fast.ai/t/fastbook-chapter-2-questionnaire-solutions-wiki/66392">fastbook chapter 2 questionnaire solutions</a></li>
  <li><a href="https://forums.fast.ai/t/fastbook-chapter-4-questionnaire-solutions-wiki/67253">fastbook chapter 4 questionnaire solutions</a></li>
</ul>

<h3 id="lesson-resources-1">Lesson resources</h3>

<ul>
  <li><a href="https://youtu.be/5L3Ao5KuCC4">Video link</a> (private)</li>
  <li><a href="https://github.com/fastai/fastbook/blob/master/02_production.ipynb">fastbook chapter 2 </a></li>
  <li><a href="https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb">fastbook chapter 4 </a></li>
  <li><a href="https://forums.fast.ai/t/lesson-3-non-beginner-discussion/67248">Non-beginner discussion</a></li>
  <li><a href="https://forums.fast.ai/t/fastbook-chapter-2-questionnaire-solutions-wiki/66392">fastbook chapter 2 questionnaire solutions</a></li>
  <li><a href="https://forums.fast.ai/t/fastbook-chapter-4-questionnaire-solutions-wiki/67253">fastbook chapter 4 questionnaire solutions</a> - feel free to contribute!</li>
</ul>

<h3 id="links-from-lesson-1">Links from lesson</h3>

<ul>
  <li><a href="https://masks4all.co/">#mask4all</a></li>
  <li><a href="https://www.washingtonpost.com/outlook/2020/03/28/masks-all-coronavirus/">Jeremy‚Äôs Washington Post article</a></li>
  <li><a href="https://www.oreilly.com/library/view/building-machine-learning/9781492045106/">Building Machine Learning Powered Applications</a> - by Emmanuel Ameisen</li>
  <li><a href="https://dam-prod.media.mit.edu/x/2019/01/24/AIES-19_paper_223.pdf">Actionable Auditing</a> - by Deborah Raji</li>
</ul>

<h3 id="other-useful-links-1">Other useful links</h3>

<p><a href="https://www.notion.so/Lesson-3-bce5575977c34c4db470f468bddb7c69">Notes</a> by @Lankinen</p>

<h2 id="lesson-3-notes">Lesson 3 Notes</h2>

<ol>
  <li>
    <p>Look at data properly e.g., looking at ‚Äúgood skin‚Äù and getting
white people touching their faces</p>
  </li>
  <li>
    <p>Always create a baseline</p>
  </li>
  <li></li>
</ol>

<h3 id="pytorch-gradient">pytorch gradient</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>xt = tensor([3.,4.,10.]).requires_grad_()
yt=f(xt) #some function say def f(x): return x**2

yt.backward() # makes the gradient
xt.grad() # accesses the gradient at points in xt.
</code></pre></div></div>

<p>Cat is for concatenation, view is for reshaping. <code class="language-plaintext highlighter-rouge">-1</code> is for the
function to figure out the dimension.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Unsqueeze</code> is for redimensioning ‚Äúvector‚Äù into a a tensor of
different dimensions such as <code class="language-plaintext highlighter-rouge">2000,1</code> or <code class="language-plaintext highlighter-rouge">1,2000</code> from just <code class="language-plaintext highlighter-rouge">2000</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)
</code></pre></div></div>

<p>Getting a tuple from <code class="language-plaintext highlighter-rouge">image+label</code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dset = list(zip(train_x,train_y))
x,y = dset[0]
x.shape,y
</code></pre></div></div>

<h2 id="todo">Todo</h2>

<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />run and understand the cells</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />do hw cosins
    <h2 id="lesson-3-hw-questions-and-answers">Lesson 3 HW questions and answers</h2>
  </li>
</ul>

<h3 id="coming-days">Coming days</h3>

<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Lecture 3 video</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />start with application notebook</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />deploy application</li>
  <li class="task-list-item">
    <p><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />think of where it can be used in my current setting</p>
  </li>
  <li class="task-list-item">
    <p><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Do the questions</p>

    <ul class="task-list">
      <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />chapter 2</li>
      <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />chapter 3??</li>
    </ul>
  </li>
</ul>

<h2 id="lesson-4">Lesson 4</h2>

<p><a href="https://forums.fast.ai/t/lesson-4-official-topic/68643">Official topic</a></p>

<p>This is lesson 4, which is week 5 since last week was out of order.</p>

<p>Note: This is a wiki post - feel free to edit to add links from the lesson or other useful info.</p>

<h2 id="resources-1">Resources</h2>

<ul>
  <li><a href="https://youtu.be/p50s63nPq9I">Video</a></li>
  <li><a href="https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb">fastbook chapter 4</a></li>
  <li><a href="https://forums.fast.ai/t/fastbook-chapter-4-questionnaire-solutions-wiki/67253">fastbook chapter 4 questionnaire solutions</a></li>
  <li><a href="https://github.com/fastai/fastbook/blob/master/05_pet_breeds.ipynb">fastbook chapter 5</a></li>
  <li><a href="https://forums.fast.ai/t/fastbook-chapter-5-questionnaire-solutions-wiki/69301">fastbook chapter 5 questionnaire solutions</a></li>
  <li><a href="https://forums.fast.ai/t/lesson-4-non-beginner-discussion/68662">Non-beginner discussion</a></li>
</ul>

<h2 id="links-from-lesson-2">Links from lesson</h2>
<ul>
  <li><a href="https://www.preprints.org/manuscript/202004.0203/v1">Face Masks Against COVID-19: An Evidence Review</a></li>
</ul>

<h2 id="other-useful-links-2">Other useful links</h2>
<p><a href="https://www.notion.so/lankinen/Lesson-4-97aa1415857d48f9a1c8a0de70d71a06">Notes</a> by @Lankinen</p>

<h3 id="python-r-summary-equivalent">python R summary equivalent</h3>

<p>Without pandas:</p>

<p>from scipy import stats</p>

<p>stats.describe(lst)
stats.scoreatpercentile(lst,(5,10,50,90,95))</p>

<p>Here is an example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">stdev</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">mu</span> <span class="o">=</span>  <span class="mi">10</span>
<span class="n">a</span><span class="o">=</span><span class="n">stdev</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">+</span><span class="n">mu</span>
<span class="n">stats</span><span class="p">.</span><span class="n">describe</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[OUT1]: DescribeResult(nobs=100, minmax=(-13.180682481878286, 40.6109521437826), mean=10.352380786199149, variance=103.27168865119998, skewness=0.13852516641657087, kurtosis=0.2691915766145532)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stats</span><span class="p">.</span><span class="n">scoreatpercentile</span><span class="p">(</span><span class="n">a</span><span class="p">,(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">90</span><span class="p">,</span><span class="mi">95</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[OUT2]: array([-7.21731609, -3.22696662, 10.39364637, 21.78527621, 24.20685179])
</code></pre></div></div>
<h3 id="plot-tensor">plot tensor</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>im3_t = tensor(im3)
df = pd.DataFrame(im3_t[4:15,4:22])
df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')
</code></pre></div></div>

<h3 id="list-zip-combine-two-tensors">list zip; combine two tensors</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dset</span><span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span><span class="n">train_y</span><span class="p">))</span>
<span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">dset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span><span class="n">y</span>

<span class="p">[</span><span class="n">Out1</span><span class="p">]:</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">784</span><span class="p">]),</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
</code></pre></div></div>

<h3 id="generate-quick-tuples">generate quick tuples</h3>

<p>https://discuss.pytorch.org/t/leaf-variable-was-used-in-an-inplace-operation/308</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ds</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">string</span><span class="p">.</span><span class="n">ascii_lowercase</span><span class="p">))</span>
<span class="n">ds</span>
</code></pre></div></div>

<h3 id="make-a-quick-list">make a quick list</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">random</span>
<span class="n">my_randoms</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>
<h3 id="leaf-variable-non-leaf-variable">leaf variable non-leaf variable</h3>

<p>Loosely, tensors you create directly are leaf variables. Tensors that are the result of a differentiable operation are not leaf variables</p>

<p>For example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span> <span class="c1"># leaf variable
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># also leaf
</span><span class="n">variable</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># not a leaf variable
</span></code></pre></div></div>

<p>An in-place operation is something which modifies the data of a
variable. For example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># in-place
</span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># not in place
</span></code></pre></div></div>

<p>PyTorch doesn‚Äôt allow in-place operations on leaf variables  that have
<code class="language-plaintext highlighter-rouge">requires_grad=True</code> (such as parameters of your model) because the
developers could not decide how such an operation should behave. If
you want the operation to be differentiable, you can work around the
limitation by cloning the leaf variable (or use a non-inplace version
of the operator).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">clone</span><span class="p">()</span>  <span class="c1"># clone the variable
</span><span class="n">x2</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># in-place operation
</span></code></pre></div></div>

<h3 id="runtimeerror-leaf-variable">runtimeerror leaf variable</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">RuntimeError</span><span class="p">:</span> <span class="n">a</span> <span class="n">leaf</span> <span class="n">Variable</span> <span class="n">that</span> <span class="n">requires</span> <span class="n">grad</span> <span class="ow">is</span> <span class="n">being</span> <span class="n">used</span> <span class="ow">in</span> <span class="n">an</span>
<span class="ow">in</span><span class="o">-</span><span class="n">place</span> <span class="n">operation</span><span class="p">.</span>
</code></pre></div></div>

<blockquote>
  <p>The new style to express this is to use torch.no_grad() to signify
that you don‚Äôt want to track the gradient of this operation (but
z.data is OK too):</p>
</blockquote>

<h3 id="shuffletrue-not-working-in-dataloader">Shuffle=true not working in dataloader</h3>

<p>I don‚Äôt know what it is but in lesson 4, if I run with shuffle =true I
get validation results of 50% meaning somewhere things are lost in
translation. i.e., the correlation between pictures and their labels
don‚Äôt exist anymore.</p>

<h3 id="making-grad-zero">Making grad zero</h3>

<p>2 ways</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">/</span><span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span>
</code></pre></div></div>

<h2 id="course-to-do">course to do</h2>

<p>Statistics course, SQL, keras, tensorflow</p>

<h2 id="lesson-4-5">Lesson 4-5</h2>

<h2 id="notes-4-5">Notes 4-5</h2>

<p>So basically we start with:</p>

<ol>
  <li>
    <p>Predictions for each breed [1x37] per image (? Unsure how it is
calculated). I know how to do predictions per value (<code class="language-plaintext highlighter-rouge">Tens@w +b</code>).</p>
  </li>
  <li>
    <p>Predictions as probabilities using Softmax ‚Äì&gt; <code class="language-plaintext highlighter-rouge">e^x/e^x.sum()</code> ‚Äì&gt;
e^x also hypes exagerrate the gap (e^1 and e^1.2 has a difference
of 0.6) ‚Äì&gt; <strong>Activations</strong></p>
  </li>
  <li>
    <p>Loss is based on NLL (negative Log Likelihood) ‚Äì&gt; torch.where
function for more than 2 classifications.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> -sm_acts[idx, targ]
	
 F.nll_loss(sm_acts, targ, reduction='none')
</code></pre></div>    </div>

    <p>Loss based on target.</p>
  </li>
  <li>
    <p>Take log of Activations to exaggerate difference between 0.99 and
0.999.
This is done in softmax as log_softmax. When we are optimizing the
system will treat 0.999 from 0.99 as a crazy improvement. That is
what we are going for as suggested <a href="https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability">here</a>.</p>
  </li>
  <li>
    <p>Log_softmax + negative log likelihood is Cross entropy.</p>
  </li>
  <li>
    <p>What does path object do? is it an object?</p>
  </li>
  <li>
    <p>Issues with my nn (why random was not working)?</p>
  </li>
</ol>

<p>Time to delve deep into this shit?</p>

<ol>
  <li>
    <p>Understand whats happening with the DataBlock</p>
  </li>
  <li>
    <p>using a number other than 0.5 in predictions</p>
  </li>
  <li>
    <p>What do the @‚Äôs do?</p>
  </li>
  <li>
    <p>Why value of preds,_ = <code class="language-plaintext highlighter-rouge">learn.get_preds(0)</code> takes time and is
different from <code class="language-plaintext highlighter-rouge">preds,_ = learn.get_preds(dl=[(x,y)])</code></p>
  </li>
  <li>
    <p>How do you get many activations? for 3 and 7? How do you generate
37 activations? What are the activations in the 3 or 7 case?
get_preds()</p>

    <p>How do you get 2 predictions one for 3 category and 7 category
where are the 2 activations?</p>

    <p>How to get 2 activations for binary case which then becomes [0.4
and 0.6] based on softmax (exp(x)/exp(x).sum)</p>
  </li>
</ol>

<h2 id="lesson-5-nb3">Lesson 5 nb3</h2>

<p><a href="https://forums.fast.ai/t/lesson-5-official-topic/68039?u=thetj09">Lesson 5 wiki</a></p>

<p><a href="https://forums.fast.ai/t/lesson-5-official-topic/68039?u=thetj09">This</a> is a wiki post - feel free to edit to add links from the lesson or other useful info.</p>

<p><strong>Note that the video and this thread is called ‚Äúlesson 5‚Äù because that‚Äôs what this will be in the MOOC, even although it‚Äôs week 4 of the course</strong></p>

<h2 id="lesson-resources-2">Lesson resources</h2>

<ul>
  <li><a href="https://youtu.be/krIVOb23EH8">Edited video</a> (private)</li>
  <li><a href="https://github.com/fastai/fastbook/blob/master/03_ethics.ipynb">fastbook chapter 3</a></li>
  <li><a href="https://forums.fast.ai/t/lesson-4-non-beginner-discussion/68040">Non-beginner discussion </a></li>
</ul>

<h2 id="links-from-lesson-3">Links from lesson</h2>

<ul>
  <li>
    <p><a href="https://www.youtube.com/watch?v=ogvPNzb9ai4&amp;list=PLtmWHNX-gukIWztK2C0eHKnXbPDuiES7x&amp;index=2&amp;t=0s">Rachel‚Äôs longer lesson on disinformation &amp; coronavirus</a></p>
  </li>
  <li>
    <p><a href="https://forums.fast.ai/t/privacy-surveillance-during-a-pandemic/68084">Rachel‚Äôs follow-up post on privacy &amp; surveillance during pandemic</a></p>
  </li>
</ul>

<h2 id="other-useful-links-3">Other useful links</h2>

<ul>
  <li><a href="https://arxiv.org/abs/1803.09010">Datasheets for datasets</a></li>
  <li><a href="https://weaponsofmathdestructionbook.com/">Weapons of math destruction</a></li>
  <li><a href="https://www.thispersondoesnotexist.com/">AI Generated Faces</a></li>
  <li><a href="https://forums.fast.ai/t/lesson-4-official-topic/68039/136?u=frapochetti">Papers/repos/tools on how to check for bias in your datasets/algorithms AND address it</a></li>
  <li><a href="https://www.scu.edu/ethics-in-technology-practice/ethical-toolkit/">Markkula Center - Ethical Toolkit</a></li>
  <li><a href="https://stratechery.com/2017/the-pollyannish-assumption/"> The Pollyannish Assumption</a></li>
  <li><a href="https://facctconference.org/2018/livestream_vh210.html">Understanding the context and consequences of pre-trial detention</a></li>
  <li><a href="https://forums.fast.ai/t/ai-ml-ethics-biases-responsibility/45592">Fast.ai community ethics resources &amp; discussion thread. Author credit: nbharatula</a></li>
  <li><a href="https://aiethics.substack.com/p/ai-ethics-6-radioactive-data-attacking">Montreal AI Ethics Institute Weekly Newsletter #6:  Radioactive data, attacking deep RL, steering AI progress, sucker‚Äôs list, AI ethics in marketing and more ‚Ä¶</a></li>
</ul>

<h2 id="lesson-6-nb5-nb6-nb8">Lesson 6 nb5 nb6 nb8</h2>

<p>Note: This is a wiki post - feel free to edit to add links from the lesson or other useful info.</p>

<h2 id="resources-2">Resources</h2>

<ul>
  <li><a href="https://youtu.be/cX30jxMNBUw">Video</a> (private)</li>
  <li><a href="https://github.com/fastai/fastbook/blob/master/05_pet_breeds.ipynb">fastbook chapter 5 </a></li>
  <li><a href="https://forums.fast.ai/t/fastbook-chapter-5-questionnaire-solutions-wiki/69301">fastbook chapter 5 questionnaire solutions</a> - feel free to contribute!</li>
  <li><a href="https://github.com/fastai/fastbook/blob/master/06_multicat.ipynb">fastbook chapter 6</a></li>
  <li><a href="https://forums.fast.ai/t/fastbook-chapter-6-questionnaire-solutions-wiki/69922">fastbook chapter 6 questionnaire solutions</a> - feel free to contribute!</li>
  <li><a href="https://github.com/fastai/fastbook/blob/master/08_collab.ipynb">fastbook chapter 8</a></li>
  <li><a href="https://forums.fast.ai/t/fastbook-chapter-8-questionnaire-solutions-wiki/69926">fastbook chapter 8 questionnaire solutions</a> - feel free to contribute!</li>
  <li><a href="https://forums.fast.ai/t/lesson-6-non-beginner-discussion/69308">Non-beginner discussion</a></li>
</ul>

<h2 id="links-from-lesson-4">Links from lesson</h2>
<ul>
  <li>Python for Data Analysis - Wes McKinney - <a href="http://shop.oreilly.com/product/0636920023784.do">link</a></li>
</ul>

<h2 id="other-useful-links-4">Other useful links</h2>
<ul>
  <li>BCE vs BCEWithLogitLoss - Pytorch Forum Discussion - <a href="https://discuss.pytorch.org/t/bceloss-vs-bcewithlogitsloss/33586/5">link</a></li>
  <li>Mixed Precision Training Comparison - <a href="https://hackernoon.com/rtx-2080ti-vs-gtx-1080ti-fastai-mixed-precision-training-comparisons-on-cifar-100-761d8f615d7f">link</a></li>
  <li><a href="https://www.notion.so/lankinen/Lesson-6-c5078fdd28e74dc8a443fb5923566a6e">Notes</a> by @Lankinen</li>
</ul>

<h2 id="lesson-6-nb-5-notes">Lesson 6 nb 5 notes</h2>

<p><strong>Learner explanation and how to get better accuracy</strong></p>

<ul>
  <li>Learning rates need to be found
    <ul>
      <li>ROT: min/10</li>
    </ul>
  </li>
  <li>Using the same LRs for all epochs doesn‚Äôt work
    <ul>
      <li>e.g., start with Lr of first epoch and then check LR using LR_find
#pnn</li>
      <li>Use <code class="language-plaintext highlighter-rouge">fit_one_cycle</code> ‚Äì&gt; Apparently it starts with low LR and then
gradually increases it to the middle and reduces it again towards
the end of layers. I guess this is when you use or provide an <code class="language-plaintext highlighter-rouge">lr_max</code>.</li>
    </ul>
  </li>
  <li>Using the same LRs across all layers is another criminal waste of
time. <code class="language-plaintext highlighter-rouge">Disciminative LRs</code> Lower layers barely need any change, upper layers need change.
    <ul>
      <li></li>
    </ul>
  </li>
  <li>fine_tune()
    <ul>
      <li>Freezes the layers until -1 (<code class="language-plaintext highlighter-rouge">self.freeze</code>)</li>
      <li>Number of epocs where only the last layer is trained is usually
<code class="language-plaintext highlighter-rouge">freeze_epocs=1</code></li>
      <li>now you do <code class="language-plaintext highlighter-rouge">fit_one_cycle</code> for epochs = Freeze_epochs</li>
      <li>then <code class="language-plaintext highlighter-rouge">unfreeze()</code></li>
      <li>and <code class="language-plaintext highlighter-rouge">fit_one_cycle</code> for X number of epochs</li>
    </ul>
  </li>
  <li>
    <p>If you find that it plateaus at 8th epoch out of 12 epochs, rerun
till 8. <code class="language-plaintext highlighter-rouge">overshooting</code></p>
  </li>
  <li>
    <p>cnn_learner <code class="language-plaintext highlighter-rouge">freezes</code> by default &lt;‚Äì <strong>Important</strong></p>
  </li>
  <li>
    <p>We could also choose a higher artchitechture (but it takes more time
to work it and potnetially <code class="language-plaintext highlighter-rouge">cuda gpu error</code>)</p>
  </li>
  <li>Might need to do cross-validation to improve accuracy</li>
</ul>

<p><strong>Other things to speed up with same accuracy</strong></p>

<p><code class="language-plaintext highlighter-rouge">learn = cnn_learner().to_fp16()</code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from fastai.callback.fp16 import *
learn = cnn_learner(dls,resnet50,metrics=f1_score_multi).to_fp16() 
</code></pre></div></div>

<p><strong>Questions</strong></p>

<ol>
  <li>
    <p>How is one epoch trained? 
Is SGD changing per batch</p>
  </li>
  <li>
    <p>Why value of preds,_ = <code class="language-plaintext highlighter-rouge">learn.get_preds(0)</code> takes time and is
different from <code class="language-plaintext highlighter-rouge">preds,_ = learn.get_preds(dl=[(x,y)])</code></p>
  </li>
  <li>
    <p>Why lr_steep is important? why not lr_min?</p>
  </li>
  <li>
    <p>Why is Jeremy training frozen for 3 epocs?</p>
  </li>
  <li>
    <p>how to make the model better at the issues raised by interpretation class.</p>
  </li>
  <li>
    <ul>
      <li>HOW TO IMPROVE FURTHER? WHAT HAVE OTHER PEOPEL DONE?</li>
    </ul>
  </li>
  <li>Look up the documentation for L and try using a few of the new methods is that it adds.</li>
  <li>Look up the documentation for the Python pathlib module and try using a few methods of the Path class.</li>
  <li>For the loss you go all the way with NLL, but for the PRedictions
you only do softmax?</li>
</ol>

<h2 id="lesson-6-nb6-notes">Lesson 6 nb6 notes</h2>

<p><strong>Datablocks</strong></p>

<blockquote>
  <p>Dataset:: A collection that returns a tuple of your independent and
dependent variable for a single item <br />
DataLoader:: An iterator that provides a stream of mini-batches,
where each mini-batch is a tuple of a batch of independent variables
and a batch of dependent variables</p>
</blockquote>

<blockquote>
  <p>Datasets:: An object that contains a training Dataset and a
validation Dataset<br />
DataLoaders:: An object that contains a training DataLoader and a validation DataLoader</p>
</blockquote>

<p>Dataset ‚Äì&gt; 1 tuple
DataLoader ‚Äì&gt; Contains mini-batch tuples</p>

<p>Dataset<strong>s</strong> ‚Äì&gt; contains both training and valid DS
DataLoader<strong>s</strong> ‚Äì&gt; mini batch tuples of both contains both training
and valid DL</p>

<ol>
  <li>Create DataBlock</li>
  <li>Pass <code class="language-plaintext highlighter-rouge">get_x</code> <code class="language-plaintext highlighter-rouge">get_y</code> functions</li>
  <li><code class="language-plaintext highlighter-rouge">dblock</code> contains <code class="language-plaintext highlighter-rouge">dsets</code> (just tuples) and <code class="language-plaintext highlighter-rouge">dloaders</code> (mini-batch)</li>
  <li>All components together including a regularized splitter function</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">splitter</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="n">train</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="o">~</span><span class="n">df</span><span class="p">[</span><span class="s">'is_valid'</span><span class="p">]].</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">valid</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">'is_valid'</span><span class="p">]].</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">train</span><span class="p">,</span><span class="n">valid</span>

<span class="n">dblock</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">ImageBlock</span><span class="p">,</span> <span class="n">MultiCategoryBlock</span><span class="p">),</span>
                   <span class="n">splitter</span><span class="o">=</span><span class="n">splitter</span><span class="p">,</span>
                   <span class="n">get_x</span><span class="o">=</span><span class="n">get_x</span><span class="p">,</span> 
                   <span class="n">get_y</span><span class="o">=</span><span class="n">get_y</span><span class="p">)</span>

<span class="n">dsets</span> <span class="o">=</span> <span class="n">dblock</span><span class="p">.</span><span class="n">datasets</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">dsets</span><span class="p">.</span><span class="n">train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<ol>
  <li>Including grabbing the right image portion using <code class="language-plaintext highlighter-rouge">item_tfms</code></li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dblock</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">ImageBlock</span><span class="p">,</span> <span class="n">MultiCategoryBlock</span><span class="p">),</span>
                   <span class="n">splitter</span><span class="o">=</span><span class="n">splitter</span><span class="p">,</span>
                   <span class="n">get_x</span><span class="o">=</span><span class="n">get_x</span><span class="p">,</span> 
                   <span class="n">get_y</span><span class="o">=</span><span class="n">get_y</span><span class="p">,</span>
                   <span class="n">item_tfms</span> <span class="o">=</span> <span class="n">RandomResizedCrop</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">min_scale</span><span class="o">=</span><span class="mf">0.35</span><span class="p">))</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">dblock</span><span class="p">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Note</strong> there is already some randomization within the <code class="language-plaintext highlighter-rouge">dsets</code> even,
whether you create mini-batch or not (within the training set and
within the valid set I mean)</p>

<p><strong>Tabular random splitter</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">msk</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mf">0.8</span>
<span class="n">splits</span> <span class="o">=</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">msk</span><span class="p">)[</span><span class="mi">0</span><span class="p">]),</span><span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="o">~</span><span class="n">msk</span><span class="p">)[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div></div>

<p><strong>Losses</strong></p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">binary Cross-entropy</code> loss is the same as <code class="language-plaintext highlighter-rouge">mnist_loss</code> except for the
log function.</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">()</span>
  <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">targets</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">inputs</span><span class="p">).</span><span class="n">log</span><span class="p">().</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Losses</code></p>

    <ul>
      <li>
        <p><code class="language-plaintext highlighter-rouge">MSELoss</code> ‚Äì&gt; distance measuring</p>

        <ul>
          <li>
            <p>used for PointBlock (e.g., Regression image centers)</p>
          </li>
          <li>
            <p><strong>Important</strong> Need to math.sqrt(valid_loss) to get final
‚Äúerror rate‚Äù</p>
          </li>
        </ul>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">Mnist_loss</code> ‚Äì&gt; sigmoid the activations + <code class="language-plaintext highlighter-rouge">torch.where</code> (<code class="language-plaintext highlighter-rouge">if</code> statement)
(basically an operation to check how your loss is based on
target)</p>

        <ul>
          <li>Used for binary Category blocks. (e.g., 3 or 7 or 8)</li>
        </ul>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">Cross-entropy loss</code> ‚Äì&gt; LOG softmax (normalizing with exp) the
  activation‚Äôs + Indexing function similar to <code class="language-plaintext highlighter-rouge">torch.where</code>.</p>

        <ul>
          <li>e.g., 3 or 7 or</li>
        </ul>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">Binary Cross-entropy Loss</code> ‚Äì&gt; <code class="language-plaintext highlighter-rouge">Sigmoid</code> and then <code class="language-plaintext highlighter-rouge">LOG</code> + <code class="language-plaintext highlighter-rouge">torch.where</code>.</p>
      </li>
    </ul>

  	- Used for Multi-Category blocks.  e.g., Pet breeds

    <blockquote>
      <p>F.binary_cross_entropy and its module equivalent nn.BCELoss
calculate cross-entropy on a one-hot-encoded target, but do
    not include the initial sigmoid. Normally for
    one-hot-encoded targets you‚Äôll want
    F.binary_cross_entropy_with_logits (or
    nn.BCEWithLogitsLoss), which do both sigmoid and binary
    cross-entropy in a single function, as in the preceding
    example.</p>
    </blockquote>

    <blockquote>
      <p>The equivalent for single-label datasets (like MNIST or the Pet
dataset), where the target is encoded as a single integer, is
F.nll_loss or nn.NLLLoss for the version without the initial
softmax, and F.cross_entropy or nn.CrossEntropyLoss for the
version with the initial softmax.</p>
    </blockquote>
  </li>
</ul>

<p><strong>Accuracy</strong></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Single Label Accuracy</code></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">targ</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
    <span class="s">"Compute accuracy with `targ` when `pred` is bs * n_classes"</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">inp</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">pred</span> <span class="o">==</span> <span class="n">targ</span><span class="p">).</span><span class="nb">float</span><span class="p">().</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Multi Label Accuracy</code></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">accuracy_multi</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">targ</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">sigmoid</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="s">"Compute accuracy when `inp` and `targ` are the same size."</span>
    <span class="k">if</span> <span class="n">sigmoid</span><span class="p">:</span> <span class="n">inp</span> <span class="o">=</span> <span class="n">inp</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">inp</span><span class="o">&gt;</span><span class="n">thresh</span><span class="p">)</span><span class="o">==</span><span class="n">targ</span><span class="p">.</span><span class="nb">bool</span><span class="p">()).</span><span class="nb">float</span><span class="p">().</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<ul>
  <li>
    <p>We need to pass the Metrics as a function, so we use <code class="language-plaintext highlighter-rouge">partial</code> to do
so with different thresholds.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))
  learn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)
</code></pre></div>    </div>
  </li>
</ul>

<p><strong>Partial</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">say_hello</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">say_what</span><span class="o">=</span><span class="s">"Hello"</span><span class="p">):</span> <span class="k">return</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">say_what</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s">."</span>
<span class="n">say_hello</span><span class="p">(</span><span class="s">'Jeremy'</span><span class="p">),</span><span class="n">say_hello</span><span class="p">(</span><span class="s">'Jeremy'</span><span class="p">,</span> <span class="s">'Ahoy!'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">say_hello</span><span class="p">,</span> <span class="n">say_what</span><span class="o">=</span><span class="s">"Bonjour"</span><span class="p">)</span>
<span class="n">f</span><span class="p">(</span><span class="s">"Jeremy"</span><span class="p">),</span><span class="n">f</span><span class="p">(</span><span class="s">"Sylvain"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Threshold</strong></p>

<p>It‚Äôs always good to be ‚Äúpractical‚Äù. We are going to use the validation
set to determine the threshold needed. In theory we are ‚Äúoverfitting‚Äù</p>

<ul>
  <li><strong>Note</strong> : We are not picking one value from a bumpy curve, but from a
real smooth coirve.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span><span class="mf">0.95</span><span class="p">,</span><span class="mi">29</span><span class="p">)</span>
  <span class="n">accs</span> <span class="o">=</span> <span class="p">[</span><span class="n">accuracy_multi</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">targs</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">sigmoid</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">]</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">accs</span><span class="p">);</span>
</code></pre></div></div>

<p><strong>Looking at image files</strong></p>

<ul>
  <li>Get them.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">img_files</span> <span class="o">=</span> <span class="n">get_image_files</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">img2pose</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Path</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)[:</span><span class="o">-</span><span class="mi">7</span><span class="p">]</span><span class="si">}</span><span class="s">pose.txt'</span><span class="p">)</span>
<span class="n">img2pose</span><span class="p">(</span><span class="n">img_files</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<ul>
  <li>Open them.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">im</span> <span class="o">=</span> <span class="n">PILImage</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="n">img_files</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">im</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>
<p>not shape is <code class="language-plaintext highlighter-rouge">col_len,row_len</code> :(</p>

<p><strong>Different types of Block</strong></p>

<ul>
  <li>
    <p>ImageBlock (for images)</p>
  </li>
  <li>
    <p>PointBlock (for 2 point tensors)</p>
  </li>
  <li>
    <p>MultiCategoryblock (mutple catergories selectable e.g., pet breeds)</p>
  </li>
  <li>
    <p>CategoryBlock (one category chosen at a time e.g., 3 or 7 or 9 ;) )</p>
  </li>
</ul>

<p><strong>Splitter</strong></p>

<p>Splits Training and Validation sets.</p>

<ul>
  <li>
    <p>Split just one person</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  splitter=FuncSplitter(lambda o: o.parent.name=='13')
</code></pre></div>    </div>
  </li>
  <li>
    <p>Split according to given df</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">splitter</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="n">train</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="o">~</span><span class="n">df</span><span class="p">[</span><span class="s">'is_valid'</span><span class="p">]].</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">valid</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">'is_valid'</span><span class="p">]].</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">train</span><span class="p">,</span><span class="n">valid</span>

<span class="n">dblock</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">ImageBlock</span><span class="p">,</span> <span class="n">MultiCategoryBlock</span><span class="p">),</span>
                   <span class="n">splitter</span><span class="o">=</span><span class="n">splitter</span><span class="p">,</span>
                   <span class="n">get_x</span><span class="o">=</span><span class="n">get_x</span><span class="p">,</span> 
                   <span class="n">get_y</span><span class="o">=</span><span class="n">get_y</span><span class="p">)</span>

<span class="n">dsets</span> <span class="o">=</span> <span class="n">dblock</span><span class="p">.</span><span class="n">datasets</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">dsets</span><span class="p">.</span><span class="n">train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<ul>
  <li>
    <p>Random splitter</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  splitter=RandomSplitter(seed=42)
</code></pre></div>    </div>
  </li>
  <li>
    <p>grandparent splitter</p>
  </li>
</ul>

<p><strong><code class="language-plaintext highlighter-rouge">Y_range</code> Cnn learner</strong></p>

<p>Here y_range is chose for the regression model.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>learn = cnn_learner(dls, resnet18, y_range=(-1,1))
</code></pre></div></div>

<h2 id="lesson-6-nb8-ka-notes">Lesson 6 nb8 ka notes</h2>

<p>Latent factors (scifi or action or comedy etc. but we don‚Äôt have any
info on them), movielens ranking</p>

<p><strong>Collaborative Filtering</strong> e.g., NETFLIX recommendations</p>

<ul>
  <li>
    <p>Uses are where we need to know what sort of diagnosis might be
applicable to someone, figure out where to click next, where you
learn from past behavior.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Cross tab</code> the data to look at 3 columns as x and y with some
numbers in between.</p>
  </li>
  <li>
    <p>CollabDataLoaders.from_df() needs a <code class="language-plaintext highlighter-rouge">user_name</code>, <code class="language-plaintext highlighter-rouge">item_name</code> (title), and
<code class="language-plaintext highlighter-rouge">rating_name</code></p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CollabDataLoaders</span><span class="p">.</span><span class="n">from_df</span><span class="p">(</span>
  <span class="n">ratings</span><span class="p">,</span>
  <span class="n">valid_pct</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
  <span class="n">user_name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
  <span class="n">item_name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
  <span class="n">rating_name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
  <span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
  <span class="n">path</span><span class="o">=</span><span class="s">'.'</span><span class="p">,</span>
  <span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
  <span class="n">val_bs</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
  <span class="n">shuffle_train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
  <span class="n">device</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
  <span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>We know how to do matrix multiplication and find gradients for
that. We don‚Äôt know how to find indices.</p>

    <p>So we use one-hot-coding to extract the corresponding rows and
columns</p>
  </li>
  <li>
    <p>Embedding is the matrix multiplication of a matrix (5x1000) with a
one hot coded vector (1000x1) but is done with just indices w<strong>ithout
really creating the one hot coded vector</strong>.</p>
  </li>
</ul>

<p><strong>Embedding</strong></p>

<ul>
  <li>So basically we have a batch (<code class="language-plaintext highlighter-rouge">x,y = dls.one(batch)</code>).</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">x.shape</code> ‚Äì&gt; 64x2 (64 indices, 2 ‚Äì&gt; 1 <code class="language-plaintext highlighter-rouge">user_name</code> and 1
<code class="language-plaintext highlighter-rouge">title_name</code>)</p>

<ul>
  <li>When we try to make a model, we can‚Äôt work with indices so we want to
do matrix multiplication. i.e., for 5 factors ‚Äì&gt;</li>
</ul>

<p>1000x5 will be the user_name matrix and 5x1000 will be the indices
matrix. We want only 64 out of them. So we would like to use a
<code class="language-plaintext highlighter-rouge">hot_coded</code> matrix. Therefore we have</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1000x5^T @ 1000x1 for user_name

5x1000 @ 1000x1 for user_title
</code></pre></div></div>

<ul>
  <li>
    <p>Instead of doing this for each batch, we try to use ‚Äúembedding‚Äù
which does the above with indices but can also enable calculation of
gradient.</p>

    <p><strong>Embedding</strong></p>

    <p><code class="language-plaintext highlighter-rouge">emb = Embedding(n_users,n_factors)</code> ‚Äúgenerates‚Äù a random matrix of that
size. To this you can pass <em>Tensor</em> integers to index by doing
<code class="language-plaintext highlighter-rouge">emb(tensor(3)</code>, gives 3rd row.</p>
  </li>
</ul>

<p><strong>Creating a colab model from scratch to train</strong></p>

<p>For this we use a <code class="language-plaintext highlighter-rouge">pytorch</code> setting where we can create a Class
(<code class="language-plaintext highlighter-rouge">dotproduct</code> class ‚Äì&gt; <code class="language-plaintext highlighter-rouge">DotProduct(Module)</code>) where in by default it
always calls the <code class="language-plaintext highlighter-rouge">forward method</code>.</p>

<p>In this <code class="language-plaintext highlighter-rouge">Forward</code> method we define the indexing using the <code class="language-plaintext highlighter-rouge">Embedding</code>
function.</p>

<p>Completely unsure how this embedding works but here is the model‚Ä¶</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DotProduct</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_users</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="n">n_factors</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">user_factors</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="n">n_factors</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">movie_factors</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">n_movies</span><span class="p">,</span> <span class="n">n_factors</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">users</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">user_factors</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">movies</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">movie_factors</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">users</span> <span class="o">*</span> <span class="n">movies</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>With this, we get a random set of vectors at the start and then pass
of the indices of each batch to the <code class="language-plaintext highlighter-rouge">embed</code> object to index into
it. <code class="language-plaintext highlighter-rouge">forward</code> is called ‚Äúsimilar‚Äù to <code class="language-plaintext highlighter-rouge">__call__</code> each time we call the
object as a function. i.e, <code class="language-plaintext highlighter-rouge">A = DotProduct(9,12,5)</code> followed by
<code class="language-plaintext highlighter-rouge">A(user_factors_batch)</code></p>

<p>Pass the batch‚Äôs independent variables to <code class="language-plaintext highlighter-rouge">forward</code>.</p>

<p><code class="language-plaintext highlighter-rouge">Embedding</code> creates a random matrix with <code class="language-plaintext highlighter-rouge">n_users,n_factors</code> from
which we can index with the <code class="language-plaintext highlighter-rouge">user_names</code> within each batch with
self.user_factors(x[:,0]). The reason we use Embedding is it comes
with grad capabilities.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">dls</span><span class="p">.</span><span class="n">one_batch</span><span class="p">()</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span><span class="n">n_factors</span><span class="p">)</span>
<span class="n">a</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># will select all `user_names` in that title.
</span></code></pre></div></div>

<p>With <code class="language-plaintext highlighter-rouge">y_range</code>, <code class="language-plaintext highlighter-rouge">sigmoid</code> and <code class="language-plaintext highlighter-rouge">bias</code> added :)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DotProductBias</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_users</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="n">n_factors</span><span class="p">,</span> <span class="n">y_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">5.5</span><span class="p">)):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">user_factors</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="n">n_factors</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">user_bias</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">movie_factors</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">n_movies</span><span class="p">,</span> <span class="n">n_factors</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">movie_bias</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">n_movies</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y_range</span> <span class="o">=</span> <span class="n">y_range</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">users</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">user_factors</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">movies</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">movie_factors</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">(</span><span class="n">users</span> <span class="o">*</span> <span class="n">movies</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">user_bias</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">movie_bias</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">sigmoid_range</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">y_range</span><span class="p">)</span>
</code></pre></div></div>

<p>This is equivalent to:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DotProductBias</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_users</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="n">n_factors</span><span class="p">,</span> <span class="n">y_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">5.5</span><span class="p">)):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">user_factors</span> <span class="o">=</span> <span class="n">create_params</span><span class="p">([</span><span class="n">n_users</span><span class="p">,</span> <span class="n">n_factors</span><span class="p">])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">user_bias</span> <span class="o">=</span> <span class="n">create_params</span><span class="p">([</span><span class="n">n_users</span><span class="p">])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">movie_factors</span> <span class="o">=</span> <span class="n">create_params</span><span class="p">([</span><span class="n">n_movies</span><span class="p">,</span> <span class="n">n_factors</span><span class="p">])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">movie_bias</span> <span class="o">=</span> <span class="n">create_params</span><span class="p">([</span><span class="n">n_movies</span><span class="p">])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y_range</span> <span class="o">=</span> <span class="n">y_range</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">users</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">user_factors</span><span class="p">[</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">movies</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">movie_factors</span><span class="p">[</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]]</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">(</span><span class="n">users</span><span class="o">*</span><span class="n">movies</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">user_bias</span><span class="p">[</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]]</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">movie_bias</span><span class="p">[</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]]</span>
        <span class="k">return</span> <span class="n">sigmoid_range</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">y_range</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="lesson-6-code-learnings">Lesson 6 code learnings</h2>

<p><strong>Transpose</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dl_a</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">first</span><span class="p">(</span><span class="n">dl_a</span><span class="p">)</span>
<span class="n">b</span>

<span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">b</span><span class="p">))</span>
</code></pre></div></div>

<p><strong>RGB</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">im</span> <span class="o">=</span> <span class="n">image2tensor</span><span class="p">(</span><span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">"images/grizzly.jpg"</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>

<span class="k">for</span> <span class="n">bear</span><span class="p">,</span><span class="n">ax</span><span class="p">,</span><span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">im</span><span class="p">,</span><span class="n">axs</span><span class="p">,(</span><span class="s">"Reds"</span><span class="p">,</span><span class="s">"Greens"</span><span class="p">,</span><span class="s">"Blues"</span><span class="p">)):</span>
    <span class="n">show_image</span><span class="p">(</span><span class="mi">255</span><span class="o">-</span><span class="n">bear</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Make own function using partial</strong></p>

<p><code class="language-plaintext highlighter-rouge">partial</code> function in use:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo

plot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4)
</code></pre></div></div>

<p><strong>Pandas Join merge</strong></p>

<p>Join two tables by common column</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ratings = ratings.merge(movies)
ratings.head()
</code></pre></div></div>

<h2 id="lesson-6-qa-discussion">Lesson 6 q&amp;A discussion</h2>

<ol>
  <li>
    <p>how to extract from DL? and db‚Äôs in general</p>
  </li>
  <li>
    <p>how to see summary <code class="language-plaintext highlighter-rouge">dblock.summary</code></p>
  </li>
  <li>
    <p>why we need to use <code class="language-plaintext highlighter-rouge">to_cpu</code></p>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">to_cpu</span><span class="p">(</span><span class="n">dls</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">one_batch</span><span class="p">())</span>
<span class="n">activs</span> <span class="o">=</span> <span class="n">learn</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">activs</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>
<ol>
  <li>
    <p><code class="language-plaintext highlighter-rouge">get_items</code> vs <code class="language-plaintext highlighter-rouge">get_x</code></p>
  </li>
  <li>
    <p>What does normalize do here?</p>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">biwi</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span>
    <span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">ImageBlock</span><span class="p">,</span> <span class="n">PointBlock</span><span class="p">),</span>
    <span class="n">get_items</span><span class="o">=</span><span class="n">get_image_files</span><span class="p">,</span>
    <span class="n">get_y</span><span class="o">=</span><span class="n">get_ctr</span><span class="p">,</span>
    <span class="n">splitter</span><span class="o">=</span><span class="n">FuncSplitter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">o</span><span class="p">:</span> <span class="n">o</span><span class="p">.</span><span class="n">parent</span><span class="p">.</span><span class="n">name</span><span class="o">==</span><span class="s">'13'</span><span class="p">),</span>
    <span class="n">batch_tfms</span><span class="o">=</span><span class="p">[</span><span class="o">*</span><span class="n">aug_transforms</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">240</span><span class="p">,</span><span class="mi">320</span><span class="p">)),</span> 
                <span class="n">Normalize</span><span class="p">.</span><span class="n">from_stats</span><span class="p">(</span><span class="o">*</span><span class="n">imagenet_stats</span><span class="p">)]</span>
<span class="p">)</span>
</code></pre></div></div>

<ol>
  <li>
    <p>discuss how embedding works</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">(users * movies).sum(dim=1)</code> but why? why not a dot product?</p>
  </li>
</ol>

<h2 id="lesson-7-nb-8">Lesson 7 nb 8</h2>

<p><strong>Collab filtering</strong></p>

<ol>
  <li>
    <p>DotProduct of factors.</p>
  </li>
  <li>
    <p>NN</p>
  </li>
</ol>

<p>?? How does colabnn work</p>

<p>When you see overfitting (i.e., valid_loss increases while training
loss decreases).</p>

<p><strong>Reduce Overfitting by</strong></p>

<ol>
  <li>
    <p>Reducing number of latent factors (but ‚Äúbiases‚Äù the shapes to
simpler ones)</p>
  </li>
  <li>
    <p>Weight decay</p>
  </li>
</ol>

<p><strong>Weight decay or L2 regularization</strong></p>

<p>It adds a ‚Äúparabola-type‚Äù function to it.</p>

<p>When you have a steep function, tendency to overfit is high. When you
have a smooth function, tendency to overfit is reduced. By adding a
smooth function like <code class="language-plaintext highlighter-rouge">0.1 x param**2</code> to the <code class="language-plaintext highlighter-rouge">loss</code>, we control the
tendency to overfit ‚Äúapparently‚Äù. I don‚Äôt get it but it works, let‚Äôs
move on.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss_with_wd = loss + wd * (parameters**2).sum()
</code></pre></div></div>

<p>Also gradient is easy to calculate.</p>

<p><strong>Parameters</strong></p>

<p>Anything you need to ‚Äúlearn‚Äù should be wrapped in <code class="language-plaintext highlighter-rouge">nn.parameters</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">T</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>

<span class="n">L</span><span class="p">(</span><span class="n">T</span><span class="p">().</span><span class="n">parameters</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="c1">#1) [Parameter containing:
</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)]</span>
</code></pre></div></div>

<p><strong>Plotting the pca of the latent factors</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g</span> <span class="o">=</span> <span class="n">ratings</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'title'</span><span class="p">)[</span><span class="s">'rating'</span><span class="p">].</span><span class="n">count</span><span class="p">()</span><span class="c1">## get movies and number of ratings for those movies
</span><span class="n">top_movies</span> <span class="o">=</span> <span class="n">g</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="n">index</span><span class="p">.</span><span class="n">values</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span> <span class="c1">## sort the above and get the name of movies
</span><span class="n">top_idxs</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="n">learn</span><span class="p">.</span><span class="n">dls</span><span class="p">.</span><span class="n">classes</span><span class="p">[</span><span class="s">'title'</span><span class="p">].</span><span class="n">o2i</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">top_movies</span><span class="p">])</span> <span class="c1">## from movie names get indexes?
</span><span class="n">movie_w</span> <span class="o">=</span> <span class="n">learn</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">movie_factors</span><span class="p">[</span><span class="n">top_idxs</span><span class="p">].</span><span class="n">cpu</span><span class="p">().</span><span class="n">detach</span><span class="p">()</span> <span class="c1">## Get 50 factors for those indices
</span><span class="n">movie_pca</span> <span class="o">=</span> <span class="n">movie_w</span><span class="p">.</span><span class="n">pca</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="c1">## do PCA of those factors
</span><span class="n">fac0</span><span class="p">,</span><span class="n">fac1</span><span class="p">,</span><span class="n">fac2</span> <span class="o">=</span> <span class="n">movie_pca</span><span class="p">.</span><span class="n">t</span><span class="p">()</span> <span class="c1">## transpose 
</span><span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">top_movies</span><span class="p">),</span> <span class="mi">50</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">idxs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">fac0</span><span class="p">[</span><span class="n">idxs</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">fac2</span><span class="p">[</span><span class="n">idxs</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">top_movies</span><span class="p">[</span><span class="n">idxs</span><span class="p">],</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">i</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">*</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Looking at data in the form of distance</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">movie_factors</span> <span class="o">=</span> <span class="n">learn</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">i_weight</span><span class="p">.</span><span class="n">weight</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">dls</span><span class="p">.</span><span class="n">classes</span><span class="p">[</span><span class="s">'title'</span><span class="p">].</span><span class="n">o2i</span><span class="p">[</span><span class="s">'Silence of the Lambs, The (1991)'</span><span class="p">]</span>
<span class="c1">## Get angle between two vectors is used here. Trivial:)
</span><span class="n">distances</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CosineSimilarity</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">movie_factors</span><span class="p">,</span> <span class="n">movie_factors</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="bp">None</span><span class="p">])</span> 
<span class="n">idx</span> <span class="o">=</span> <span class="n">distances</span><span class="p">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">descending</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">dls</span><span class="p">.</span><span class="n">classes</span><span class="p">[</span><span class="s">'title'</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
</code></pre></div></div>
<h2 id="lesson-7-decision-tree-and-random-forests-tabular-data">Lesson 7 Decision tree and random forests tabular data</h2>

<p>Decision tree (learn from statquest)</p>

<p><strong>handling dates in tabular</strong></p>

<p>Adds all these columns to the data.</p>

<p><code class="language-plaintext highlighter-rouge">'saleWeek saleYear saleMonth saleDay saleDayofweek saleDayofyear
saleIs_month_end saleIs_month_start saleIs_quarter_end
saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed'</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">add_datepart</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">'saledate'</span><span class="p">)</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s">'Test.csv'</span><span class="p">,</span> <span class="n">low_memory</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">add_datepart</span><span class="p">(</span><span class="n">df_test</span><span class="p">,</span> <span class="s">'saledate'</span><span class="p">)</span>
<span class="s">' '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">o</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">o</span><span class="p">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'sale'</span><span class="p">))</span> <span class="c1">## not sure
## what it does
</span></code></pre></div></div>

<p><strong>Processes</strong></p>

<p>See Explanation on the docs <a href="https://docs.fast.ai/tabular.core.html#FillMissing">here</a></p>

<p>Fastai‚Äôs tabular comes with 2 processes <code class="language-plaintext highlighter-rouge">procs = [Categorify,
FillMissing]</code>, changes categories into numbers just like in
<code class="language-plaintext highlighter-rouge">dls.vocab</code>, and does NOT fill missing values with median value but
with #na with a new column with <code class="language-plaintext highlighter-rouge">True</code> or <code class="language-plaintext highlighter-rouge">False</code>.</p>

<p>For neural nets (nns) however we would also like to add <code class="language-plaintext highlighter-rouge">normalize</code> as
this could shoot the loss function of the system and make other
variables not work properly.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dft</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'a'</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]})</span>
<span class="n">tot</span> <span class="o">=</span> <span class="n">TabularPandas</span><span class="p">(</span><span class="n">dft</span><span class="p">,</span> <span class="n">Categorify</span><span class="p">,</span><span class="n">cat_names</span><span class="o">=</span><span class="p">[</span><span class="s">"a"</span><span class="p">])</span>
<span class="n">dft</span><span class="p">.</span><span class="n">dtypes</span><span class="p">,</span><span class="n">tot</span><span class="p">.</span><span class="n">items</span><span class="p">.</span><span class="n">dtypes</span>
<span class="n">tot</span><span class="p">.</span><span class="n">items</span>
</code></pre></div></div>

<p>Shows how categorify works. You see <code class="language-plaintext highlighter-rouge">0</code> instead of
<code class="language-plaintext highlighter-rouge">nan</code>. <code class="language-plaintext highlighter-rouge">tot.classes["a"]</code> gives the classes as <code class="language-plaintext highlighter-rouge">['#na#', 0.0, 1.0,
2.0]</code>. Not sure how to go back though.</p>

<p><strong>Splitting train and valid by date</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cond</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">saleYear</span><span class="o">&lt;</span><span class="mi">2011</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">saleMonth</span><span class="o">&lt;</span><span class="mi">10</span><span class="p">)</span>
<span class="n">train_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span> <span class="n">cond</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">valid_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="o">~</span><span class="n">cond</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">splits</span> <span class="o">=</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">train_idx</span><span class="p">),</span><span class="nb">list</span><span class="p">(</span><span class="n">valid_idx</span><span class="p">))</span>
</code></pre></div></div>
<p><strong>Split to cont and cat variables</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cont,cat = cont_cat_split(df, 1, dep_var=dep_var)
</code></pre></div></div>

<p><strong>Defining a tabular dsets</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">to.classes</code> shows the vocab and <code class="language-plaintext highlighter-rouge">to.show</code> shows the actual vocab  in
a table. <code class="language-plaintext highlighter-rouge">to.items</code> shows the table without translated vocabs</p>

<p><strong>Before initiating tabular dsets</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## split to train and valid
</span><span class="n">msk</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mf">0.8</span>
<span class="n">splits</span> <span class="o">=</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">msk</span><span class="p">)[</span><span class="mi">0</span><span class="p">]),</span><span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="o">~</span><span class="n">msk</span><span class="p">)[</span><span class="mi">0</span><span class="p">]))</span>
<span class="c1"># df_tr = df[msk]
# df_vd = df[~msk]
# len(df_tr)/len(df), len(df_vd)/len(df)
</span><span class="nb">len</span><span class="p">(</span><span class="n">splits</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">+</span><span class="nb">len</span><span class="p">(</span><span class="n">splits</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Leaf nodes and number of rows</strong></p>

<ol>
  <li>
    <p>At every leaf node, the dater is split. i.e., 50k gets split to 35k
and 15k.</p>

    <p><strong>Eventually all the leaf nodes sum of dater will give you total
dater length.</strong></p>
  </li>
  <li>
    <p>So if ‚Äì&gt;  no. of leaf nodes = length of dater,</p>

    <p>then ‚Äì&gt; in theory you have fit almost all of the training data
 one to one (#overfit).</p>

    <p>so use <code class="language-plaintext highlighter-rouge">min_samples_leaf=25</code>. It wont split below 25 samples.</p>
  </li>
</ol>

<p>Why are there more nodes than columns? There will be bruv. Max number
of leaf nodes is <code class="language-plaintext highlighter-rouge">2^n_columns</code>. This is an insane number of leaf nodes. But
you will max out on samples. So don‚Äôt worry. :)</p>

<p>How to get deep trees without overfitting? Bagging.</p>

<p>How do we deal with N/A?</p>

<p><strong>Decision Tree</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span><span class="p">,</span> <span class="n">DecisionTreeClassifier</span>
<span class="c1">#!pip install dtreeviz
</span><span class="kn">from</span> <span class="nn">dtreeviz.trees</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">m</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">draw_tree</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">leaves_parallel</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Gini impurity</strong></p>

<p>Main idea is <a href="https://victorzhou.com/blog/gini-impurity/">here</a>. I make an example out of this
<a href="https://www.youtube.com/watch?v=7VeUPuFGJHk">statquest</a>. Also look at <a href="https://medium.com/@pandulaofficial/implementing-cart-algorithm-from-scratch-in-python-5dd00e9d36e">this medium post</a>.</p>

<p>I think it helps with some context: Let‚Äôs say there is a machine that
can detect Heart Disease (HD). The machine can predict HD 30% of the
time. Following is the sample we have:</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>HD</th>
      <th>!HD</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Machine</td>
      <td>30%</td>
      <td>70%</td>
    </tr>
  </tbody>
</table>

<p>This means the following cases are possible:</p>

<ol>
  <li>Machine classify as HD and it is HD (P = 0.3*0.3)</li>
  <li>Machine classify as !HD and it is !HD (P = 0.7*0.7)</li>
  <li>Machine classify as HD but it is !HD</li>
  <li>Machine classify as !HD but it is HD</li>
</ol>

<p>We pray that cases 3&amp;4 happen less often. The sum of all probabilities
is 1.  P(3&amp;4) is therefore given by <code class="language-plaintext highlighter-rouge">1-(0.3^2)-(0.7^2)</code>=0.42. P(3&amp;4)
is the impurity or how bad the machines‚Äô prediction is, AKA
<strong>GINI=0.42</strong>.</p>

<p>The alternative is to check if someone is clutching his chest or not
due to chest pain (CP) and then guess based on probability data if he
has HD or not. The following is the sample we have. For each case we
calculate the GINI. Then we take the average of it (assuming similar
sample size) and this estimates the GINI impurity using CP to predict
HD.</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>HD</th>
      <th>!HD</th>
      <th>GINI</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>!CP</td>
      <td>25%</td>
      <td>75%</td>
      <td>0.375</td>
    </tr>
    <tr>
      <td>CP</td>
      <td>80%</td>
      <td>20%</td>
      <td>0.32</td>
    </tr>
    <tr>
      <td>avg</td>
      <td>NA</td>
      <td>NA</td>
      <td>0.38</td>
    </tr>
  </tbody>
</table>

<p>Smaller the impurity the better. So we decide instead of buying the
machine (GINI=0.42) we can just use CP as an indicator (GINI=0.38).</p>

<p><strong>Feature importance‚Äôs</strong></p>

<p>Weighted decrease in impurity and then normalized.</p>

<p><a href="https://stackoverflow.com/a/49171133/5986651">Calculated as</a> : <code class="language-plaintext highlighter-rouge">N_t / N * (impurity - N_t_R / N_t * right_impurity
                      - N_t_L / N_t * left_impurity)</code></p>

<p>to get the raw sklearn ‚Äúfeature importances‚Äù:
``m.tree_.compute_feature_importances(normalize=False)`</p>

<p><strong>Bootstrapping</strong></p>

<p>Making a dataset with random picking <strong>with replacement</strong>. Size of
dataset picked, is the same as the orginal.</p>

<p><strong>Bagging</strong> (rand choose rows and make multiple models)</p>

<p>Here is the procedure that Breiman is proposing:</p>

<ol>
  <li>Randomly choose a subset of the rows of your data (i.e., ‚Äúbootstrap
replicates of your learning set‚Äù).</li>
  <li>Train a model using this subset.</li>
  <li>Save that model, and then return to step 1 a few times.</li>
  <li>This will give you a number of trained models. To make a
prediction, predict using all of the models, and then take the
average of each of those model‚Äôs predictions.</li>
</ol>

<p>Idea being you get random errors which are not correlated with one
another. so when you average it you reduce the uncertainty or the
error by <code class="language-plaintext highlighter-rouge">sqrt(number of models)</code></p>

<p><strong>Random forest</strong></p>

<p>In addition to the above randomly chosen rows per model (<strong>bagging</strong>),
randomly selecting from a subset of columns at each node is Random
forest.</p>

<p>check out the ‚ÄúElements of Statistical Learning‚Äù</p>

<p>Note: <strong>You somehow don‚Äôt overfit</strong>, as you keep averaging and hence
you plateau with accuracy. Which is very interesting. (lesson 7 Jeremy)</p>

<p><strong>Out-of-bag-error</strong></p>

<p>There are two reasons why you training and validation rmse are
different.</p>

<ol>
  <li>
    <p>Random forests was not accurate enough,</p>
  </li>
  <li>
    <p>There is actual difference between training and validation set
(e.g., time offset).</p>
  </li>
</ol>

<p>Using out-of-bag rmse error we can determine which of the ones it is, i.e.,
how accurate your training and validation rmse is.</p>

<p><em>How is it done?</em></p>

<ol>
  <li>
    <p>Take each row and find the models where it is not used and compute
the rmse</p>

    <p>It gives an impression of ho wthe rf works on not seen data (aka
valid data). Beware this seems to defeat the point when we use time
series dataset.</p>
  </li>
  <li>
    <p>Do it for every row.</p>
  </li>
</ol>

<p>In <a href="https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb">this lecture on tabular data</a> they show a training error of
17%, OOB error of 21% and a validation error of 23%.  Is my
understanding below correct?</p>

<p>The training error is different from OOB error ‚Äì&gt; This implies
overfitting.  Because the OOB error is ‚Äúmuch less‚Äù than the validation
error ‚Äì&gt; This implies that the validation set is ‚Äúout of domain‚Äù
(e.g., from a different time series).</p>

<p><strong>Model interpretation</strong></p>

<ul>
  <li>How confident are we in our predictions using a particular row of data?</li>
  <li>For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?</li>
  <li>Which columns are the strongest predictors, which can we ignore?</li>
  <li>Which columns are effectively redundant with each other, for purposes of prediction?</li>
  <li>How do predictions vary, as we vary these columns?</li>
</ul>

<p><strong>Variance of prediction</strong></p>

<p>We usually look at the mean of all the predictions per row. This can
have high std or low std. In the case that the mean=0.23 and std=0.21
(I personally would not be very confident). We can use this info for
each prediction we make to see if we should take a deeper look at
something.</p>

<p><em>Question</em> But am quite unsure about the how to interpret the data. Are you
supposed to look at other std of other rows or?</p>

<p><strong>Feature Importance</strong> (not fully sure)</p>

<blockquote>
  <p>The way these importances are calculated is quite simple yet
elegant. The feature importance algorithm loops through each tree,
and then recursively explores each branch. At each branch, it looks
to see what feature was used for that split, and how much the model
improves as a result of that split. The improvement (weighted by the
number of rows in that group) is added to the importance score for
that feature. This is summed across all branches of all trees, and
finally the scores are normalized such that they add to 1.</p>
</blockquote>

<p><strong>Remove columns that are not important</strong></p>

<p>From the above select only columns that are <code class="language-plaintext highlighter-rouge">&gt;0.005</code> in importance.</p>

<p><strong>Remove redundant columns i.e., correlated columns, i.e., linearly
dependent colath</strong></p>

<ol>
  <li>
    <p>Using clustering to see which columns are close to each other.</p>
  </li>
  <li>
    <p>Jermey looks at oob when keeping the variables and removing them
and then finally removing all the ones that are a double. If you see
‚Äúlittle‚Äù oob change then they are fine to be thrown.</p>
  </li>
  <li>
    <p>He uses only 50k samples here randomly chosen with replacement.</p>
  </li>
  <li>
    <p>However I wonder why not also see the actual valid and train rmse
as a result of these. Actually he ends up doing it with the whole
dataset.</p>
  </li>
</ol>

<p><strong>Partial Dependence</strong></p>

<p>Book to understand formalism of subtleties: ‚ÄúThe Book of Why‚Äù</p>

<p>How does yearmade affect <code class="language-plaintext highlighter-rouge">SalesPrice</code> all else being equal?</p>

<p>The thing is we can‚Äôt just take average sale price for a particular
year. This does not account for changes in the other variables.</p>

<p>So we resort to another kind of study, where we generate different
datasets. DATASET1: all years changed to 1950, DATASET2: all years
changed to 1970‚Ä¶</p>

<p>We can predict using our model on each data set. and plot the values.</p>

<blockquote>
  <p>For instance, how does YearMade impact sale price, all other things being equal?</p>
</blockquote>

<blockquote>
  <p>To answer this question, we can‚Äôt just take the average sale price for
each YearMade. The problem with that approach is that many other
things vary from year to year as well, such as which products are
sold, how many products have air-conditioning, inflation, and so
forth.</p>
</blockquote>

<blockquote>
  <p>Instead, what we do is replace every single value in the YearMade
column with 1950, and then calculate the predicted sale price for
every auction, and take the average over all auctions. Then we do
the same for 1951, 1952, and so forth until our final year
of 2011. This isolates the effect of only YearMade (even if it does
so by averaging over some imagined records where we assign a
YearMade value that might never actually exist alongside some other
values).</p>
</blockquote>

<p>But the N/A columns can also show up.</p>

<blockquote>
  <p>The ProductSize partial plot is a bit concerning. It shows that the
final group, which we saw is for missing values, has the lowest
price. To use this insight in practice, we would want to find out
why it‚Äôs missing so often, and what that means. Missing values can
sometimes be useful predictors‚Äîit entirely depends on what causes
them to be missing. Sometimes, however, they can indicate data
leakage.</p>
</blockquote>

<p>Not sure how to broach such a topic or what it means. :(</p>

<p><strong>Tree interpreter</strong></p>

<p>Similar to ‚Äúfeature importance‚Äù, but works with just one row of
data. So you can understand what you are trying to do.</p>

<p><strong>Random Forests is really bad on unseen data</strong></p>

<p>It can‚Äôt predict what happens in a straight line projection between
sales price and year for example.</p>

<p>Very hard for RF to extrapolate outside the data it has seen. Example,
time series. Make sure the data is not ‚ÄúOUT OF DOMAIN‚Äù. HUh? what is
that?</p>

<p><strong>OUT OF DOMAIN DATER</strong></p>

<p>So to identify if the valid set and training set are ‚Äúdifferent‚Äù or if
they are ‚Äúout of domain‚Äù, we can do an rf to predict if dater from
training or valid.</p>

<p>If the rmse is small, or if the predicting power of the rf is high,
then we have different valid dataset. It is ‚Äúdifferent‚Äù than the
‚Äútraining‚Äù.</p>

<p>We can look at the feature importance. In the case of the problem
there is Sales Elapsed, MachineId and SalesID which are mostly
important to predict training and valid. It makes sense and it can be
imagined that the machine id keeps increasing over time, same with
sales id. :) So it‚Äôs understandable that these predict the valid.</p>

<p>If we remove these then you loose predicting power. Also if you remove
this, then you error on the main model slightly improves. So we can
consider removing these time related parameters that we expect to not
make much of a difference. THis makes the system <strong>more resillient to
time changes</strong>.</p>

<p>Similarly you can also remove certain dates to train better or just
keep the latest dates.</p>

<p><strong>NN</strong></p>

<p>Also looks good for tab dater.</p>

<blockquote>
  <p>By default, for tabular data fastai creates a neural network with
two hidden layers, with 200 and 100 activations, respectively. This
works quite well for small datasets, but here we‚Äôve got quite a
large dataset, so we increase the layer sizes to 500 and 250:</p>
</blockquote>

<p><strong>Ensembling</strong></p>

<p>Some sort of weighted or equaly-weighted results from two models.</p>

<p><strong>Boosting</strong></p>

<p>IT WILL OVERFIT.</p>

<ul>
  <li>Train a small model that underfits your dataset.</li>
  <li>Calculate the predictions in the training set for this model.</li>
  <li>Subtract the predictions from the targets; these are called the
‚Äúresiduals‚Äù and represent the error for each point in the training
set.</li>
  <li>Go back to step 1, but instead of using the original targets, use
the residuals as the targets for the training.</li>
  <li>Continue doing this until you reach some stopping criterion, such as
a maximum number of trees, or you observe your validation set error
getting worse.</li>
</ul>

<p>Is training with small sample and getting residuals and training to
reduce these residuals‚Ä¶ In the end you add up all of the predictions
unlike in bagging. There is gradient boosting machines and gradient boosting
decision trees and some other variants.</p>

<p><strong>Entity embedding</strong></p>

<p><strong>Conclusion</strong></p>

<ul>
  <li>
    <p><em>Random forests</em> are the easiest to train, because they are
extremely resilient to hyperparameter choices and require very
little preprocessing. They are very fast to train, and should not
overfit if you have enough trees. But they can be a little less
accurate, especially if extrapolation is required, such as
predicting future time periods.</p>
  </li>
  <li>
    <p><em>Gradient boosting machines</em> in theory are just as fast to train as
random forests, but in practice you will have to try lots of
different hyperparameters. They can overfit, but they are often a
little more accurate than random forests.</p>
  </li>
  <li>
    <p><em>Neural networks</em> take the longest time to train, and require extra
preprocessing, such as normalization; this normalization needs to be
used at inference time as well. They can provide great results and
extrapolate well, but only if you are careful with your
hyperparameters and take care to avoid overfitting.</p>
  </li>
</ul>

<p><strong>HW</strong></p>

<ol>
  <li>Start a kaggle competition</li>
  <li>Do KNN</li>
  <li>and EE as well and see where that takes your results. Tabulate them.</li>
</ol>

<h3 id="data-leakage-example-in-nb-9">Data leakage example in nb 9</h3>

<p>In the paper <a href="https://dl.acm.org/doi/10.1145/2020408.2020496">‚ÄúLeakage in Data Mining: Formulation, Detection, and
Avoidance‚Äù</a>, Shachar Kaufman, Saharon Rosset, and Claudia Perlich
describe leakage as:</p>

<blockquote>
  <p>: The introduction of information about the target of a data mining
    problem, which should not be legitimately available to mine
    from. A trivial example of leakage would be a model that uses the
    target itself as an input, thus concluding for example that ‚Äòit
    rains on rainy days‚Äô. In practice, the introduction of this
    illegitimate information is unintentional, and facilitated by the
    data collection, aggregation and preparation process.</p>
</blockquote>

<p>They give as an example:</p>

<blockquote>
  <p>: A real-life business intelligence project at IBM where potential
    customers for certain products were identified, among other
    things, based on keywords found on their websites. This turned out
    to be leakage since the website content used for training had been
    sampled at the point in time where the potential customer has
    already become a customer, and where the website contained traces
    of the IBM products purchased, such as the word ‚ÄòWebsphere‚Äô (e.g.,
    in a press release about the purchase or a specific product
    feature the client uses).</p>
</blockquote>

<p>Data leakage is subtle and can take many forms. In particular, missing
values often represent data leakage.</p>

<p>For instance, Jeremy competed in a Kaggle competition designed to
predict which researchers would end up receiving research grants. The
information was provided by a university and included thousands of
examples of research projects, along with information about the
researchers involved and data on whether or not each grant was
eventually accepted. The university hoped to be able to use the models
developed in this competition to rank which grant applications were
most likely to succeed, so it could prioritize its processing.</p>

<p>Jeremy used a random forest to model the data, and then used feature
importance to find out which features were most predictive. He noticed
three surprising things:</p>

<ul>
  <li>The model was able to correctly predict who would receive grants
over 95% of the time.</li>
  <li>Apparently meaningless identifier columns were the most important
predictors.</li>
  <li>The day of week and day of year columns were also highly predictive;
for instance, the vast majority of grant applications dated on a
Sunday were accepted, and many accepted grant applications were
dated on January 1.</li>
</ul>

<p>For the identifier columns, one partial dependence plot per column
showed that when the information was missing the application was
almost always rejected. It turned out that in practice, the university
only filled out much of this information <em>after</em> a grant application
was accepted. Often, for applications that were not accepted, it was
just left blank. Therefore, this information was not something that
was actually available at the time that the application was received,
and it would not be available for a predictive model‚Äîit was data
leakage.</p>

<p>In the same way, the final processing of successful applications was
often done automatically as a batch at the end of the week, or the end
of the year. It was this final processing date which ended up in the
data, so again, this information, while predictive, was not actually
available at the time that the application was received.</p>

<p>This example showcases the most practical and simple approaches to
identifying data leakage, which are to build a model and then:</p>

<ul>
  <li>Check whether the accuracy of the model is <em>too good to be true</em>.</li>
  <li>Look for important predictors that don‚Äôt make sense in practice.</li>
  <li>Look for partial dependence plot results that don‚Äôt make sense in
practice.</li>
</ul>

<p>Thinking back to our bear detector, this mirrors the advice that we
provided in ¬´chapter_production¬ª‚Äîit is often a good idea to build a
model first and then do your data cleaning, rather than vice
versa. The model can help you identify potentially problematic data
issues.</p>

<p>It can also help you identifyt which factors influence specific
predictions, with tree interpreters.</p>

<h2 id="computer-vision-assignment-kaggle">Computer Vision Assignment Kaggle</h2>

<ol>
  <li>
    <p><a href="https://www.kaggle.com/c/herbarium-2020-fgvc7/overview">Kaggle Competition</a></p>
  </li>
  <li>
    <p><a href="https://forums.fast.ai/t/fastchai-and-kaggle-group-based-projects/81384">Fastchai</a></p>
  </li>
</ol>

<ul>
  <li>
    <p>Read about the competition (15 mins)</p>

    <blockquote>
      <p>So what Kaggle does is to itself split the data into a training set
and a test set. They release these to us and the training set contains
the outcomes to predict, the test set does not. So the short answer is
that you submit your outcome list for the test set that Kaggle has
given you. What Kaggle does is to take your guesses and apply it to a
random 50% of the test set and give you your results back (the exact
metric used changes and can be found in the evaluation tab of the
competition info). So during the competition you are getting feedback
on a random static 50% of the test set. At the end of the competition
they release how you went on the other 50% for your chosen top 2
guesses. So one thing to watch out for is overfitting on the 50% of
the test set during the competition itself - it is common to see quite
big changes to the leader board once a competiton has finished because
of this. Hope this makes sense! ‚Äî <a href="https://www.kaggle.com/getting-started/9551">user</a></p>
    </blockquote>
  </li>
  <li>
    <p>Come up with a plan</p>

    <ol>
      <li>
        <p>Submit one result (random selection or the top one)</p>
      </li>
      <li>
        <p>How to not overfit?</p>
      </li>
      <li>
        <p>Get a smaller subset</p>
      </li>
      <li>
        <p>Come up with most basic solution and submit</p>
      </li>
      <li>
        <p>How to improve?</p>
      </li>
    </ol>
  </li>
  <li>
    <p>EDA</p>

    <ol>
      <li>how many species have one item to train with?</li>
      <li>what are strategies to not ‚Äúremember a picture‚Äù?</li>
    </ol>
  </li>
  <li>
    <p>Strategies</p>

    <ol>
      <li>More epochs?</li>
      <li>Presizing?</li>
      <li>different model for the ones with few?</li>
      <li>Cross validation?</li>
      <li>Getting more data from the internet</li>
      <li>Augmentation for sure</li>
      <li>different sizes of tensor</li>
      <li>how to not overfit?</li>
    </ol>
  </li>
  <li>
    <p>Prepare your arsenal of tools (1hr)</p>
  </li>
  <li>
    <p>documentation?</p>
    <h2 id="todo-1">TODO</h2>
  </li>
</ul>

<p>Goals ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
by Monday, 7th Dec
@agent18</p>

<ul class="task-list">
  <li class="task-list-item">
    <p><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />-Adding fastai and test one command</p>

    <ul class="task-list">
      <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />download data as pdf from gradient</li>
    </ul>
  </li>
  <li class="task-list-item">
    <p><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Download fastai stuff from gradient</p>
  </li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Make a random ‚Äúcsv‚Äù submission and SUBMIT TO KAGGLE  (?)</li>
  <li class="task-list-item">Make a simple model with shit accurcy and run with fastai (?)</li>
  <li class="task-list-item">just find out how to increase time of GPU (?)</li>
  <li class="task-list-item">Take a small subset (1%) and setup the CSV submission when species are missing</li>
</ul>

<p>by Tuesday, morning</p>

<ul>
  <li>Setup one run of the with shit accuracy with fastai
    <ul>
      <li>Using a random sample extract parts of json you need (done)</li>
      <li>Make df with it</li>
      <li>Make data block
        <ul>
          <li>getx</li>
          <li>gety</li>
        </ul>
      </li>
      <li>resnet 18/resnet50.fp16</li>
    </ul>
  </li>
  <li>do an eda of the 32k and how many specimens they have</li>
</ul>

<p>by wednesday</p>

<h2 id="kaggle">Kaggle</h2>

<p><a href="https://www.kaggle.com/alexisbcook/getting-started-with-titanic">Getting started with Kaggle</a></p>

<ol>
  <li>
    <p>Submit a csv for final results.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Save Version</code> when you want to save a version or you want to
submit that versions results as</p>
  </li>
  <li>
    <p>Every Version seems to be like in VERSION CONTROL. e.g., you can
select the Notebook Version and the output file generated.</p>
  </li>
  <li>
    <p>Submit any versions output documents to get back on the leader
board.</p>
  </li>
</ol>

<p>Don‚Äôt understand difference bewtween final submission and non-final
submission.</p>

<blockquote>
  <p>You may submit a maximum of 10 entries per day. You may select up to
5 final submissions for judging.</p>
</blockquote>

<h3 id="installing-dtreeviz">installing dtreeviz</h3>

<p>You might get errors. Just proceed it seems ok. Otherwise refer
<a href="https://github.com/parrt/dtreeviz/issues/108">this</a></p>
<h3 id="ieee-fraud-detection">IEEE fraud detection</h3>

<ol>
  <li><a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/111284">First place solution part 1</a></li>
  <li><a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/111308#647776">First place Part 2</a></li>
  <li><a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/111510">First place Finding UID</a></li>
  <li>
    <p><a href="https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600">First place XGB solution</a></p>
  </li>
  <li><a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203">Data and description</a></li>
  <li>
    <p>notebook on <a href="https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms">adverserial validation</a></p>
  </li>
  <li><a href="https://www.kaggle.com/c/lish-moa/discussion/202256">another competition</a></li>
</ol>

<h3 id="kaggle-api-installation-and-working">kaggle api installation and working</h3>

<h2 id="python-learnings">Python Learnings</h2>

<ul>
  <li>
    <p>pay more attention to code, don‚Äôt take things for granted. preferably check with an example data
frame so you are absolutely sure of the transactions.</p>
  </li>
  <li>
    <p>discuss on how to generate hypothesis..</p>
  </li>
  <li>
    <p>I am quickly convinced by ‚Äúproof‚Äù. e.g., I was convinced that set(a)
outputing 2000 values was enough proof that I had done it
right. Which it was</p>
  </li>
  <li>
    <p>Look at what is happening different between you and another
example. After identifying where things are going wrong I didn‚Äôt
know how to probe further (e.g., iloc, loc issue) Strip problem to
basic issues. Try to see if you can write a MWE?</p>
  </li>
</ul>

<h3 id="json-file-opening">Json file opening</h3>

<p>If error: <code class="language-plaintext highlighter-rouge">UnicodeDecodeError: 'utf8' codec can't decode byte 0x96 in
position 0: invalid start byte</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">json</span><span class="p">,</span><span class="n">codecs</span>
<span class="k">with</span> <span class="n">codecs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">"../input/herbarium-2020-fgvc7/nybg2020/train/metadata.json"</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span>
                 <span class="n">encoding</span><span class="o">=</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">train_meta</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">train_json_path</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">"utf8"</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
     <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="dataframe-index-iloc-loc-pandas">Dataframe index iloc loc pandas</h3>

<p><strong>Very Big Disclaimer</strong>:</p>

<p>Sorting or any operations that shuffles the rows in the DF is
DANGEROUS. So Row number is not the same as Index (row name).</p>

<p>Pandas has Columns and indexes, column names and index names</p>

<p><strong>Indexing</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dft</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span> <span class="n">columns</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="s">'ABCD'</span><span class="p">))</span>
<span class="n">dft</span> <span class="o">=</span> <span class="n">dft</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s">"D"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">dft</span><span class="p">[</span><span class="s">"C"</span><span class="p">].</span><span class="n">iloc</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span><span class="n">dft</span><span class="p">.</span><span class="n">loc</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="s">"C"</span><span class="p">])</span>
<span class="n">dft</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">iloc</code> ‚Äì&gt; gives rows 0 and 1 (NOT INDEX) ‚Äì&gt; [99, 50]</p>

<p><code class="language-plaintext highlighter-rouge">loc</code>  ‚Äì&gt; gives based on INDEX (rowname)‚Äì&gt; [18,7]</p>

<p><code class="language-plaintext highlighter-rouge">loc</code> is based on Labels (Index, rowname); LABEL BASED INDEXING</p>

<p><code class="language-plaintext highlighter-rouge">iloc</code> is based on Positions (row number != index); POSITION BASED INDEXING</p>

<p><code class="language-plaintext highlighter-rouge">Table: dft</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="n">A</span>	<span class="n">B</span>	<span class="n">C</span>	<span class="n">D</span>
<span class="mi">18</span>	<span class="mi">33</span>	<span class="mi">87</span>	<span class="mi">99</span>	<span class="mi">5</span>
<span class="mi">4</span>	<span class="mi">98</span>	<span class="mi">77</span>	<span class="mi">50</span>	<span class="mi">9</span>
<span class="mi">1</span>	<span class="mi">25</span>	<span class="mi">61</span>	<span class="mi">7</span>	<span class="mi">10</span>
<span class="mi">5</span>	<span class="mi">47</span>	<span class="mi">90</span>	<span class="mi">86</span>	<span class="mi">11</span>
<span class="mi">6</span>	<span class="mi">0</span>	<span class="mi">40</span>	<span class="mi">42</span>	<span class="mi">18</span>
<span class="mi">2</span>	<span class="mi">29</span>	<span class="mi">51</span>	<span class="mi">99</span>	<span class="mi">22</span>
<span class="mi">17</span>	<span class="mi">22</span>	<span class="mi">20</span>	<span class="mi">35</span>	<span class="mi">30</span>
<span class="mi">10</span>	<span class="mi">93</span>	<span class="mi">39</span>	<span class="mi">59</span>	<span class="mi">30</span>
<span class="mi">13</span>	<span class="mi">70</span>	<span class="mi">65</span>	<span class="mi">61</span>	<span class="mi">35</span>
<span class="mi">16</span>	<span class="mi">2</span>	<span class="mi">74</span>	<span class="mi">48</span>	<span class="mi">39</span>
<span class="mi">9</span>	<span class="mi">42</span>	<span class="mi">30</span>	<span class="mi">13</span>	<span class="mi">40</span>
<span class="mi">14</span>	<span class="mi">33</span>	<span class="mi">91</span>	<span class="mi">51</span>	<span class="mi">49</span>
<span class="mi">7</span>	<span class="mi">99</span>	<span class="mi">38</span>	<span class="mi">91</span>	<span class="mi">53</span>
<span class="mi">0</span>	<span class="mi">69</span>	<span class="mi">76</span>	<span class="mi">18</span>	<span class="mi">58</span>
<span class="mi">12</span>	<span class="mi">96</span>	<span class="mi">65</span>	<span class="mi">48</span>	<span class="mi">69</span>
<span class="mi">11</span>	<span class="mi">75</span>	<span class="mi">64</span>	<span class="mi">14</span>	<span class="mi">72</span>
<span class="mi">8</span>	<span class="mi">41</span>	<span class="mi">33</span>	<span class="mi">18</span>	<span class="mi">74</span>
<span class="mi">19</span>	<span class="mi">63</span>	<span class="mi">71</span>	<span class="mi">53</span>	<span class="mi">81</span>
<span class="mi">3</span>	<span class="mi">34</span>	<span class="mi">2</span>	<span class="mi">20</span>	<span class="mi">92</span>
<span class="mi">15</span>	<span class="mi">36</span>	<span class="mi">76</span>	<span class="mi">33</span>	<span class="mi">96</span>
</code></pre></div></div>

<p><strong>Using iloc and loc</strong></p>

<p><code class="language-plaintext highlighter-rouge">iloc</code> ‚Äì&gt; <code class="language-plaintext highlighter-rouge">dft.iloc[0,1]</code> or <code class="language-plaintext highlighter-rouge">dft.iloc[0,]</code>
‚Äì&gt; 33</p>

<p><code class="language-plaintext highlighter-rouge">loc</code> ‚Äì&gt; <code class="language-plaintext highlighter-rouge">dft.loc[0,"A"]</code> ‚Äì&gt; 69</p>

<p>To use <code class="language-plaintext highlighter-rouge">iloc</code> but with <code class="language-plaintext highlighter-rouge">labels</code></p>

<p><code class="language-plaintext highlighter-rouge">dft.iloc[dft.index.get_loc(0),dft.columns.get_loc("C")]</code></p>

<p><strong>Get index (rowname) or row number from value</strong></p>

<p><code class="language-plaintext highlighter-rouge">dft.index</code> ‚Äì&gt; gives index order</p>

<p><code class="language-plaintext highlighter-rouge">dft.index.values</code> ‚Äì&gt; gives an array of index order</p>

<p><code class="language-plaintext highlighter-rouge">dft.index[dft["C"]==12]</code>, <code class="language-plaintext highlighter-rouge">dft[dft["C"]==12].index</code> both give same
answer i.e., row name (INDEX)</p>

<p><strong>Get row Number (!rowname)</strong></p>

<p><strong>Get colnumber and index number from value</strong></p>

<p><code class="language-plaintext highlighter-rouge">dft.columns.get_loc("C")</code></p>

<p><strong>Index order</strong></p>

<p><code class="language-plaintext highlighter-rouge">dft.index.values</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">array</span><span class="p">([</span><span class="mi">17</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<p>Can be reset using: <code class="language-plaintext highlighter-rouge">dft.reset_index(drop=True, inplace=True)</code></p>

<p><strong>Avoid issues by using</strong></p>

<ul>
  <li>reset index, pay attention to using loc (labels) or iloc (positions)</li>
</ul>

<p><strong>doing multiple operations on pandas</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g</span> <span class="o">=</span> <span class="n">ratings</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'title'</span><span class="p">)[</span><span class="s">'rating'</span><span class="p">].</span><span class="n">count</span><span class="p">()</span>
<span class="n">top_movies</span> <span class="o">=</span> <span class="n">g</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="n">index</span><span class="p">.</span><span class="n">values</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">g</code> groups by <code class="language-plaintext highlighter-rouge">title</code> and then gets ‚Äúratings‚Äù and then counts listings
per group. Lists as <code class="language-plaintext highlighter-rouge">title</code> and <code class="language-plaintext highlighter-rouge">number of counts</code>. :)</p>

<p><code class="language-plaintext highlighter-rouge">top_movies</code> sorts g in descending order, finds the index and
gets</p>

<h3 id="pandas-groupby-sort_values-merge">Pandas groupby, sort_values, merge</h3>

<p>Adding function by grouping.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="s">"len_rows"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">"category_id"</span><span class="p">)[[</span><span class="s">"category_id"</span><span class="p">]].</span><span class="n">transform</span><span class="p">(</span><span class="nb">len</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s">"len_rows"</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">df_all</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">"category_id"</span><span class="p">).</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span>
<span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)))).</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> 
<span class="n">df</span> <span class="o">=</span> <span class="n">df_all</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">"category_id"</span><span class="p">).</span><span class="n">head</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># displays 10 of the top
</span></code></pre></div></div>
<p>Random selection of index values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_ind_1</span> <span class="o">=</span> <span class="p">[</span><span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">"category_id"</span><span class="p">]</span><span class="o">==</span><span class="n">ctg</span><span class="p">].</span><span class="n">index</span><span class="p">.</span><span class="n">tolist</span><span class="p">(),</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">ctg</span> <span class="ow">in</span> <span class="n">ctg_unq</span><span class="p">]</span>
</code></pre></div></div>
<p>Merging‚Ä¶</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">transaction</span><span class="p">.</span><span class="n">merge</span><span class="p">(</span><span class="n">identity</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="reading-fastai-documents">reading fastai documents</h3>

<p>Go here first and look at training. All important functions are
listed: https://docs.fast.ai/vision.learner.html</p>

<p>Fastai github: https://github.com/fastai/fastai/blob/master/fastai/metrics.py#L141</p>

<h3 id="testing-metrics-tracing-the-functions-methods-def">Testing metrics Tracing the functions methods def</h3>

<p><code class="language-plaintext highlighter-rouge">F1Score</code> needs to be instantiated.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,(</span><span class="mi">64</span><span class="p">,))</span>
<span class="n">targ</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,(</span><span class="mi">64</span><span class="p">,))</span>
<span class="n">f1score_ins</span> <span class="o">=</span> <span class="n">F1Score</span><span class="p">(</span><span class="n">average</span><span class="o">=</span><span class="s">"macro"</span><span class="p">)</span>
<span class="n">f1score_ins</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">targ</span><span class="p">)</span>
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">Learn</code> needs a <code class="language-plaintext highlighter-rouge">metric</code> function. In this case <code class="language-plaintext highlighter-rouge">f1score_ins</code>.</p>

<p>It looks like a function but is actually a class and needs to be
instantiated. <code class="language-plaintext highlighter-rouge">F1Score</code> is a class based on the <code class="language-plaintext highlighter-rouge">skm_to_fastai</code>
functions which returns a Class object (<code class="language-plaintext highlighter-rouge">AccumMetric</code>). <code class="language-plaintext highlighter-rouge">AccumMetric</code>
has function <code class="language-plaintext highlighter-rouge">__call__</code> which basically allows you to call the
<code class="language-plaintext highlighter-rouge">object</code> as a <code class="language-plaintext highlighter-rouge">function</code></p>

<p>So It will be:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f1_score_multi</span> <span class="o">=</span> <span class="n">F1Score</span><span class="p">(</span><span class="n">average</span><span class="o">=</span><span class="s">"macro"</span><span class="p">)</span> <span class="c1">## convert class to functie
</span><span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span><span class="n">resnet18</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="n">f1_score_multi</span><span class="p">)</span>
</code></pre></div></div>
<h3 id="star-variable-variable-self">Star variable <code class="language-plaintext highlighter-rouge">*variable</code>, <code class="language-plaintext highlighter-rouge">*self</code></h3>

<p>unpacks list or dict</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">lis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">dic</span><span class="o">=</span><span class="p">{</span><span class="s">'a'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s">'b'</span><span class="p">:</span><span class="mi">20</span><span class="p">}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">functionA</span><span class="p">(</span><span class="o">*</span><span class="n">lis</span><span class="p">,</span> <span class="o">**</span><span class="n">dic</span><span class="p">)</span>  <span class="c1">#it is similar to functionA(1, 2, 3, 4, a=10, b=20)
</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="p">{</span><span class="s">'a'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s">'b'</span><span class="p">:</span> <span class="mi">20</span><span class="p">}</span>
</code></pre></div></div>

<h3 id="unsqueeze-using-none">Unsqueeze using None,</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xs_lin</span> <span class="o">=</span> <span class="n">x_lin</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x_lin</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span><span class="n">xs_lin</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_lin</span><span class="p">[:,</span><span class="bp">None</span><span class="p">].</span><span class="n">shape</span>
</code></pre></div></div>

<p>Both output: <code class="language-plaintext highlighter-rouge">torch.Size([40, 1])</code> from <code class="language-plaintext highlighter-rouge">torch.Size([40])</code></p>

<h3 id="gpu-cpu-usage">GPU CPU usage</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">print_cpu_gpu_usage</span><span class="p">():</span>
    <span class="err">!</span><span class="n">gpustat</span> <span class="o">-</span><span class="n">cp</span>
    <span class="err">!</span><span class="n">free</span> <span class="o">-</span><span class="n">m</span>
    <span class="c1">#!top -bn1 | grep "Cpu(s)" | sed "s/.*, *\([0-9.]*\)%* id.*/\1/" |
</span>    <span class="n">awk</span> <span class="s">'{print 100 - $1"%"}'</span>
</code></pre></div></div>

<h3 id="nan-na-none-null-inf">Nan na None Null inf</h3>

<p>When you import a dataframe I think all missing values are encoded as
<code class="language-plaintext highlighter-rouge">NaN</code>. Whether it is an object, boolean or other. Use this to see all
in one goi.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>df.isna().sum()	
</code></pre></div></div>

<p><strong>Dropping them</strong></p>

<p><a href="https://stackoverflow.com/a/13434501/5986651">Stack</a>, <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html">python dropna()</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">dropna</span><span class="p">()</span>     <span class="c1">#drop all rows that have any NaN values
</span><span class="n">df</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">how</span><span class="o">=</span><span class="s">'all'</span><span class="p">)</span>     <span class="c1">#drop only if ALL columns are NaN
</span><span class="n">df</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">thresh</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>   <span class="c1">#Drop row if it does not have at least two values that are **not** NaN
</span></code></pre></div></div>
<p><strong>FASTAI</strong></p>

<p>The thing is  fastai‚Äôs Tabular pandas has this attribute <code class="language-plaintext highlighter-rouge">classes</code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>to.classes["ProductCD"]
</code></pre></div></div>

<p>This shows : [‚Äò#na#‚Äô, ‚ÄòC‚Äô, ‚ÄòH‚Äô, ‚ÄòR‚Äô, ‚ÄòS‚Äô, ‚ÄòW‚Äô]</p>

<p>FASTAI converts the dtype ‚Äúcategories or obejects‚Äù to numerical
(<code class="language-plaintext highlighter-rouge">int</code>). While providing the <code class="language-plaintext highlighter-rouge">VOCAB</code> it changes NaN types to <code class="language-plaintext highlighter-rouge">#na#</code>
string.</p>

<p>It always shows ‚Äú#na#‚Äù for categorical variables.</p>

<p><strong>Guess</strong>: <em>I think categories can have ‚Äú#na#‚Äù while working with skitlearn but
not cont variables</em></p>

<p><strong>General</strong></p>

<p><code class="language-plaintext highlighter-rouge">NaN</code> is not a number. <code class="language-plaintext highlighter-rouge">R</code> has <code class="language-plaintext highlighter-rouge">na</code> and <code class="language-plaintext highlighter-rouge">Null</code> etc‚Ä¶ But <code class="language-plaintext highlighter-rouge">Python</code>
doesn‚Äôt. <code class="language-plaintext highlighter-rouge">Pandas</code> is built on top of <code class="language-plaintext highlighter-rouge">numpy</code>.</p>

<blockquote>
  <ul>
    <li>
      <p>To detect <code class="language-plaintext highlighter-rouge">NaN</code> values numpy uses <code class="language-plaintext highlighter-rouge">np.isnan()</code>.</p>
    </li>
    <li>
      <p>To detect <code class="language-plaintext highlighter-rouge">NaN</code> values pandas uses either <code class="language-plaintext highlighter-rouge">.isna()</code> or <code class="language-plaintext highlighter-rouge">.isnull()</code>.
The <code class="language-plaintext highlighter-rouge">NaN</code> values are inherited from the fact that <code class="language-plaintext highlighter-rouge">pandas</code> is built on
top of <code class="language-plaintext highlighter-rouge">numpy</code>, while the two functions‚Äô names originate from <code class="language-plaintext highlighter-rouge">R</code>‚Äôs
<code class="language-plaintext highlighter-rouge">DataFrames</code>, whose structure and functionality pandas tried to
mimic. ‚Äî<a href="https://datascience.stackexchange.com/a/37879/67821">Stack</a></p>
    </li>
  </ul>
</blockquote>

<blockquote>
  <p>dropna(), fillna() that handles missing values and it always helps
to remember easily.</p>
</blockquote>

<p>What are they? how to identify them in the context of pandas.</p>

<p>To check for both <code class="language-plaintext highlighter-rouge">None</code> and <code class="language-plaintext highlighter-rouge">NaN</code></p>

<p><code class="language-plaintext highlighter-rouge">np.inf</code> is to denote infinity.</p>

<p><strong>Pandas</strong></p>

<p>NaN in about missing values in pandas. use <code class="language-plaintext highlighter-rouge">np.isnan(array)</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">NaN</span><span class="p">])</span>
<span class="n">np</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">array([ True])</code></p>

<p><strong>Fastest way to replace nan and inf</strong>
<a href="https://stackoverflow.com/a/17478495/5986651">Stack</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">In</span> <span class="p">[</span><span class="mi">11</span><span class="p">]:</span> <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">])</span>
    
    <span class="n">In</span> <span class="p">[</span><span class="mi">12</span><span class="p">]:</span> <span class="n">df</span><span class="p">.</span><span class="n">replace</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">)</span>
    <span class="n">Out</span><span class="p">[</span><span class="mi">12</span><span class="p">]:</span>
        <span class="mi">0</span>
    <span class="mi">0</span>   <span class="mi">1</span>
    <span class="mi">1</span>   <span class="mi">2</span>
    <span class="mi">2</span> <span class="n">NaN</span>
    <span class="mi">3</span> <span class="n">NaN</span>
</code></pre></div></div>

<h3 id="pandas-and-tabular-with-gpu">pandas and tabular with GPU</h3>

<p>This <a href="https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/">kaggle kernel</a> is an introduction to using rapids.</p>

<blockquote>
  <p>RAPIDS is a collection of libraries, GitHub page here. The library
cuDF has the same API as Pandas and does what Pandas does. The
library cuML has the same API has Scikit-learn and does what
Scikit-learn does.</p>
</blockquote>

<blockquote>
  <p>Their purpose is to allows us to do our data analytics and machine
learning on GPU instead of CPU giving us speed increases between 10x
and 1000x! They are open source libraries currently in
development. They still have some bugs and are still missing some
features that Pandas and Scikit-learn have but the existing
functions are very fast!</p>
</blockquote>

<p>also check <a href="https://www.kaggle.com/cdeotte/rapids-data-augmentation-mnist-0-985">this</a> out.</p>

<h3 id="pass-by-reference">Pass by reference</h3>

<p>Immutable objects are those that when passed by reference (in a
function) get mutated. Understanding this shows why we can modify a
list and not an int variable.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Column Names in dictionaries.
</span><span class="n">col_count</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">def</span> <span class="nf">count_col</span><span class="p">(</span><span class="n">str_sch</span><span class="p">,</span><span class="n">col_count</span><span class="o">=</span><span class="n">col_count</span><span class="p">):</span>
    <span class="n">tf</span> <span class="o">=</span> <span class="p">[</span><span class="nb">bool</span><span class="p">(</span><span class="n">re</span><span class="p">.</span><span class="n">search</span><span class="p">(</span><span class="s">"^"</span><span class="o">+</span><span class="n">str_sch</span><span class="o">+</span><span class="s">"[0-9]+"</span><span class="p">,</span><span class="n">col</span><span class="p">))</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">to_list</span><span class="p">()]</span>
    <span class="n">col_count</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+=</span><span class="nb">sum</span><span class="p">(</span><span class="n">tf</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">str_sch</span><span class="p">,</span><span class="n">cols</span><span class="p">[</span><span class="n">tf</span><span class="p">].</span><span class="n">to_list</span><span class="p">()]</span>

<span class="n">col_lst</span> <span class="o">=</span> <span class="p">[[</span><span class="s">"other"</span><span class="p">,</span><span class="n">cols_sngl</span><span class="p">]];</span> <span class="n">col_count</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+=</span><span class="nb">len</span><span class="p">(</span><span class="n">cols_sngl</span><span class="p">)</span>
<span class="p">[</span><span class="n">col_lst</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">count_col</span><span class="p">(</span><span class="n">str_sch</span><span class="p">))</span> <span class="k">for</span> <span class="n">str_sch</span> <span class="ow">in</span> <span class="n">str_mltpl</span><span class="p">]</span>
<span class="n">col_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">col_lst</span><span class="p">)</span>
<span class="n">col_count</span>
</code></pre></div></div>
<h3 id="pandas-correlation">Pandas correlation</h3>

<p>Pandas excludes nans while calculating corrs‚Ä¶ So we really don‚Äôt
know the contribution of the nans, and their spacing.</p>

<p>Calculating corr with other collumns</p>

<h3 id="remove-certain-columns-pandas">remove certain columns pandas</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tf_V</span> <span class="o">=</span> <span class="p">[</span><span class="nb">bool</span><span class="p">(</span><span class="n">re</span><span class="p">.</span><span class="n">search</span><span class="p">(</span><span class="s">"^V"</span><span class="o">+</span><span class="s">"[0-9]+"</span><span class="p">,</span><span class="n">col</span><span class="p">))</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">to_list</span><span class="p">()]</span>
<span class="n">cols</span> <span class="o">=</span> <span class="n">xs</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="o">~</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">tf_V</span><span class="p">))]</span>
<span class="nb">len</span><span class="p">(</span><span class="n">cols</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="split-column-by-another-column-and-count-groupby">split column by another column and count groupby</h3>

<p>Most elegant solution</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">groupby</span><span class="p">([</span><span class="s">"id_31"</span><span class="p">,</span><span class="s">"isFraud"</span><span class="p">]).</span><span class="n">size</span><span class="p">().</span><span class="n">unstack</span><span class="p">(</span><span class="n">fill_value</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span><span class="s">'isFraud0'</span><span class="p">,</span><span class="mi">1</span><span class="p">:</span><span class="s">'isFraud1'</span><span class="p">}).</span><span class="n">reset_index</span><span class="p">()</span>
</code></pre></div></div>
<p>had to <strong>reset index</strong> otherwise kept getting keyerror, group error etc‚Ä¶.. Huh!</p>

<h3 id="select-and-display-one-client">select and display one client</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_filter</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">uid</span><span class="p">].</span><span class="n">to_dict</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="s">'index'</span><span class="p">)[</span><span class="mi">10876</span><span class="p">]</span><span class="c1">#[10302]
</span><span class="k">print</span><span class="p">(</span><span class="n">my_filter</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Looking at one UID
</span><span class="n">df_all</span><span class="p">[</span><span class="n">feat</span><span class="p">].</span><span class="n">loc</span><span class="p">[(</span><span class="n">df_all</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">my_filter</span><span class="p">)]</span> <span class="o">==</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">(</span><span class="n">my_filter</span><span class="p">)).</span><span class="nb">all</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>
</code></pre></div></div>
<h3 id="pandas-and-aggregate">pandas and aggregate</h3>

<p>https://stackoverflow.com/a/53781645/5986651</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">temp</span><span class="p">.</span><span class="n">groupby</span><span class="p">([</span><span class="s">'A'</span><span class="p">,</span> <span class="s">'B'</span><span class="p">])[</span><span class="s">'C'</span><span class="p">,</span><span class="s">"D"</span><span class="p">].</span><span class="n">agg</span><span class="p">([(</span><span class="s">'average'</span><span class="p">,</span><span class="s">'mean'</span><span class="p">),(</span><span class="s">'total'</span><span class="p">,</span><span class="s">'sum'</span><span class="p">)])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">temp</span><span class="p">,</span><span class="n">temp</span><span class="p">.</span><span class="n">groupby</span><span class="p">([</span><span class="s">'A'</span><span class="p">,</span> <span class="s">'B'</span><span class="p">])[</span><span class="s">'C'</span><span class="p">,</span><span class="s">"D"</span><span class="p">].</span><span class="n">transform</span><span class="p">(</span><span class="s">"mean"</span><span class="p">).</span><span class="n">add_suffix</span><span class="p">(</span><span class="s">"_mean_uid"</span><span class="p">)],</span>
          <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="statistics-learning">Statistics learning</h2>

<p>Several metrics are discussed <a href="https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28">here</a>.</p>

<h3 id="confusion-matrix-precision-recall">Confusion matrix, precision, recall</h3>

<p>Everything adds up to total number. In python <code class="language-plaintext highlighter-rouge">confusion_matrix</code> from
<code class="language-plaintext highlighter-rouge">sklearn.metrics</code> has this transposed.</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>Actual isFraud</th>
      <th>Actual !isFraud</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Predicted isFraud</td>
      <td>TP</td>
      <td>FP</td>
    </tr>
    <tr>
      <td>Predicted !isFraud</td>
      <td>FN</td>
      <td>TN</td>
    </tr>
  </tbody>
</table>

<p>Sensitivity TPR (recall): What percentage of Frauds were correctly identified.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TP/(TP+FN)
</code></pre></div></div>

<p>Specificity TNR (1-FNR) (recall): What percentage of !Frauds were correctly identified</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TN/(FP+TN)
</code></pre></div></div>

<p><strong>Precision</strong> (across same prediction): The precision of a class define how
trustable is the result when the model answer that a point belongs to
that class. From what ever it predicts it does a good job if Precision
is high.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TP/(FP+TN)_
</code></pre></div></div>

<p>Think of P and N as two classes. There could be more classes.</p>

<p><strong>recall</strong> (across same Class): The recall of a class expresses how well the
model is able to detect that class. If recall is low for a class then
model is unable to</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TP/(TP+FN)
</code></pre></div></div>

<p>Like your accuracy of prediction.</p>

<h3 id="roc-curve">ROC curve</h3>

<ol>
  <li>
    <p>Graph between TPR and FPR. If TPR is 1 it means that all samples
have been correctly predicted for ‚Äútrue‚Äù class. If FPR is 1, it
means that all samples of the ‚Äúfalse‚Äù class, are predicted wrong.</p>
  </li>
  <li>
    <p>What we want thus is a low FPR and a high TPR for a given
threshold. We want the ratio of TPR to FPR to be as high as possible.</p>
  </li>
  <li>
    <p>Above the line is good. Below the line not.</p>
  </li>
</ol>

<h3 id="python-precision-confusion-recall-balanced_accuracy">python Precision, confusion, recall, balanced_accuracy</h3>

<p>Calculate the precision score or recallof one label alone.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">precision_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">m</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">precision_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">m</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">average</span><span class="o">=</span><span class="s">"macro"</span><span class="p">,</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">precision_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">m</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">average</span><span class="o">=</span><span class="s">"binary"</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">m_conf</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">):</span> 
    <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xs</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Metrics functions
</span><span class="k">def</span> <span class="nf">r_mse</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">y</span><span class="p">):</span> <span class="k">return</span> <span class="nb">round</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">pred</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()),</span> <span class="mi">6</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">m_rmse</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span> <span class="k">return</span> <span class="n">r_mse</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
<span class="c1">#def m_accuracy(m,xs,y): return sum(m.predict(xs)==y)/len(xs)
</span><span class="k">def</span> <span class="nf">m_accuracy2</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">):</span> <span class="k">return</span> <span class="n">m</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">m_conf</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">):</span> 
    <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xs</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">m_prec</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">):</span> <span class="k">return</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">m</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">m_recall</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">):</span> <span class="k">return</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">m</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>
<p>Why sample_weight does not work is still a mystery to me‚Ä¶???</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## is balanced accuracy the same as recall average? YES
</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">balanced_accuracy_score</span>
<span class="n">avg1</span> <span class="o">=</span> <span class="p">(</span><span class="n">tr_rep</span><span class="p">[</span><span class="s">"1"</span><span class="p">][</span><span class="s">"recall"</span><span class="p">]</span><span class="o">+</span><span class="n">tr_rep</span><span class="p">[</span><span class="s">"0"</span><span class="p">][</span><span class="s">"recall"</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
<span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1000</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">y</span><span class="p">])</span>
<span class="n">avg2</span> <span class="o">=</span> <span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">m</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span><span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
<span class="n">avg1</span><span class="p">,</span><span class="n">avg2</span>
</code></pre></div></div>

<h3 id="unbalanced-data">Unbalanced data</h3>

<p>It is important to realize that we might not need to do anything for
this dataset regarding unbalanced dataset.</p>

<blockquote>
  <p>I did try that. It helped my CatBoost models using hyperparameter
class_weights=[1,2.5] but over/under sampling didn‚Äôt help my LGBM
nor XGB models.</p>

  <p>I don‚Äôt have any great insights but I think the proportion of
isFraud in the test.csv file makes a difference. For example if the
train.csv had 3% isFraud but the test.csv had 10% isFraud then it
would be important to upsample so that your predictor would predict
more isFraud than 3% in the test.csv. ‚Äî<a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/111308#647776">topper</a></p>
</blockquote>

<ol>
  <li>
    <p>Balance the samples by over or undersampling</p>
  </li>
  <li>
    <p>Sort of summary <a href="https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/">here</a>.</p>
  </li>
  <li>
    <p>There are python libraries that do it. SMOTE, ‚ÄúUnbalancedDataset‚Äù
module has other features too. Check it out.</p>
  </li>
</ol>

<p>I tried using a 29% data as opposed to 3% data of minority class, and
got .89 (auc) on vlid as opposed to 0.83 (auc).</p>

<p>consider undersampling (done ROCKS with 29% 5% increase in AUC)</p>

<p>Considering just oversampling with replacement (Done good for precision)</p>

<p>Considering oversampling with smote, Rose</p>

<p>Considering undersampling with oversampling (could try)</p>

<p>Considering weighting each observation <a href="https://stackoverflow.com/questions/20082674/unbalanced-classification-using-randomforestclassifier-in-sklearn">(apparently can be done on
RF)</a> (improves by a percent).</p>

<p>There are more resources <a href="https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/">here</a></p>

<p><a href="https://www.reddit.com/r/MachineLearning/comments/12evgi/classification_when_80_of_my_training_set_is_of/">Classification when 80% of dataset is
one class</a></p>

<p><a href="http://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set">Quora ideas on how to handle unb ds</a></p>

<p><strong>Smote implementation</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ## Oversampling SMOTE
</span><span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">SMOTE</span>
<span class="n">sm</span> <span class="o">=</span> <span class="n">SMOTE</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span><span class="n">k_neighbors</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span><span class="n">Y</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">rf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
<span class="n">Y</span><span class="p">.</span><span class="n">value_counts</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="n">value_counts</span><span class="p">(),</span>
</code></pre></div></div>

<p><strong>ROS implementation</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># #oversampling
</span><span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">RandomOverSampler</span>
<span class="n">ros</span> <span class="o">=</span> <span class="n">RandomOverSampler</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span><span class="n">Y</span> <span class="o">=</span> <span class="n">ros</span><span class="p">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">rf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Balancing weights manually</strong></p>

<p>Weighing the minority class.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Weighting the samples
</span><span class="n">wt</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">value_counts</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">y</span><span class="p">.</span><span class="n">value_counts</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span><span class="p">.</span><span class="n">value_counts</span><span class="p">(),</span><span class="n">wt</span>
<span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">wt</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">y</span><span class="p">])</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">rf</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="trying-pipeline-for-oversampling">trying pipeline for oversampling</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">imblearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">RandomOverSampler</span>
<span class="n">pipe_ros</span> <span class="o">=</span> <span class="n">RandomOverSampler</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">"ros"</span><span class="p">,</span> <span class="n">pipe_ros</span><span class="p">),(</span><span class="s">"rf"</span><span class="p">,</span><span class="n">pipe_rf</span><span class="p">)])</span>
<span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>
<p>Doesn‚Äôt work. testing pipeline with just rf and was able to reproduce
results.</p>

<p><strong>Pipline with ROS + RF and gridsearchcv</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">imblearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="c1">## define pipeline
</span><span class="n">steps</span> <span class="o">=</span>  <span class="p">[(</span><span class="s">'ros'</span><span class="p">,</span> <span class="n">RandomOverSampler</span><span class="p">()),</span> <span class="p">(</span><span class="s">'rf'</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">())]</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span>

<span class="c1"># score={'AUC':'roc_auc', 
#            'RECALL':'recall'}
</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s">"rf__max_features"</span><span class="p">:[</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.7</span><span class="p">,</span><span class="mf">0.8</span><span class="p">],</span>
              <span class="s">"rf__max_samples"</span><span class="p">:[</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.6</span><span class="p">],</span>
              <span class="s">"rf__min_samples_leaf"</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">35</span><span class="p">,</span><span class="mi">50</span><span class="p">],</span>
              <span class="s">"rf__n_estimators"</span><span class="p">:[</span><span class="mi">40</span><span class="p">]</span>
             <span class="p">}</span>

<span class="n">cv_rf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">"roc_auc"</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                     <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">cv_rf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="random-forests">Random forests</h3>

<p><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#balance">Berkeley info</a></p>

<p>Using <code class="language-plaintext highlighter-rouge">m.estimators_</code> and getting the mean of predictions or using <code class="language-plaintext highlighter-rouge">m</code>
(the entire model) for predicting is the same.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">preds_p</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">t</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">valid_xs</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">m</span><span class="p">.</span><span class="n">estimators_</span><span class="p">])</span>
<span class="n">m</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">valid_xs</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="p">[</span><span class="n">preds_p</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)]</span>
</code></pre></div></div>

<h3 id="remove-out-of-domain-data-adversarial-validation-ood">Remove out of domain data (adversarial validation) OOD</h3>

<p>nb9 tabular</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## finding out of domain dater 
</span><span class="n">df_dom</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">xs</span><span class="p">,</span> <span class="n">valid_xs</span><span class="p">])</span>
<span class="n">is_valid</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_xs</span><span class="p">))</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">rf</span><span class="p">(</span><span class="n">df_dom</span><span class="p">,</span> <span class="n">is_valid</span><span class="p">)</span>
<span class="n">rf_feat_importance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">df_dom</span><span class="p">,</span> <span class="n">df_dom</span><span class="p">)[:</span><span class="mi">6</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Removing OOD dater.
</span><span class="n">ood_drop</span> <span class="o">=</span> <span class="p">[</span><span class="s">"TransactionDT"</span><span class="p">,</span><span class="s">"TransactionID"</span><span class="p">,</span><span class="s">"D15"</span><span class="p">]</span>
<span class="n">drop_var_xs</span><span class="p">(</span><span class="n">ood_drop</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">valid_xs</span><span class="p">)</span>
<span class="n">X</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">ood_drop</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">m</span><span class="o">=</span><span class="n">rf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
<span class="n">metrics</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">valid_xs</span><span class="p">,</span><span class="n">valid_y</span><span class="p">)</span>
</code></pre></div></div>
<p>Kaggle how to: <a href="https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms">here</a></p>

<p>Link from previous: <a href="http://fastml.com/adversarial-validation-part-two/">here</a></p>

<p>Use of Adv Validation in IEEE fraudulent: <a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/111284">here</a></p>

<p><strong>AV done for IEEE but with test</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## finding out of domain dater 
</span><span class="n">df_dom</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">xs</span><span class="p">,</span> <span class="n">test_xs</span><span class="p">],</span><span class="n">ignore_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">is_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">test_xs</span><span class="p">))</span>
<span class="nb">len</span><span class="p">(</span><span class="n">df_dom</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">idxT</span><span class="p">,</span><span class="n">idxV</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_dom</span><span class="p">)),</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">m</span> <span class="o">=</span> <span class="n">xgb_av_fun</span><span class="p">(</span><span class="n">df_dom</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idxT</span><span class="p">],</span> <span class="n">is_test</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idxT</span><span class="p">],</span> 
            <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">df_dom</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idxV</span><span class="p">],</span><span class="n">is_test</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idxV</span><span class="p">])])</span>
</code></pre></div></div>

<p>The ideas online about AV is that do it and then select your train
data most similar to your test data‚Ä¶</p>

<h3 id="grid-search-cv">Grid search cv</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Getting best hyperparameters
</span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s">"max_features"</span><span class="p">:[</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.7</span><span class="p">,</span><span class="mf">0.8</span><span class="p">],</span>
              <span class="s">"max_samples"</span><span class="p">:[</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.6</span><span class="p">],</span>
              <span class="s">"min_samples_leaf"</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">35</span><span class="p">,</span><span class="mi">50</span><span class="p">]</span>         
             <span class="p">}</span>
<span class="n">cv_rf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">m</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">"roc_auc"</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">cv_rf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="c1">## Get the results in a csv
</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cv_rf</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">).</span><span class="n">to_csv</span><span class="p">(</span><span class="s">"gscv.csv"</span><span class="p">)</span>
<span class="c1">## Save the pickle file of GSCV
</span><span class="kn">import</span> <span class="nn">joblib</span>
<span class="n">joblib</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">cv_rf</span><span class="p">,</span> <span class="s">'GSCV1.pkl'</span><span class="p">)</span>
<span class="n">cv_rf_test</span> <span class="o">=</span> <span class="n">joblib</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"GSCV1.pkl"</span><span class="p">)</span>
</code></pre></div></div>

<p>Mean of all estimators gives the random forests results</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## checking if mean of all estimators gives the random forest results
</span><span class="k">def</span> <span class="nf">r_mse</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">y</span><span class="p">):</span> <span class="k">return</span> <span class="nb">round</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">pred</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()),</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">preds_p</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">t</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">valid_xs</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">m</span><span class="p">.</span><span class="n">estimators_</span><span class="p">])</span>
<span class="n">r_mse</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">valid_xs</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">preds_p</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="interpreting-each-decision-tree"><strong>Interpreting each decision tree</strong></h3>
<p>using waterfalls and tree interpreter</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#hide
</span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="p">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s">'ignore'</span><span class="p">,</span> <span class="nb">FutureWarning</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">treeinterpreter</span> <span class="kn">import</span> <span class="n">treeinterpreter</span>
<span class="kn">from</span> <span class="nn">waterfall_chart</span> <span class="kn">import</span> <span class="n">plot</span> <span class="k">as</span> <span class="n">waterfall</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">draw_tree</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">leaves_parallel</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">row</span> <span class="o">=</span> <span class="n">valid_xs</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
<span class="n">prediction</span><span class="p">,</span><span class="n">bias</span><span class="p">,</span><span class="n">contributions</span> <span class="o">=</span> <span class="n">treeinterpreter</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">row</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">bias</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">contributions</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">1</span><span class="p">].</span><span class="nb">sum</span><span class="p">())</span>

<span class="n">waterfall</span><span class="p">(</span><span class="n">valid_xs</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">contributions</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">rotation_value</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span><span class="n">formatting</span><span class="o">=</span><span class="s">'{:,.3f}'</span><span class="p">);</span>
</code></pre></div></div>

<p>The contributions are the difference in the mean values of internal
node and the previous node. In case they are probabilities they are
the ratios of ‚Äútrue‚Äù to ‚Äúfalse‚Äù values at each internal node.</p>

<p>The bias is the mean value at the top most node (doesn‚Äôt change for
any row. Sum of contributions + bias gives the prediction.</p>

<p>Docs description <a href="http://blog.datadive.net/interpreting-random-forests/">here</a>.</p>

<h3 id="partial-dependence-plot">Partial dependence plot</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">plot_partial_dependence</span>

<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plot_partial_dependence</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">valid_xs_imp</span><span class="p">,</span> <span class="n">fi</span><span class="p">.</span><span class="n">cols</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span>
                        <span class="n">grid_resolution</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">);</span>
</code></pre></div></div>

<p>In this plot is done with the existing model. First the data
independent variable is all made X, then X+ 1 then X+2 etc‚Ä¶ and
dependent variable is plotted.</p>

<h3 id="handling-missing-values">Handling missing values</h3>

<p>In the IEEE fraud detection dataset, using RFs and removing variables
with 25% N/A values led to 1% increase (88%‚Äì&gt;89%) ROSed sample.</p>

<ul>
  <li>
    <p>algorithms such as naive bayes, k nearest neigbours can work with
it.</p>
  </li>
  <li>
    <p><a href="https://discuss.analyticsvidhya.com/t/what-should-be-the-allowed-percentage-of-missing-values/2456">People seem to want to drop columns with more than 30% max</a>. But
if the variable is ‚Äúimportant‚Äù then you might want to keep it. It is
said that the age column in the titanic dataset adds a lot of value
to the final result despite not having 30% of the age data.</p>
  </li>
  <li>
    <p>could use linear regression to determine the missing values</p>
  </li>
  <li>
    <p>could use mean/median to determine the values (fastai in addition
does another column where it finds where stuff is NaN)</p>
  </li>
  <li>
    <p>MICE imputation???, check out variance of the variable etc‚Ä¶</p>
  </li>
</ul>

<h3 id="handling-missing-data-only-in-test-dataset">Handling missing data only in test dataset</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">na_check</span><span class="p">(</span><span class="n">col</span><span class="p">):</span> <span class="k">return</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">col</span><span class="p">].</span><span class="n">isna</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">,</span><span class="n">df_te</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">col</span><span class="p">].</span><span class="n">isna</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span><span class="o">&gt;</span><span class="mi">0</span>
<span class="n">na_col</span> <span class="o">=</span> <span class="p">{</span><span class="n">col</span><span class="p">:</span> <span class="n">na_check</span><span class="p">(</span><span class="n">col</span><span class="p">)</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cont</span><span class="p">}</span>
<span class="n">na_col_te</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cont</span> <span class="k">if</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">col</span><span class="p">].</span><span class="n">isna</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span><span class="o">==</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">df_te</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">col</span><span class="p">].</span><span class="n">isna</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">]</span>
<span class="c1">#na_col_te = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14']
</span><span class="n">na_col_te</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">.</span><span class="n">median</span><span class="p">().</span><span class="n">to_frame</span><span class="p">().</span><span class="n">T</span><span class="p">,</span> <span class="n">df</span><span class="p">],</span> <span class="n">ignore_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## add values to that row
</span><span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">na_col_te</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span>
<span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="s">"isFraud"</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span> <span class="c1">## forcing it to be an int rather than in between like 0.5
</span><span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">na_col_te</span><span class="p">]</span>
<span class="n">df</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<h3 id="gradient-boosting">Gradient boosting</h3>

<p>IT WILL OVERFIT.</p>

<ul>
  <li>Train a small model that underfits your dataset.</li>
  <li>Calculate the predictions in the training set for this model.</li>
  <li>Subtract the predictions from the targets; these are called the
‚Äúresiduals‚Äù and represent the error for each point in the training
set.</li>
  <li>Go back to step 1, but instead of using the original targets, use
the residuals as the targets for the training.</li>
  <li>Continue doing this until you reach some stopping criterion, such as
a maximum number of trees, or you observe your validation set error
getting worse.</li>
</ul>

<p>Is training with small sample and getting residuals and training to
reduce these residuals‚Ä¶ In the end you add up all of the predictions
unlike in bagging. There is gradient boosting machines and gradient boosting
decision trees and some other variants.</p>

<ol>
  <li>
    <p>Start with mean value one leaf</p>
  </li>
  <li>
    <p>Get residual, and train an ‚Äúunderfit‚Äù tree with it</p>
  </li>
  <li>
    <p>Scale it</p>
  </li>
  <li>
    <p>now make many more with residuals.</p>
  </li>
</ol>

<p>Note: use 8 to 32 leafs as max.</p>

<p>Note2: We use ‚Äúscaling‚Äù i.e., use a learning rate (0.1) to make
smaller steps towards the final value. ‚ÄúAccording to empirical
evidence‚Äù, taking lots of small steps in the right direction results
in better predictions.‚Äù</p>

<p><strong>GB classification</strong></p>

<p>Here we do classification by looking at log odds. Log(odds) if
something ‚ÄúisFraud‚Äù or not. We convert log(odds) to probability like
in logistic regression, to</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>e^(log(odds))/(1+e^log(odds))
</code></pre></div></div>

<p><strong>Links</strong></p>

<p><a href="https://towardsdatascience.com/simplifying-gradient-boosting-5dcd934e9c76">Another explanation of GB</a></p>

<h3 id="saving-tabular-pandas-and-loading-them">saving tabular pandas and loading them</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">save_pickle</span><span class="p">(</span><span class="s">"to.pkl"</span><span class="p">,</span><span class="n">to</span><span class="p">)</span>
<span class="n">to</span> <span class="o">=</span> <span class="n">load_pickle</span><span class="p">(</span><span class="n">input_path</span><span class="o">+</span><span class="s">"fraud-detection/to.pkl"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="xgboost-xgb">XGBoost xgb</h3>

<blockquote>
  <p>xgboost decides at training time whether missing values go into the
right or left node. It chooses which to minimise loss. If there are no
missing values at training time, it defaults to sending any new
missings to the right node.‚Äî<a href="https://stats.stackexchange.com/questions/235489/xgboost-can-handle-missing-data-in-the-forecasting-phase">stack</a></p>
</blockquote>

<h3 id="implement-kfold-or-stratified-k-fold">implement kfold or stratified k fold</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="n">skf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">shuffle</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>
<span class="n">m_cv</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">df_dom</span><span class="p">,</span> <span class="n">is_test</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'roc_auc'</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">skf</span><span class="p">,</span>
<span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">KFold</code> can be substituted by <code class="language-plaintext highlighter-rouge">StratifiedKFold</code>.</p>

<p>To get output you can do:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">m_cv</span><span class="p">[</span><span class="s">"test_score"</span><span class="p">],</span><span class="n">m_cv</span><span class="p">[</span><span class="s">"train_score"</span><span class="p">],</span><span class="n">m_cv</span><span class="p">[</span><span class="s">"fit_time"</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="feature-importance-handy-definitions">feature importance handy definitions</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Plot  important features
</span><span class="k">def</span> <span class="nf">xgb_fi</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span><span class="n">df_real</span><span class="p">):</span>
    <span class="n">fi</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'cols'</span><span class="p">:</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="s">'imp'</span><span class="p">:</span><span class="n">m</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">}</span>
                       <span class="p">).</span><span class="n">sort_values</span><span class="p">(</span><span class="s">'imp'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="c1">#fi["isCont"] = fi.cols.isin(cont)
</span>    <span class="n">fi</span><span class="p">[</span><span class="s">"countNA"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">df_real</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">col</span><span class="p">].</span><span class="n">isna</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df_real</span><span class="p">)</span> <span class="k">if</span>
    <span class="n">col</span> <span class="ow">in</span> <span class="n">df_real</span><span class="p">.</span><span class="n">columns</span> <span class="k">else</span> <span class="nb">float</span><span class="p">(</span><span class="s">"NaN"</span><span class="p">)</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">fi</span><span class="p">.</span><span class="n">cols</span><span class="p">]</span>
	
    <span class="k">return</span> <span class="n">fi</span>

<span class="k">def</span> <span class="nf">plot_fi</span><span class="p">(</span><span class="n">fi</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">fi</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="s">'cols'</span><span class="p">,</span> <span class="s">'imp'</span><span class="p">,</span> <span class="s">'barh'</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">7</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="metrics">Metrics</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">m_rep</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">):</span> <span class="k">return</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">digits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">output_dict</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>



<span class="k">def</span> <span class="nf">metrics</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">y</span> <span class="p">,</span> <span class="n">valid_xs</span><span class="p">,</span> <span class="n">valid_y</span><span class="p">):</span> 
    <span class="n">tr_rep</span> <span class="o">=</span> <span class="n">m_rep</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    <span class="n">vd_rep</span> <span class="o">=</span> <span class="n">m_rep</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">valid_xs</span><span class="p">,</span><span class="n">valid_y</span><span class="p">)</span>
    <span class="n">tr_auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">m</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">xs</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">oob_auc</span><span class="o">=</span><span class="mf">0.0000</span>
    <span class="c1">#oob_auc = roc_auc_score(y,m.oob_decision_function_[:,1])
</span>    <span class="n">vd_auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">valid_y</span><span class="p">,</span><span class="n">m</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">valid_xs</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">tr_rec_1</span> <span class="o">=</span> <span class="n">tr_rep</span><span class="p">[</span><span class="s">"1"</span><span class="p">][</span><span class="s">"recall"</span><span class="p">]</span>
    <span class="n">tr_rec_0</span> <span class="o">=</span> <span class="n">tr_rep</span><span class="p">[</span><span class="s">"0"</span><span class="p">][</span><span class="s">"recall"</span><span class="p">]</span>
    <span class="n">vd_rec_1</span> <span class="o">=</span> <span class="n">vd_rep</span><span class="p">[</span><span class="s">"1"</span><span class="p">][</span><span class="s">"recall"</span><span class="p">]</span>
    <span class="n">vd_rec_0</span> <span class="o">=</span> <span class="n">vd_rep</span><span class="p">[</span><span class="s">"0"</span><span class="p">][</span><span class="s">"recall"</span><span class="p">]</span>
    <span class="n">tr_prec_1</span> <span class="o">=</span> <span class="n">tr_rep</span><span class="p">[</span><span class="s">"1"</span><span class="p">][</span><span class="s">"precision"</span><span class="p">]</span>
    <span class="n">tr_prec_0</span> <span class="o">=</span> <span class="n">tr_rep</span><span class="p">[</span><span class="s">"0"</span><span class="p">][</span><span class="s">"precision"</span><span class="p">]</span>
    <span class="n">vd_prec_1</span> <span class="o">=</span> <span class="n">vd_rep</span><span class="p">[</span><span class="s">"1"</span><span class="p">][</span><span class="s">"precision"</span><span class="p">]</span>
    <span class="n">vd_prec_0</span> <span class="o">=</span> <span class="n">vd_rep</span><span class="p">[</span><span class="s">"0"</span><span class="p">][</span><span class="s">"precision"</span><span class="p">]</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">'{:.4f} ; {:.4f} ; {:.4f}; {:.4f}, {:.4f}; {:.4f}, {:.4f}; {:.4f}, {:.4f}; {:.4f}, {:.4f}'</span>
          <span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">tr_auc</span><span class="p">,</span> <span class="n">oob_auc</span><span class="p">,</span> <span class="n">vd_auc</span><span class="p">,</span> <span class="n">tr_rec_1</span><span class="p">,</span> <span class="n">tr_rec_0</span><span class="p">,</span> <span class="n">vd_rec_1</span><span class="p">,</span><span class="n">vd_rec_0</span><span class="p">,</span><span class="n">tr_prec_1</span><span class="p">,</span> <span class="n">tr_prec_0</span><span class="p">,</span> <span class="n">vd_prec_1</span><span class="p">,</span><span class="n">vd_prec_0</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="xgb-cv-for-loop-oof-out-of-fold-predictions">xgb cv for loop OOF out of fold predictions</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="n">skf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>

<span class="n">tr_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">))</span>
<span class="n">oof_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">))</span>
<span class="c1">#te_preds = np.zeros(len(X_test))
</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">idxT</span><span class="p">,</span> <span class="n">idxV</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">skf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">y</span><span class="p">)):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Fold'</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">' rows of train ='</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">idxT</span><span class="p">),</span><span class="s">'rows of holdout ='</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">idxV</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">xgb_fun</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="n">cols</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="n">idxT</span><span class="p">],</span> <span class="n">y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idxT</span><span class="p">],</span> 
         <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">xs</span><span class="p">[</span><span class="n">cols</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="n">idxV</span><span class="p">],</span><span class="n">y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idxV</span><span class="p">])])</span>
    
    <span class="n">tr_pred</span><span class="p">[</span><span class="n">idxT</span><span class="p">]</span> <span class="o">+=</span> <span class="n">m</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="n">cols</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="n">idxT</span><span class="p">])[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">skf</span><span class="p">.</span><span class="n">n_splits</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">oof_pred</span><span class="p">[</span><span class="n">idxV</span><span class="p">]</span> <span class="o">+=</span> <span class="n">m</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="n">cols</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="n">idxV</span><span class="p">])[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1">#te_preds += clf.predict_proba(test_xs[cols])[:,1]/skf.n_splits
</span>    <span class="k">del</span> <span class="n">m</span><span class="p">;</span> <span class="n">x</span><span class="o">=</span><span class="n">gc</span><span class="p">.</span><span class="n">collect</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">'{:.4f} ; {:.4f} ; {:.4f}'</span>
          <span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">tr_pred</span><span class="p">),</span><span class="mf">0.0000</span><span class="p">,</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">oof_pred</span><span class="p">)))</span>
</code></pre></div></div>

<h3 id="prediction-of-test-dataset-code">Prediction of Test dataset code</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">format_test_preds</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span><span class="n">test_xs</span><span class="p">,</span><span class="n">comment</span><span class="p">):</span>
    <span class="c1">#preds = m.predict_proba(test_xs[cols])[:,1]
</span>    <span class="k">print</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">preds</span><span class="p">).</span><span class="n">describe</span><span class="p">())</span>
    
    <span class="c1">## Make into submission dataframe
</span>    <span class="n">df_pred</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">"TransactionID"</span><span class="p">:</span><span class="n">test_xs</span><span class="p">.</span><span class="n">TransactionID</span><span class="p">.</span><span class="n">to_list</span><span class="p">(),</span>
                            <span class="s">"isFraud"</span><span class="p">:</span> <span class="n">preds</span><span class="p">})</span>
    
    <span class="c1">## save
</span>    <span class="n">df_pred</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">comment</span><span class="o">+</span><span class="s">"my_subs.csv"</span><span class="p">,</span><span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">df_pred</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">df_pred</span><span class="p">;</span> <span class="n">x</span><span class="o">=</span><span class="n">gc</span><span class="p">.</span><span class="n">collect</span><span class="p">()</span>
</code></pre></div></div>
<h3 id="regex">regex</h3>

<p><strong>R in front</strong></p>

<p><code class="language-plaintext highlighter-rouge">r"ba[rzd]"</code> says to find exactly ‚Äúba[rzd]‚Äù and ignore the <code class="language-plaintext highlighter-rouge">[]</code>. <code class="language-plaintext highlighter-rouge">r</code>
helps to treat the whole thing as string.</p>

<p>Cheat Sheet Table is <a href="https://realpython.com/regex-python/">here</a></p>

<p><strong>Getting True False values from regex‚Äôed columns</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tf</span> <span class="o">=</span> <span class="p">[</span><span class="nb">bool</span><span class="p">(</span><span class="n">re</span><span class="p">.</span><span class="n">search</span><span class="p">(</span><span class="s">"^"</span><span class="o">+</span><span class="n">str_sch</span><span class="o">+</span><span class="s">"[0-9]+"</span><span class="p">,</span><span class="n">col</span><span class="p">))</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span>
<span class="n">xs</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">to_list</span><span class="p">()]</span>
</code></pre></div></div>

<h3 id="heatmap-correlation">heatmap correlation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="n">xs_corr</span> <span class="o">=</span> <span class="n">xs</span><span class="p">.</span><span class="n">corr</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
<span class="c1">#mask = np.triu(xs_corr)
</span><span class="n">sns</span><span class="p">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">xs_corr</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'RdBu_r'</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span><span class="c1">#, mask=mask)
</span><span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'All Cols'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>corrwith</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="n">cols</span><span class="p">].</span><span class="n">corrwith</span><span class="p">(</span>
    <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]).</span><span class="n">reset_index</span><span class="p">().</span><span class="n">sort_values</span><span class="p">(</span>
    <span class="mi">0</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">rename</span><span class="p">(</span>
    <span class="p">{</span><span class="s">'index'</span><span class="p">:</span><span class="s">'Column'</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="s">'Correlation with '</span> <span class="o">+</span> <span class="n">col</span><span class="p">},</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:</span><span class="mi">20</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="cardinality-ordered-and-unordered-categorical-data">cardinality ordered and unordered categorical data</h3>

<p><a href="https://stackoverflow.com/a/34688604/5986651">Here</a> they say when you need to model unordered categorical data
you could use one hot encoding. Other wise with label encoding you end
up treating it like an ordered category i.e., numerical
treatment‚Ä¶ A&gt;5, A&lt;5 in the decision tree. Where certain pairs might
not be possible.</p>

<blockquote>
  <p>man‚Ä¶ Great find! damn! Never knew these things existed.  Thanks a
ton. He essentially seems to be saying ‚Äúnot bad‚Äù about using
‚Äúordered categories‚Äù in a ‚Äúnon-ordered-way‚Äù with label
encoding. right? and we can also specify that and label encode if
necessary. I guess we can assume this is the case for ‚Äúnon-ordered
categories‚Äù as well. What ‚Äúnot bad‚Äù means is not clear to me, but
for now, this is good enough I guess. I can skip worrying about it
and move on then‚Ä¶ :slight_smile: thanks mate good
one. :slight_smile: ‚Äì Pandian on discord</p>
</blockquote>

<p><a href="https://youtu.be/CzdWqFTmn0Y?t=3583">Here</a>, exactly jeremy talks about the above. For now we don‚Äôt
think this is too big of a problem. :) Maybe we can check this later.</p>

<p>Also check <a href="https://stats.idre.ucla.edu/other/mult-pkg/whatstat/what-is-the-difference-between-categorical-ordinal-and-numerical-variables/#:~:text=An%20ordinal%20variable%20is%20similar,clear%20ordering%20of%20the%20categories.&amp;text=Even%20though%20we%20can%20order,the%20levels%20of%20the%20variables.">this</a> out.</p>

<p><a href="https://pbpython.com/categorical-encoding.html">This</a> describes how to do it in python</p>

<h3 id="splitting-columns">splitting columns</h3>

<p>cents and dollars</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">split_cols</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">"TransactionAmt"</span><span class="p">):</span>
    <span class="n">df</span><span class="p">[</span><span class="s">'cents'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">mod</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">floordiv</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>R_emaildomain</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">split_cols_str</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">col</span><span class="p">,</span><span class="n">delim</span><span class="o">=</span><span class="s">"."</span><span class="p">):</span>
    <span class="n">df</span><span class="p">[[</span><span class="n">col</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="n">col</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="mi">2</span><span class="p">)]]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="nb">str</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">delim</span><span class="p">,</span><span class="n">expand</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  
</code></pre></div></div>

<h3 id="replace-string-in-column">Replace string in column</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="n">lst_to_rep</span> <span class="o">=</span> <span class="p">[</span><span class="sa">r</span><span class="s">"^.*chrome.*$"</span><span class="p">,</span><span class="sa">r</span><span class="s">"^.*aol.*$"</span><span class="p">,</span><span class="sa">r</span><span class="s">"^.*[Ff]irefox.*$"</span><span class="p">,</span><span class="sa">r</span><span class="s">"^.*google.*$"</span><span class="p">,</span><span class="sa">r</span><span class="s">"^.*ie.*$"</span><span class="p">,</span><span class="sa">r</span><span class="s">"^.*safari.*$"</span><span class="p">,</span><span class="sa">r</span><span class="s">"^.*opera.*$"</span><span class="p">,</span><span class="sa">r</span><span class="s">"^.*[Ss]amsung.*$"</span><span class="p">,</span><span class="sa">r</span><span class="s">"^.*edge.*$"</span><span class="p">,</span><span class="sa">r</span><span class="s">"^.*[Aa]ndroid.*$"</span><span class="p">]</span>

<span class="n">lst_val</span> <span class="o">=</span> <span class="p">[</span><span class="s">"chrome"</span><span class="p">,</span><span class="s">"aol"</span><span class="p">,</span><span class="s">"firefox"</span><span class="p">,</span><span class="s">"google"</span><span class="p">,</span><span class="s">"ie"</span><span class="p">,</span><span class="s">"safari"</span><span class="p">,</span><span class="s">"opera"</span><span class="p">,</span><span class="s">"samsung"</span><span class="p">,</span><span class="s">"edge"</span><span class="p">,</span><span class="s">"chrome"</span><span class="p">]</span>
<span class="n">df_all</span><span class="p">[</span><span class="s">"id_31_browser"</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="n">to_replace</span><span class="o">=</span><span class="n">lst_to_rep</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">lst_val</span><span class="p">,</span>
<span class="n">regex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">);</span>
</code></pre></div></div>

<h3 id="day-hr-conversion-from-seconds">day hr conversion from seconds</h3>

<p>TransactionDT given in seconds‚Ä¶</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_all</span><span class="p">[</span><span class="s">"TransactionHr"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_all</span><span class="p">[</span><span class="s">"TransactionDT"</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="mi">60</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span><span class="o">%</span><span class="mi">24</span><span class="o">//</span><span class="mi">1</span><span class="o">/</span><span class="mi">24</span>
<span class="n">df_all</span><span class="p">[</span><span class="s">"TransactionHr2"</span><span class="p">]</span> <span class="o">=</span>
<span class="p">(</span><span class="n">df_all</span><span class="p">[</span><span class="s">'TransactionDT'</span><span class="p">]</span><span class="o">%</span><span class="p">(</span><span class="mi">3600</span><span class="o">*</span><span class="mi">24</span><span class="p">)</span><span class="o">/</span><span class="mi">3600</span><span class="o">//</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mf">24.0</span> <span class="c1">#CD's solution
</span></code></pre></div></div>
<p>Seconds to days‚Ä¶</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_all</span><span class="p">[</span><span class="s">"TransactionDay"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_all</span><span class="p">[</span><span class="s">"TransactionDT"</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="mi">60</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span><span class="o">//</span><span class="mi">24</span>
<span class="n">df_all</span><span class="p">[[</span><span class="s">"TransactionDay"</span><span class="p">,</span><span class="s">"TransactionDT"</span><span class="p">]].</span><span class="n">sample</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>
<p>days to weekday</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">1477538</span><span class="o">/</span><span class="p">(</span><span class="mi">3600</span><span class="o">*</span><span class="mi">24</span><span class="p">)</span><span class="o">/</span><span class="mi">7</span><span class="p">,</span><span class="mi">1477538</span><span class="o">/</span><span class="p">(</span><span class="mi">3600</span><span class="o">*</span><span class="mi">24</span><span class="p">)</span><span class="o">%</span><span class="mi">7</span><span class="p">,</span><span class="mi">1477538</span><span class="o">/</span><span class="p">(</span><span class="mi">3600</span><span class="o">*</span><span class="mi">24</span><span class="p">)</span><span class="o">%</span><span class="mi">7</span><span class="o">//</span><span class="mi">1</span><span class="p">,</span><span class="mi">1477538</span><span class="o">/</span><span class="p">(</span><span class="mi">3600</span><span class="o">*</span><span class="mi">24</span><span class="p">)</span><span class="o">%</span><span class="mi">7</span><span class="o">/</span><span class="mi">7</span>
<span class="n">df_all</span><span class="p">[</span><span class="s">"WkDayNum"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_all</span><span class="p">[</span><span class="s">"TransactionDT"</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="mi">3600</span><span class="o">*</span><span class="mi">24</span><span class="p">)</span><span class="o">%</span><span class="mi">7</span><span class="o">/</span><span class="mi">7</span>
<span class="n">df_all</span><span class="p">[[</span><span class="s">"WkDayNum"</span><span class="p">,</span><span class="s">"TransactionDT"</span><span class="p">]].</span><span class="n">sample</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="encoding-labels">Encoding labels</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">change_col_dtype</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">col</span><span class="p">):</span>

<span class="n">unum</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">unique</span><span class="p">())</span>
<span class="c1">#print(col,unum,df[col].dtype)
</span>
<span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">],</span><span class="n">_</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">factorize</span><span class="p">()</span> <span class="c1"># Label encoding, nans changed to -1
</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">)</span>
<span class="c1">#     if unum&lt;128: df[col] = df[col].astype('int8')
#     elif unum&lt;32768: df[col] = df[col].astype('int16')
#     else: df[col].astype('int32')
</span>
<span class="k">print</span><span class="p">(</span><span class="n">col</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">unique</span><span class="p">()),</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></div>
<h3 id="encoding-combine">encoding combine</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">encode_CB2</span><span class="p">(</span><span class="n">df1</span><span class="p">,</span><span class="n">df2</span><span class="p">,</span><span class="n">uid</span><span class="p">):</span>
    <span class="n">newcol</span> <span class="o">=</span> <span class="s">"_"</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">uid</span><span class="p">)</span>
    <span class="c1">## make combined column
</span>    <span class="n">df1</span><span class="p">[</span><span class="n">newcol</span><span class="p">]</span> <span class="o">=</span> <span class="n">df1</span><span class="p">[</span><span class="n">uid</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">).</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s">'_'</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">df2</span><span class="p">[</span><span class="n">newcol</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="n">uid</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">).</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s">'_'</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1">## concat and then factorize
</span>    <span class="n">temp_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df1</span><span class="p">[</span><span class="n">newcol</span><span class="p">],</span><span class="n">df2</span><span class="p">[</span><span class="n">newcol</span><span class="p">]],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">temp_df</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">temp_df</span><span class="p">.</span><span class="n">factorize</span><span class="p">(</span><span class="n">sort</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1">## unconcat    
</span>    <span class="k">if</span> <span class="n">temp_df</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span><span class="o">&gt;</span><span class="mi">32000</span><span class="p">:</span> 
        <span class="n">df1</span><span class="p">[</span><span class="n">newcol</span><span class="o">+</span><span class="s">"_fact"</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp_df</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">df1</span><span class="p">)].</span><span class="n">astype</span><span class="p">(</span><span class="s">'int32'</span><span class="p">)</span>
        <span class="n">df2</span><span class="p">[</span><span class="n">newcol</span><span class="o">+</span><span class="s">"_fact"</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp_df</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">df1</span><span class="p">):].</span><span class="n">astype</span><span class="p">(</span><span class="s">'int32'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">df1</span><span class="p">[</span><span class="n">newcol</span><span class="o">+</span><span class="s">"_fact"</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp_df</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">df1</span><span class="p">)].</span><span class="n">astype</span><span class="p">(</span><span class="s">'int16'</span><span class="p">)</span>
        <span class="n">df2</span><span class="p">[</span><span class="n">newcol</span><span class="o">+</span><span class="s">"_fact"</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp_df</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">df1</span><span class="p">):].</span><span class="n">astype</span><span class="p">(</span><span class="s">'int16'</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">newcol</span><span class="p">,</span><span class="n">newcol</span><span class="o">+</span><span class="s">"_fact"</span><span class="p">]</span>
</code></pre></div></div>

<h2 id="datacrunch-setup">datacrunch setup</h2>

<p>Docs are <a href="https://datacrunch.io/docs/adding-a-new-user/">here</a>.</p>

<p>What I did is, first ssh‚Äôed into the system using</p>

<p>to create an instance you need to create a key set, for that I used:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh-keygen -t ed25519 -C "your_email@example.com"
</code></pre></div></div>

<p>Then I provided the public key to DC and then to ssh into that shit I did</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh -i path-to-private-key host-ip-address

sudo ssh -i /home/eghx/.ssh/id_ed25519 135.181.63.176

sudo ssh -i /home/eghx/.ssh/id_ed25519 username@135.181.63.176
</code></pre></div></div>

<p>shortcutted as <code class="language-plaintext highlighter-rouge">dc_connect</code></p>

<p>For the first time it is suggested to create a new user.</p>

<p>When you go into root you can create a new user follwoing these
<a href="https://datacrunch.io/docs/adding-a-new-user">instructions</a>.</p>

<p>To get into a user use <code class="language-plaintext highlighter-rouge">su - username</code>.</p>

<p>currently unable to open jupyter not as root:
https://askubuntu.com/questions/1038339/i-always-have-to-access-jupyter-notebook-as-a-root-user</p>

<p><strong>fastai image</strong></p>

<p>It turns out that in the ‚Äúfastai‚Äù image there is already a user: ‚Äúuser‚Äù. Only
in this ‚Äúuser‚Äù (not root or other users you yourself create) do the
things happen such as <code class="language-plaintext highlighter-rouge">conda</code> or <code class="language-plaintext highlighter-rouge">pip</code>. This is where anaconda is
installed.</p>

<p>To go into this I needed to ssh the following:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo ssh -i /home/eghx/.ssh/id_ed25519 user@135.181.63.176
</code></pre></div></div>

<p><strong>Kaggle API</strong></p>

<ol>
  <li>
    <p>Go to kaggle account and then create token aPI</p>
  </li>
  <li>
    <p>Copy the downloaded file from <a href="https://unix.stackexchange.com/a/106482/267853">B to A</a>.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> scp Desktop/kaggle.json user@135.181.63.176:~/.kaggle/
</code></pre></div>    </div>
  </li>
  <li>
    <p>check if it has reached. :)</p>
  </li>
  <li>
    <p>change permissions not sure why, but is on the <a href="https://github.com/Kaggle/kaggle-api">official kaggle
documentation</a></p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> chmod 600 ~/.kaggle/kaggle.json
</code></pre></div>    </div>
  </li>
  <li>
    <p>401 permissions unauthorized?</p>
  </li>
</ol>

<p>Then just get a new token, possible that the old one expired.</p>

<p><strong>changing permissions of folder created by root</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo chown -R user:user agent18/
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">user:user</code> refers to the user ‚Äì&gt; user and the group ‚Äì&gt; user.</p>

<p><strong>Extracting zip files from folder</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for i in *.zip; do unzip "$i" -d "${i%%.zip}"; done
</code></pre></div></div>

<p><strong>Extracting datasets from yours and from kernels</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kaggle datasets download -d thejravichandran/fraud-detection

kaggle competitions download -c ieee-fraud-detection
kaggle datasets download thejravichandran/ieee-fraud-detection-joined-tables 

kaggle kernels pull thejravichandran/fraud-detection-v10-understanding-adv-val
</code></pre></div></div>

<p><strong>Error with import fastai vision related staments</strong></p>

<p>Check if fastcore version is correct using <code class="language-plaintext highlighter-rouge">conda list fast</code> and
comparing it with the fastai github yaml content.</p>

<h2 id="setting-up-google-cloud">Setting up google cloud</h2>

<p>https://www.kaggle.com/alexisbcook/get-started-with-google-cloud-platform</p>

<p>https://www.kaggle.com/akashram/auto-updating-data-using-kaggle-api-gcp</p>

<p>https://www.kaggle.com/rosebv/train-model-with-tensorflow-cloud (how
to work with tensorflowcloud)</p>

<p>https://www.kaggle.com/product-feedback/159602 ‚Äì&gt; direct upgrade</p>

<p><strong>Installing</strong></p>

<p><strong>Identify instance OS</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat /etc/*-release 
</code></pre></div></div>

<p><strong>Installing monitoring system</strong></p>

<p>follwing these instructions:  https://cloud.google.com/monitoring/agent/installation</p>

<p><code class="language-plaintext highlighter-rouge">df -h #for disk space</code></p>

<p>Follow this on SSH terminal (very very different from the terminal
associated with jupyterlabs).</p>

<p><strong>Check installed packages</strong></p>

<p><code class="language-plaintext highlighter-rouge">dpkg --get-selections | grep nvidia</code></p>

<h2 id="todo-cv-11-days-to-go">todo CV (11 days to go)</h2>

<p><del>1. baseline</del></p>
<ol>
  <li>move to gcp or datacrunch.io</li>
  <li>bigger dataset results</li>
  <li>callbacks to get export</li>
  <li>Other architectures</li>
  <li>Implement works from round 5</li>
  <li>what can you find from confusion matrix</li>
  <li>over-fitting, false +ves, false -ves result</li>
  <li>Write a blog</li>
</ol>

<h2 id="todo-tabular">todo tabular</h2>

<ol>
  <li><del>look at data (common man)</del></li>
  <li>clean tables (understand nan na type stuff)</li>
  <li>build pipeline (markus)</li>
  <li>baseline</li>
  <li>Categories fixing</li>
  <li>do the entire nb (by tomorrow)</li>
  <li>tweak stuff (sunday)</li>
  <li>see if you can do something else (add novelty) (monday)</li>
  <li>overfitting reduction (tuesday)</li>
  <li>Write blog on the learnings.(wednesday)</li>
</ol>

<p>Am thinking of starting with rf working with the valid for now. Let‚Äôs
see where that gets us. this includes,</p>

<p>Decision trees don‚Äôt work well here as we require prolties and AOC. So
let‚Äôs kill it. Going back to the drawing board to look at the data.</p>

<p>Also Let‚Äôs stick to basic techniques. Let‚Äôs see what OOC error we get
then come back and fine tune‚Ä¶ Min. effort. quick draft PG style?</p>

<ul>
  <li>Cleaning (replacing zeros strategies? categorical variables?)
9- Bagging, RF, and DL</li>
  <li>looking at data and what else to remove cluster etc‚Ä¶</li>
  <li>reduction of size of columns (svd, clustering etc‚Ä¶)</li>
  <li>boosting?</li>
  <li>regression anova and other stuff later?</li>
</ul>

<p>Also want to implement sigmoid stuff today for the other assignment.
or test other architectures.</p>

<h2 id="today">today</h2>

<p>predict to test,</p>

<p>understand conf matrix, understand what output is required? does label
selection make a difference. etc‚Ä¶</p>

<p>add  the split: 
https://www.kaggle.com/c/ieee-fraud-detection/discussion/99993</p>

<p>‚ÄúLooks like the data has a time split‚Ä¶ so make sure you are not using time as a feature.‚Äù</p>

<p>‚ÄúI confirm here that the data is a clean time series split on the TransactionDT. It looks like we might need to consider different validation techniques.‚Äù</p>

<p>Heat map might require clustering and scaling to see how they
group. Not sure what to do after that‚Ä¶</p>

<h2 id="now-deprecated">now (deprecated)</h2>

<p><del>Test on the bigg data, ‚Äúbalanced subsample‚Äù, and ‚Äúoversampling‚Äù, and
and use the metrics generated from gridcv.</del></p>

<p><del>- reproduce conf matrix, what is tfr and tfr?</del>
<del>- reproduce praba ‚Äì&gt; roc coiv</del></p>

<ul>
  <li>
    <p>fastai 5/5 plan to understand rf‚Ä¶</p>
  </li>
  <li>
    <p>look at removing time features as adviced by Peter?</p>
  </li>
  <li>
    <p>CV again understanding whats happening.</p>
  </li>
  <li>
    <p>check where you are today in the whole plan (today lets dag)</p>
  </li>
  <li>
    <p>remove time related features understand why?</p>
  </li>
  <li>
    <p>SETUP proper CV and do the whole feature importance based on validxs
again.</p>
  </li>
  <li>
    <p>Make a plan for the coming days.</p>
  </li>
</ul>

<ol>
  <li>
    <p>start new sheet</p>
  </li>
  <li>
    <p>Print score function</p>
  </li>
  <li>
    <p>Do gridsearchCV, do feature importance and clustering with check against valid,
remove is_valid params, reduce overfitting, Look at params where
you add shit loads of nans based on feature
importance. Undersampling but not random‚Ä¶ check correlation.</p>

    <p>Rewrite the whole thing. Clean version.</p>

    <p>Let‚Äôs remove features and check for isna issues of top features
then do testing‚Ä¶</p>

    <p>and then do the testing?</p>

    <ol>
      <li>
        <p>remove the parts that make it non resilient?</p>
      </li>
      <li>
        <p>Set up RF</p>
      </li>
      <li>
        <p>Setup GridsearchCV with certain params and then run cv on
it 5. Number of features, etc‚Ä¶</p>
      </li>
      <li>
        <p>Run weighted oversampling ‚Äúbalanced dataset‚Äù for unbalanced DS
see if results change then try smote as well.</p>
      </li>
      <li>
        <p>Run RF and check feature importance</p>
      </li>
      <li>
        <p>Filter to 20-50</p>
      </li>
      <li>
        <p>check isna for filled up columns and how we might be affecting it?</p>
      </li>
      <li>
        <p>Filter further by running induvidual tests</p>
      </li>
      <li>
        <p>Look at clustering to remove similar stuff.</p>
      </li>
      <li>
        <p>See how to look at clustering and remove</p>
      </li>
      <li>
        <p>run grid search one last time?</p>
      </li>
      <li>
        <p>Make sure you have provided some extra care for isna types and
IMB</p>
      </li>
      <li>
        <p>Generating other columns with date?</p>
      </li>
      <li>
        <p>extrapolate to the all dater.</p>
      </li>
    </ol>
  </li>
  <li>clusterning remove var, check current grid, make ok.</li>
  <li>Tomo look at overfitting and do the same thing?</li>
  <li>start with GBM asap</li>
  <li>
    <p>consider NNs perhaps.</p>
  </li>
  <li>
    <p>final set</p>

    <ol>
      <li>try undersampling alone with not random undersampling but ‚Äúmost
different row selection‚Äù undersampling.</li>
      <li>try what we have learnt so far‚Ä¶</li>
    </ol>
  </li>
  <li>Look at GBM and understand the scene‚Ä¶</li>
</ol>

<h2 id="now">now</h2>

<p>Goal ultimately: XGB, regression types, NN, or <a href="https://www.kaggle.com/c/lish-moa/discussion/202256">this</a>.</p>

<ol>
  <li>
    <p><del>check if you increase the ood numbers if you get better results.</del></p>
  </li>
  <li>
    <p>do adversarial validation on daterset between test and train using
rfs and xgb. Now that I have access to both.</p>
  </li>
  <li>
    <p>See an example of XGB</p>
  </li>
  <li>
    <p>what is XGB?</p>
  </li>
  <li>
    <p><a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/111510">reproduce</a> identifying ‚Äúsame clients‚Äù in test and train‚Ä¶</p>
  </li>
  <li>
    <p>Understand XGB, stratified kfold and then adverserial validation???
(8 videos + notes)</p>
  </li>
  <li>
    <p>come up with plan for next 7 days. where do you want to be and
check everyday. Parkinson‚Äôs law.</p>
  </li>
  <li>
    <p>EDA reproing his plots and undersstanding what he means,</p>
  </li>
  <li>
    <p>Run XGB and see where we are at with default</p>
  </li>
  <li>
    <p>plan some tuning with cv</p>
  </li>
</ol>

<p>Also see stratified kfold. See what he has done and plan reproduction‚Ä¶</p>

<p>find out what they did for their unbalanced dataset. He didn‚Äôt care.</p>

<h2 id="tomorrow">tomorrow</h2>
<ol>
  <li>
    <p>ensemble, gbm, regression</p>
  </li>
  <li>
    <p>rf parameters and what they mean, what is the output actually
outputting? auc? TP?</p>
  </li>
  <li>
    <p>removing time elements</p>
  </li>
  <li>
    <p>how to tune which parameters</p>
  </li>
  <li>
    <p>why is rf overfitting.</p>
  </li>
  <li>
    <p>Improve ERROR</p>
  </li>
  <li>
    <p>REMOVING 200 columns?</p>
  </li>
  <li>
    <p>heatmaps, oversampling, undersampling, fastai checks (boosting
etc‚Ä¶) and remove variables and test, remove time variables and
it‚Äôs corr variables, pca, normalizing?, isfraud a float or a class?</p>
  </li>
  <li>
    <p>nn?</p>
  </li>
  <li>
    <p>regression anova etc‚Ä¶</p>
  </li>
</ol>

<h2 id="experiments">Experiments</h2>

<ol>
  <li>with valid without valid prediction for whole dataset</li>
  <li>100k prediction vs 590k prediction</li>
  <li>wht columns can you remove?</li>
</ol>

<h2 id="observations">observations</h2>

<ol>
  <li>categ split between 590k set and 50k sample is the same 3% in this
case.</li>
  <li>time based split.</li>
  <li>Peter suggests to remove time details.</li>
  <li>for the final set take everything</li>
  <li>
    <p>Apprently most of the ML techniques are expecting Balanced datasets
‚ÄîStatquest</p>

    <table>
      <thead>
        <tr>
          <th>¬†</th>
          <th>tr recall</th>
          <th>vd recall</th>
          <th>tr prec</th>
          <th>vd prec</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>baseline</td>
          <td>37.4</td>
          <td>26.9</td>
          <td>94.8</td>
          <td>79.6</td>
        </tr>
        <tr>
          <td>balanced  (1,26)</td>
          <td>67.6 (80%)</td>
          <td>30.2 (12%)</td>
          <td>88.7 (-6.5%)</td>
          <td>75.3 (-5.5%)</td>
        </tr>
        <tr>
          <td>subsample balanced</td>
          <td>67.5 (80%)</td>
          <td>32.9 (22%)</td>
          <td>88.0 (-7.2%)</td>
          <td>70.9  (-11.7%)</td>
        </tr>
        <tr>
          <td>overweighting (1,1000)</td>
          <td>65.3 (80%)</td>
          <td>25.1 (-7%)</td>
          <td>84.5 (-10.9%)</td>
          <td>84.0  (5%)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>You want to predict on a set where that has times that are not seen
in training. This will give you how good your dater is. In my case
Valid gives 0.88 while training gives 0.99 or something. Using OOB
score I think is pointless as it behaves like cross-validation. The
same reason why we think cv is useless, we also think oob is useles.</li>
</ol>

<p><strong>Reason for difference in train and valid auc score</strong></p>

<ol>
  <li>
    <p>different imbalance in train and valid</p>

    <p>NO. Both have the same % of imbalance :) I would go on to say that
 the same will be the case of imbalance in the test dataset too as
 I get similar auc scores.</p>
  </li>
  <li>
    <p>‚Äúoverfitting‚Äù</p>
  </li>
  <li>
    <p>time info inside</p>
  </li>
  <li>
    <p>too many unnecessary rows</p>
  </li>
  <li>
    <p>precision and recall of ‚Äú1‚Äù too poor( 78% and 26% in valid). even
training is bad.</p>
  </li>
</ol>

<h2 id="plannensie">plannensie</h2>

<ol>
  <li>
    <p>USing GPU???? rapids?</p>
  </li>
  <li>
    <p>Get data processed (done)</p>
  </li>
  <li>
    <p>Do <a href="https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms/comments">AV like Konrad</a>, but based on suggestions from <a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/111510">Chris</a>
read part 1 and 2 of <a href="http://fastml.com/adversarial-validation-part-one/">this</a> as linked by Konrad</p>
  </li>
  <li>
    <p>Use XGB to predicta awards section without imputing.</p>
  </li>
  <li>
    <p>Userid identify</p>
  </li>
  <li>
    <p>EDA</p>
  </li>
  <li>
    <p>tackle NaNs</p>
  </li>
  <li>
    <p>feature importance</p>
  </li>
  <li>
    <p>Check validation strategy
come back and see how to circumvent gpu issues. I am thinking of going
to datacrunch today‚Ä¶ use GPU, get a bit faster computations‚Ä¶ How
much faster are they‚Ä¶ Please find out?</p>
  </li>
</ol>

<p>download the dater‚Ä¶</p>

<p>then we need to do other things‚Ä¶
check fastai install!</p>
:ET