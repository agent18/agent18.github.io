---
layout: post
comments: true
title:  "DP 1 hr per day!"
date:    15-05-2019 
categories: drafts
tags: DP, feedback, examples
permalink: /:title.html
published: true
---

## What are you confused about?

career plans!

I want to do 1k claims!

A lot in statistics

I want to check with an STM what I should be doing and then go on it
chento percento! I don't know what I am trying to do with this DS
thingy.

Take a wikipedia page and detail it out on say statistics!

## Goal 

is to do concrete thinking, i.e., thinking with examples.

## automation tasks needed


- numbered ()[] pnn
- 

## Sticking my head out!

## Entire article from start to finish challenge

> At 80,000 Hours, we (help people find careers that more effectively
> ‘make a difference’)[1], ‘do good’, or ‘have a positive impact’ on a
> (large scale)[2].

**Question**: Does 80khours do [1] on [2]?

**Example**: [Here](https://80000hours.org/about/impact/studies-of-career-change/#1-owen-cb) we see studies after studies about how peoples
plans "amazingly changed" to something else. Zero fucks seem to be
given about some numbers that would say things like, "if he continued
he would have got X impact, but he didn't and look at his current
impact! Suck it". Maybe there is more explanation later.

Making a difference on a large scale maybe thought of as things like
saving 1000 people over a lifetime. I will take it as a win then? or

> Here, we lay out what we mean by these phrases. In a nutshell
> ‘(making a difference)[3]’ is about (promoting the long-term welfare
> of everyone)[4] in ways that (respect the rights of others)[5].

Not a claim but a definition.

**Split**:

For [4], what does it mean? increasing the life expectancy? in
underdeveloped countries? 

From [AMF website](https://www.againstmalaria.com/WhyMalaria.aspx), we think of decreasing the number of deaths
from 1.5 million to 0.5 million and allowing 1 million people to
live. All this per year!

For [5], I am not really sure what sort of example would cut it. Right
to pray? right to eat? right to live in their environment, right to
healthcare, right to cheap education. 

But I still don't know what it means to do [3] in ways of [4]. I skip
it for now. Not worth spending time on this shit!

**Example**:

> This section also sketches out some of the ethical considerations
> that inform our advice. Much of our advice doesn’t entirely depend
> on these views, but we think it’s important to be transparent about
> them. If you want to read our practical suggestions about which
> global problems and careers to focus on, skip ahead.

So we skip the whole thing above, and we deal with it as it comes in
the passage. I am not interested if they have actually detailed
something out in the 

#### impartial concern

> When it comes to (making a difference)[1], we aim to be (impartial)[2]
> in the sense that we give (equal weight to everyone’s
> interests)[3]. This means we strive to (avoid bias against people
> based on their race, gender, sexuality or other identities)[4]. We
> also try not to (privilege any particular place, nation, time, or
> even species above any other)[5].

**Question**: Does 80khrs aim to give [3]?

**Example**: I will skip this, and move to the next sentence that has
more concrete explanations of what they mean with "[3]".

*Aim implies that an example needs to be provided which aims at being
impartial, but needn't succeed?*

**Question**: Does 80khours strive to avoid bias against race?

**Example**: 80khours supports to give donation to Effective Altruism
Foundation which in turn pumps funds to GiveWell, which pumps funds to
reduce deaths in sub-saharan Africa. *This only shows that it is not
biased against black people?*

**Question**: Does 80k hours strive to avoid bias agains people based
on gender?

**Example**: 80khours supports donations to sub-saharan africa where
gender doesn't come in the picture.

*Is this an example that is enough?*

**Question**: Does 80khours try not to [5]?

**Split**:

*What does above any other even mean?*

> Instead, we aim to have (moral concern for the interests of all
> sentient beings in proportion to how much they will gain or lose by
> our actions.)[6] This includes (those who are far away from us)[7]
> as well as (those who will (potentially) be members of future
> generations)[8].

**Question**: 80khours has [6].

**Example**: 80khours support donations to GiveWell (indirectly via
EAF) which primarily pumps fund to sub-saharan Africa. GiveWell looks
at the number of lives saved per <span>$</span> spent to identify the
best charities and pumps money to these charities. It happens that in
sub-saharan africa the <span>$</span> value per life saved is the
highest. This way they don't focus on everyone in need, but just the
people in need that they can help the most. 

**Question**:  Does the people 80khours focus on, [7]?

**Example**: 80khours is based in California but tries to focus on
health in poor countries (sub-saharan africa).

**Question**: Does 80khours focus on [8].

**Example**: 80khours rates AI danger, climate change and also factory
farming as top priority problems as it has the potential to end the
world completely reducing the population to 0. which will probably be
tolerable for this generation, but it's really scary what happens to
the rest of the world.

> From this perspective, we aim to increase (the expected welfare of
> others)[9] by (as much as possible)[10], (enabling more individuals to have
> lives that are long, healthy, fulfilled, and free from avoidable
> suffering)[11].

**Question**: Does 80k aim to increase [9]?

**Example**: 80k tries to get people to donate to GiveWell which
inturn pumps funds to AMF, which has reduced the number of deaths in
sub-saharan africa. 

**Question**: Has 80k, done [11]?

**Example**: 80k tries to get people to donate to GiveWell which
inturn pumps funds to AMF, which has reduced the number of deaths in
sub-saharan africa. 

> As individuals, (we all have other goals)[12] besides (impartially
> making a difference in this way)[13]. (We care about our friends,
> personal projects, other moral aims, and so on)[14]. But we think
> the (impartial perspective)[15] is an important one, and it’s what
> our (research and recommendations)[16] are focused on.

**Question**: Do we ALL care about other things, besides [13]?

**Example**: *Am like what does goal mean? based on the definition of
goals, can the claim be true or not true. then once you read the rest
I guess they are talking about caring!*

I care about making a significant design contribution like the entire
layout of the stage, for my work besides EA.


**Question**: [15] is an important one? 

**Example**: GiveWell suggests to put money in sub-saharan africa to
cure malaria and save X number of lives for Y <span>$</span>. GiveWell
doesn't support the money being used to solve homelessness in the US
for example. Thereby GiveWell Saves more lives.

**Question**: 80k's research is focused on [15].

**Example**: 80k, in it's important problem profiles, treats 'health in poor
countries' as well as 'factory farming and its inhumane treatment of
animals'.



#### longtermism (checking and correcting)

> (Homo sapiens)[0] is still an (infant species)[1]. We evolved
> around 200,000 years ago, and industrial civilization only
> began several hundred years ago; however, the average
> species lasts for 1-10 million years. With the (benefit of
> technology and foresight)[5], humanity could in principle survive
> for at least as long as the earth is habitable — (probably hundreds
> of millions of)[6] years.

**Question**: Is [0], [1]?

**Example**: Humans are around since 200k years ago. Average species
lasts for 1 to 10 million years.

**Question**: With [5], we could survive for [6], years.

**Example**: If technology is able to solve potential existential
risks like, climate change, meteor crashing into the earth, and also
able to eradicate life shortening diseases like malaria, cancer,
immunity syndromes etc... then potentially we can live as long as the
earth is habitable.

*How do you give an example of something you predict in
the future?*

> The (possibility of a long future)[1] means there will, in
> expectation, be (far more people in the future than there are alive
> today)[2]. (Impartial concern)[3] most likely implies (we should value
> their welfare as much as anyone’s)[4]. If (our actions)[5] can predictably
> (affect future generations in nontrivial ways)[5a], then because the
> (welfare of so many others would be at stake)[6], (these effects would be
> what most matter morally about our actions)[7].

**Question**: Does [1], mean [2]?

**Example**: 

If:
- we live in a planet where due to technology diseases are going to be
rare and threat from existential risks are low,

- and as a result if continue with the same growth rate 1.07%/year,

then: 
we will have 20 billion people by 21 billion people! (3 times within
100 years!

The Population of the world from 1804 has increased from
1 billion to 4 billion in \< 100 years starting from 1804. If
technology continues to 

*How do you give an example of something you predict in the future?*

**Question**: Does [3], most likely imply [4]?

**Example**: 

*I dont' know how to give an example for `deductions and
reasoning` like the above. or this is a definition*

*I see "most likely", I guess all this means is 1 example from my
side!*

**Question**: Can [5], do [5a]?

**Example**:  [According to here](https://climate.nasa.gov/news/2458/why-a-half-degree-temperature-rise-is-a-big-deal/), we see that causing a 2 degree
increase within this century can increase the sealevels [by 0.9m](https://www.activesustainability.com/climate-change/sea-level-rise-causes-and-consequences/) which
poses an increased chance of floods starting with coastal regions
around the world, expected to displace 200-300 million people.

**Question**: if [5] can [5a], then [7] is because of [6].

**Example**: *"Because", so we skip for now*

As there are many billions more lives at stake this implies that this
is one of the most important problems.

---------------------

**Breaking down 7**

> (these effects)[a] would be what (most matter morally)[b] about our (actions)[c]

For A, we think of the rise in sea levels and potential increase in
chances of floods. i.e., death of billions of lives and potential
failure to continue mankind

For B, we think of billions of lives saved (over a million years)

For C, we think of not doing anything about the rise in temperature,
i.e., no change in policies to control rise in temperatures.

> If (this)[0] is correct, then (approaches to improving the world)[1] should be
> evaluated mainly in terms of (their potential long-term impact, over
> thousands, millions, or even billions of years)[2]. Making these
> evaluations is part of an emerging field of study called
> longtermism.


For "[7] is correct", we think of "saving n number of lives over a
billion years" is what matters to us.

**Claim**: [1] should be evaluated based on [2], if [0]or[7] is true.

**Question**: Should [1], be evaluated based on [2], given [7] is correct?

**Example**: 

Working on keeping the increase in temperature within 2 degrees within
the century could potentially save billions of lives over millions of
years, as the earth would still be available for habitation. Instead
if we choose to improve the quality of lives of the homeless in the
US, we might not end up keeping the temperature within 2 degrees and
possibly loose billions of lives. 

*This one paragraph above took me 2 hrs atleast over multiple days, I
think primarily because I didn't understand [7]. The clue was in
properly understanding what 7 meant, hence the breakdown. Your
comments? Is the depth at which this example was covered satisfactory?
i.e., do we need to know what "potentially save billions of lives over
millions of years" is exactly?*

------------------------------------

> It’s difficult to (predict the long-term effects of our actions)[1],
> but we think it’s clear that (the interests of future
> generations)[2] are neglected by (most people and institutions
> today)[3], suggesting there are (untaken opportunities to
> help)[4]. We also think (some of our actions)[5] do have( very
> long-term effects)[6] — at the very least we can affect the
> (probability of existential risks)[7], as covered in the next
> section, and there may be (other ways to affect the future as
> well)[8]

**Claim**: Difficult to predict [1].
 
**Question**: Is it Difficult to predict [1]?

**Example**: One estimate of temperature increase is [4 degrees](https://www.givewell.org/shallow/climate-change/impacts#Malaria). The
uncertainty lies between 2.4 to 6.4 degrees Celsius. Damages from
climate change are proportional to the square of temperature
change. So it is difficult to predict what is going to happen a 2
degree increase leading to 9 meters sea level increase or a 6 degree
increase which is going to be "much much worse".


*I Googled climate change for a while, but am not able to
come up with a proper example. The example, would need to look like
this I guess: Look at climate model A,B,C and see the variance in the
estimates? or something that shows the uncertainty in prediction of
likelihood of a disaster*

**Claim**: [2] is neglected by [3].

**Question**: Who or What are the [3], that neglect [2].

**Example**: There are about 100 people worldwide working on "control
problem" for AI, so that machines can pursue "realistic human goals
safely". One simple example of the harms of AI could be that AI thinks
killing people is a better way to contain a virus from growing. AI
could thus affect the future generation.

**Claim**: [2] neglected by [3] suggests [4].

**Question**: Is there [4], given [3] neglect [2]?

**Example**: "Positively shaping AI" has 100 people working it with a
budget of 100m$ for something that could "potentially save billions of
lives." Every additional person working here could potentially
contribute to saving billions of lives.

*I don't know if the above example makes sense for "given [3] neglect [2]"*

**Claim**: [5] has [6].

**Question**: What are the [6], of [5]?

**Example**:

According to [here](https://www.nature.com/scitable/knowledge/library/what-happens-after-global-warming-25887608), it takes much greater than 100 years before
the greenhouse gases subsude to levels that bring the temperature down
by 1 degree even. We are expecting a 6 degree rise with a 10%
probability by 2100. The number of people living in water-stressed
river basins increases by 

*Actually I can't find one spot where I can identify it as a long term
impact". In most cases either the number of lives are not accounted
for or the outcome is better with CC than without!*



**Claim**: We can affect [7], other than 7. 

**Claim**: There may be [8].




> We remain unsure about (many of these arguments)[1], but overall
> we’re persuaded that (focusing more on the very long-term
> effects of our actions)[2] is (one of the most important ways we can
> do more good)[3]. Such a (radical claim)[4] requires (much more
> argument)[5], and we outline the (considerations for and against
> it)[6], as well as (list further reading)[7], in our full article on
> this topic.

**Claim**: We are unsure about [1].

**Question:** Are we unsure about [1]?

**Example**: I don't know what they are unsure about and why! 

For [1] we think of, "With the benefit of technology and foresight,
humanity could survive at least as long as the earth is habitable -- a
few 100's of millions of years."

**Claim**: We are persuaded that [2] is [3].

**Question**: Are we persuaded that [2] is [3]?

**Example**: We can save a few 10's of people by focusing on homeless
people in US, or we can save possibly billion people by focussing on
climate change. The later keeps in mind future generations!

**Claim**: [4] requires much more of [5].

**Question**: Does [4], require much more of [5]?

**Example**: There is probability involved. There are no numbers
given. *is this an example?*

**Claim**: We outline [6], as well as [7] in [this article](https://80000hours.org/articles/future-generations/).
	
**Question**:  

**Example**: 

Need to reflect on the article! no internet access for now!


### Moral uncertainty and respecting rights!


> As covered, we think that the most important thing for us to
> focus on from an impartial perspective is increasing the long-term
> welfare of everyone, such as by helping people have longer, more
> fulfilling, and happier lives. However, we are not sure that this is
> the only thing that matters morally.

> (Some moral views)[1] that were widely held in the past are regarded
> as (flawed or even abhorrent today)[1a]. (This)[2] suggests we
> should expect our (own moral views)[3] to be (flawed in ways that
> are difficult for us to recognize)[4]. What’s more, there is still
> (significant moral disagreement)[5] within (society)[6], among
> contemporary moral philosophers)[7], and, indeed, (within the 80,000
> Hours team)[8]. It’s also (extremely difficult)[9] to know all the
> (ethical implications of our actions)[10], and (grand projects to
> advance abstract ethical aims)[11] often go badly.

**Claim**: [1], that were widely held in the past are regarded as
[1a]. 

**Question**: Were [1] held in the past abhorrent today?

**Example**: There was a time when blacks were to be seen only as
slaves, but now it is widely unpopular to do anything like that!

**Claim**: [2] suggests [3] to be [4]. 

**Question**: Does [2] suggest that [3] is [4]?

*This is reasoning. How do you give an example that it suggests [3]
is flawed; Skipping this for now!*

**Claim**: [3] could be [4]

**Question**: Is [3], [4]?

**Example**: Until a few years back all I was focusing on was
Women and not on things like EA which is what I should have always
been focusing on. If it were not for an STM who explained to me with
countless hours how my moral views were flawed and needed to be
oriented towards EA, I am not sure I would have recognized it.

**Claim**: There is [5] within [6].

**Question**: Is there [5] within [6]?

**Example**: Recently Anti-abortion laws were passed in Alabama,
Missouri and Georgia. The law said that even in the case of rape the
abortion cannot go through. This is a predominantly Republican view
and Democrats seem to be completely against that.

**Claim**: there is [5] within [7].

**Question**: Is there [5], within [7]?

**Example**: *Can't find an example for it. Looked at Robin Hanson and
Eliezer and also things to do with Peter Singer*

**Claim**: There is [5] within [8].

*Not sure I can come up with this without needless research on the
whole 80000 website. So I skip this!*

**Claim**: It is [9], to know all [10].

**Question**: Is it [9], to know all [10]?

**Example**: I work for a world leader in Lithography and I am able to
donate 10% of my income. I think I have saved at the end of the year
approximately one life. What I don't know is how many lives I have
taken as a result of promoting this company. For example, this company
needs material and sources from the earth, I don't know where its
materials are sourced from and if for example child labor was
involved, or poor conditions of work and health leading to reduced
life of someone else. Or another example would be the depletion of
resources in the end bringing the earth to a stop a few days
earlier. I have no way of estimating it and hence it is difficult.

**Claim**: [11] often goes bad!

**Question**: Does [11], often go bad?

**Split**: 

For grand projects, we think of Heifer International and their giving
of livestock to "people in need". 

For abstract ethical aims, we think of their aim to "improve the
lives" of people or "bring them out of poverty"

For goes badly, we think of the lack of evidence from Heifer's side to
prove the claim that : "A cow is better for you than anything else you
could buy with what the cow costs". 

[Here there are more "grand schemes"](https://blog.givewell.org/2009/12/28/celebrated-charities-that-we-dont-recommend/) that GIVEWELL doesn't support aka,
"go badly".

*Once more a reminder that without evidence you are nothing!*

------------------------------------------------------------------

> As a result (of ("grand projects going badly")[1]), we think it’s
> important to be (modest about our moral views)[2], and in the (rare
> cases where there’s a tension)[3], try very hard to (avoid actions
> that seem seriously wrong from a common-sense perspective)[4]. This
> is both because (such actions)[5] might be (wrong in themselves)[6],
> and because they seem likely to lead to (worse long-term
> consequences)[7].

> More generally, we aim to factor ‘moral uncertainty’ into our views
> and to uphold cooperative norms. We do this by taking into account a
> variety of reasonable ethical perspectives, rather than simply
> acting in line with a single point of view.

**Claim**: Due to [1], it is important to be [2].

**Question**: Why does [1], lead to [2] being important?

**Example**: *because*

**Claim**: It is important to be [2].

**Question**: Why is it important to be [2]?

**Example**: 

For moral views, we think of "a cow is better for poor people than
giving them cash for the cows worth".

For not modest, we think of Heifer International's continued claim to
support giving of livestock, without demonstrating the effectiveness
of such a gift.

For important, we think of 0 funds being delivered to Heifer from
GiveWell. And possibly people taking out funds or demanding more
evidence from them.


*For important I wanted to say something about the wastage of funds as
HI spends on something whose effectiveness it doesn't know, but I
don't have any numbers!*


Some mental masturbation perhaps! but whateves. Am I actually enjoying
it? I feel happy when I am able to come up with an example as a result
of my research! Sometimes I mentally masturbate, procrastinate while
looking for info!

**Claim**: Due to [1], during [3], try very hard to [4].

*The English is very troubling. Are they bad writers? or what? How
would any one know what they mean by common sense perspective (I guess
unless they read 80khours in and out, aka, articles related to it!). With my
current example above, it's not common sense I think to trust nothing
but controlled randomized experiments. Period!*

*In hindsight I think I am working on a summary, and maybe that is the
reason I feel like dying! ;)*

**Example**: *because!*

**Claim**: During [3], try very hard to [4]!

**Question**: During [3], should we try very hard for [4]?

**Example**: *No idea! Not gonna break my head over this English! But
as I read some related articles on further reading, maybe they are
talking about...* *this whole common sense thing is wierd. I am trying
to look in my current commong sense thinking which has been quite
upgraded due to an STM*

During WWII, there was a lot of tension between nazi Germany and
others! And many Germans might be in favor of killing other races. In
such cases it would be worthwhile to try very hard not to kill people?

**Claim**: During [3], try very hard to [4], because [5] might be [6].

**Example**: *because*

**Claim**: During [3], try very hard to [4], because they seem to lead
to [7].

**Example**: *because*

> More generally, we aim to (factor ‘moral uncertainty’ into our views)[1]
> and to (uphold cooperative norms)[2]. We do this by (taking into account a
> variety of reasonable ethical perspectives)[3], rather than simply
> (acting in line with a single point of view)[4].

**Claim**: We aim to do [1].

**Question**: Does 80k aim to do [1]. 

*How am I supposed to come up with examples for these? Am I? It's so
fucking hard. What is the goal here? I am not even sure if there is a
point to this! Please tell me your highness*

**Example**: 


**Split**: For moral uncertainty I think of my uncertainty in the
value of animal lives, in the value of insects, bees, cockroaches.

How do you decide which theory to follow.... total util has the
drawback that it will have some really unhappy people



For moral uncertainty, we think of not know if stealing in a
particular case is wrong!


**Claim**: We aim to do [2].

**Claim**: We do this by [3] instead of [4].

**What is unclear?**

- I don't have an example for [1]. I don't know how it looks. As a
  result I don't know how 80khours does [1] by [3] instead of [4].
  
  I spent 2-3 hrs on this and still not good. I will move on for
  now. In parallel I should read about it, find examples about
  reasoning amidst moral uncertainty.
  
  
**Claim**: We should factor moral uncertainty by using multiple
theories instead of just sticking to one.

**Example**: ???
  

> We think that a rights framework captures much of what matters in
> these considerations. So we formulate the one-sentence version of
> our views as: promoting long-term welfare while respecting the
> rights of others.

> These are some of the reasons that we think it’s so important to
> respect the rights of others at the same time as aiming to promote
> long-term welfare.

Skip! Lite!

## Global Priorities

> Now that we have a sense of (what ‘making a difference’ means)[1], we can
> ask (which career options make a difference most effectively)[2].

**Claim**: We have a sense of [1]. 

**Question**: Do we have a sense of [1]?

**Example**: 80khours promotes working on climate change primarily
because of the risks it poses to the lives of future generations. One
estimate suggests [300 million people might need to be
displaced](https://www.givewell.org/shallow/climate-change/extreme-risks). To put that in perspective, we know that displacing 6
million people from Syria, [was already quite hard for the refugees as
many countries were not accepting refugees.](https://www.npr.org/sections/parallels/2018/03/09/589973165/europe-does-not-see-us-as-human-stranded-refugees-struggle-in-greece?t=1559420984970)

**Claim**: Knowing [1], leads to asking [2].

**Question**: Does knowing [1], lead to asking [2]?

**Example**: Working as a management Consultant can earn money and not
do much for the 300 million people about to be displaced in 2100,
where as becoming president of US, you could change policies to bring
the temperature increase within controllable limits and promote clean
technology.


> We think that probably the most important single factor that
> determines the (expected impact of your work)[1] is the (issue you choose
> to focus on — whether that’s climate change, education,
> technological development or something else)[2].

*Rant: I guess they are trying to segue into their page which talks
about different issues. It's been a few hours of having worked on this
statement, but, I still don't get it. Don't get what? I am not able to
wrap my head around why the issue you focus on most important? Like
Why? As an STM oftens frets about to me, "measuring anything but the
actual outcome you are interested in is all hogwash!"* 

*Now Having said that, disprove the motherfucker!*

**Claim**: [2] is most important factor, which determines [1].

**Question**: Is, [2] really the most important factor when it is
judged based on [1].

**Example**: 

Whether you choose climate change or homelessness or AI, makes quite a
difference in your delivered impact. One can save about a few hundreds
of thousands, the other can save about 100's of people, and the last
one potentially, millions. 

OK! But what about the probability of actually making an impact at
said field, Isn't that an important factor too? If we work in 'making
AI safe' it is estimated below that the impact is about 6.3 million
people. But what are the odds of me being hired by MIRI because I
don't have the skill. In my case, I would think 1%. In that case 1% of
6.3 million is 63000. 

In this case atleast, personal fit doesn't seem to make a big
difference. So it looks like 'the issue' could indeed be a very
important factor! Oops!

But none of this actually means anything, unless I factor this with
say my capabilities or my probability of succeeding.

Moreover, what I am trying to say is the expected impact of my work
100% depends on the expected impact of my work only. Just because I
work in CC or AIsafety doesn't automatically catapult me. There are
several other factors like personal fit (i.e., my probability of
success). Just looking at one seems to be foolishness as with poor
personal fit, you could suck so bad that it might not be worth your starting!

To conclude, the expected impact of my work, is dependent, I don't
know how to say if it is the most important factor. As one factor
without the other seems pointless!

*Based on my example, 80khours seems to have been seriously
exposed. Now what! Can you comment on this?*

*To be re-written*

> It’s harder to have (a big impact on commonly supported causes)[1]
> because (work in most areas has diminishing marginal returns)[2]. In
> other words, if (an area already receives plenty of attention)[3],
> then (there will usually already be people working on the most
> promising interventions)[4].

**Claim**: It's harder to have [1].

**Question**: Is it harder to have [1]?

**Example**: If you look at making AI safe, there are 100 people
working on it, barely common. If we assume that the current chances of
making AI safe is 0.1%, then we are able to save say 7 million people
effectively. 80khours claims if an additional 100 people work on it,
the chances of making it is 1% more, aka 70 million lives could be
saved. Per person added we seem to have about 6.3million in effective
people saved on average. Talk about BIG!

If you look at Climate Change, there is a 10% chance of 7 billion
people being affected. i.e., effectively 700 million are in
danger. Assuming that there are only 700 people working on CC---which
is probably not accurate at all considering the 10 billion
<span>$</span> in funding---the effective possible people saved is 1
million (highly conservative estimate). 

**Claim**: Its' harder to have [1], because of [2].

*because!*

**Claim**: If [3], then [4].

**Question**: If [3], then ____?

**Example**: 

For [3], we think of Climate Change(CC), it receives 10b
<span>$</span> in funding.

For [4], we think of the Paris Climate Agreement whose whole aim is to
keep temperature increase "well below" 2 degrees enforced by 194
"states" spanning 88% of the Green house emissions.

> Although we’d like to see more people working on many global
> problems, we think (additional people)[1] can have the most impact
> by focusing on the (issues that are most neglected)[2] relative to
> (the magnitude of the stakes)[3] and (the number of promising
> opportunities to make progress)[4].

**Claim**: [1] can have the most impact by working on [2], in relation
to [3].

**Question**: Can [1], have the most impact by working on [2], in
relation to [3]?

**Split**: 

For [2], we think of `making AI safe` with only 100 people working on
it. For comparison we also think of `institutional decision making`
with only 100 to 1k people working on it. Or for that matter we look
into `Global Priorities Research` which also seems to have similar
funding of 5m-10m <span>$</span> for it's spending.

For [3],  we think of expected savings/person.

|                                | AI  | Decision making | Priorities |
|--------------------------------|-----|-----------------|------------|
| Total value of solving         | 1%  | 0.5%*           | 0.5%*      |
| Doubling effort solves         | 1%  | 1%              | 1%         |
| Neglectedness                  | 7   | 8               | 9          |
| num of additional people       | 100 | 500*            | 500?       |
| expected people saved/person** | 7k  | 700             | 700        |

/* - averaged value between upper and lower limit given in 80khours
? -  "educated" guess
/**- expected savings based on assumption that total damage would be 

*And without such clarity in numbers how the fuck do you even begin to
compare different interventions! I wish 80khours used same numbers
across everywhere and not vague terms like "[improves value of future
by 1%](https://80000hours.org/problem-profiles/improving-institutional-decision-making/#what-can-you-do-in-this-area)"*


**Example**: 

In order to the make the most impact one would choose AI (7k people),
which seems to be based on "most neglected issue" and "the magnitude
of the stakes".

**Claim**: [1] can have the most impact by working on [2], by
focusing on '[3] and [4]'.

**Question**: Can [1], have the most impact by working on [2],
focusing on '[3] and [4]'?

**Split**: 

For [2], we think of `making AI safe`, `institutional decision
making` and `global priorities research`

For [3], we have seen above

For [4], number of opportunities, for me (I assume), which seem to be
the same for all issues below:


|                          | AI | Decision Making | Priorities |
|--------------------------|----|-----------------|------------|
| Policy                   | N  | N               | N          |
| Earning to give          | Y  | Y               | Y          |
| Research/PhD             | N  | N               | N          |
| Work at some grant giver | N  | N               | N          |
| non-academic  research   | N  | N               | N          |
| complementary roles      | M  | -               | N          |

Considering [3], and [4], it appears that [4] does not seem to add
value in the above case. It appears that the claim is true but [4],
doens't seem to add any value.


*Am feeling different, I am ready to work on this late even, usually
it was like start and get over with. When I start I feel like this is
my thing, This is pandians turf! I feel like its my basketball, I know
the smell, I am ready, I feel like a guy who PUA's every single day! #
not afraid!*


> So, what are the most neglected and solvable issues that have the
> biggest stakes for long-term welfare?



### Current View

> In the 1950s, the (large-scale production of nuclear weapons)[1]
> meant that, (for the first time, a few world leaders gained the
> ability to kill hundreds of millions of people)[2] — and possibly
> many more if they triggered (a nuclear winter)[2a], which would make it
> nearly impossible to grow crops for several years)[3]. Since then,
> the possibility of (runaway climate change)[4] has joined the list of
> (catastrophic risks facing humanity)[5].

**Claim**: [1] meant [2].

**Question**: Does [1] mean [2]?

**Example**:

For [1], we think of the sudden rise in capacity of the US nuclear
weapons from [2 to 299 from 1945 to 1950](https://en.wikipedia.org/wiki/Historical_nuclear_weapons_stockpiles_and_nuclear_tests_by_country).

For [2]: [One nuclear weapon can destroy hundreds of thousands of
people in a "major city"](https://www.un.org/press/en/2009/gadis3385.doc.htm). With 299 weapons the could kill up to 2
million people.

*I am unable to find the number of people possible to wound with one
nuclear weapon in 1950's.*

**Claim**: [2a] results in many more deaths

**Question**: Does [2a]  result in many more deaths

**Example**: 

The smallest nuclear powers today have 50 Hiroshima-sized nuclear
weapons, which when used could cause the temperature of earth to rise
several degrees over large areas in North America and Eurasia, including most of
the grain-growing regions for more than a year. Resulting in famines
and starvation and possible death. 

[Source](https://www.sciencedaily.com/releases/2006/12/061211090729.htm).

*Yes not many numbers are given but I am unable to go deeper, as this
already took an hr!*

*As far as I am reading there seem to be no nuclear winters in the past+*

--------------------------------------------


**Claim**: Since 1950 [4] has joined [5].

**Question**: Since when did [4], join [5]?

**Example**: By late 1960's the number of scientific papers published
on the Climate Change leapt from roughly 3 to 20 papers
year. ---[source](https://www.historyextra.com/period/modern/climate-change-warnings-history/)

> During the next century we may develop (new transformative
> technologies, such as advanced artificial intelligence and
> bioengineering)[1], that could bring about a (radically better future)[2] —
> but may also (pose grave risks)[3].

**Claim**: We may develop [1].

**Question**: Are we going to develop [1]?

**Example**: An AI system named AlphgGo, in the course of the year, it
became the best player in the world with a 60-win streak. Why this is
impressive(transformative) is because it is not possible to win the
game with brute force, but only with strategic intuition, which the AI
developed. --- [Source](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/)

**Claim**: [1] can cause [2].

**Question**: Can [1], cause [2]? or How many people can AI save?

**Example**: Assuming everything can be cured, and that it is just a
matter of time and understanding, AI could cure Cancer which killed
9.6 million people in 2017.


**Claim**: [1] can cause [3]. 

**Question**: Can [1], cause [3]?

 **Example**: In an autonomous assignment to reduce the incidence of
 Cancer, an AI decides to kill human beings before they can grow old
 to develop cancer as this was the only way to drive cancer rates to
 0.
 
> Previously, we focused on improving (near-term global health)[1],
> and we still think it’s (an important cause)[2]. However, over the
> past eight years, we’ve come to realise that (these technological
> developments)[5] mean that the (actions of the present generation)[3]
> may put the (entire future of civilization)[4] at stake.

~~Claim: In the past 80khours focussed on [1].~~

**Claim**: [1], is still [2]. 

**Question**: Is [1], important?

**Example**: Every year around 10 million people in poorer countries
die of curable illnesses that can be very cheaply prevented (100
<span>$</span> to 1000 <span>$</span> per year) including malaria and
HIV.

**Claim**: [5] implies, [3] may result in [4].

**Question**: 

**Example**: Technologies such as AI could produce grave issues, if
not enough effort is put into it (for example, in the control
problem). In the future if AI, is much more powerful and intelligent
than a human brain, it could choose to simply eliminate rival
intellects aka existential risk.

> In combination with our (growing confidence in longtermism)[1], this
> has persuaded us that the most important challenge of the next
> century is likely to be to (reduce ‘existential risk’ — events
> that would drastically damage the long-term future of humanity.)[2]

**Claim**: [2] is the most important challenge.

**Question**: Is [2] the most important challenge? 

**Example**: AI could potentially wipe out the entire population and
the future generations, if not "enough work" is done on the 'control
problem'.

<!-- 13 in 3 hrs [7] claims -->

--------------------------------

> There are several types of (existential risk)[1]. Currently, we’re
> most concerned by the risk of (global catastrophes)[2] that might
> lead to (billions of deaths)[3] and (threaten to permanently end
> civilization)[4]. There are several reasons we think it’s
> overwhelmingly (important to address these risks)[5].

**Claim**: There are several types of [1].

**Question**: What are the several types of [1]?

**Example**: *I don't know what they are, can't find something online
related this as well*

**Claim**: 80khours is currently, concerned by [2] that might lead to
[3] and [4].

**Question**: Is 80k hours concerned by [2]? that might lead to [3]
and [4]. 

**Example**: 80khours scores the issues due to climate change as
14/16, as compared to "improving health in poor countries" scored at 13/16.

~~Claim**: [2] might lead to [3] and [4].~~

**Claim**: There are several reasons for [5].


**Question**: Are there several reasons for [5]?

**Example**: These are written below...

> First, because of the (power of the new technologies)[1] noted above,
> we think that the (probability of this kind of catastrophe occurring
> in our lifetime)[2] is too big to ignore.

**Claim**: [2] is too big to ignore because of [1].

*because*

**Claim**: [2] is too big to ignore. 

**Question**: Is [2], too big to ignore?

**Example**: 

For [2], we think of a [5% chance](https://en.wikipedia.org/wiki/Global_catastrophic_risk) of a super intelligent AI ending
in human extinction i.e., a loss of billions of lives multiplied by 5%
resulting in expected loss of 70m people (not counting the future
generations)

> Second, it seems like (such an event)[3] would be among the (worst
> things that could happen)[4]. This is especially true if one takes a
> (longtermist perspective)[5], because (extinction)[6] would also mean
> the (loss of the potential welfare of all future generations)[7].

**Claim**: [3] would be [4].

**Question**: Would [3] be [4]?

**Example**: As seen in the previous example, a loss of 70m
(not taking into account the future generations) is expected.

**Claim**: It is especially true if you take into account [5].

**Example**: With a 5% chance and a 11 billion population, we already
have an expected loss of 110m people. If we consider future
generations, this number goes upwards only based on the expected
population and the time they have on earth. For example if we assume a
population of 20b by 2200 without the catastrophe, then we shall have
an expected loss of 9b and 110m people.

<!-- 12 in 1 hr 20 mins adn 6 claims -->

-------------------------------------------------------

> Third, (some of these risks)[1] are highly neglected. For instance,
> the fields of (AI safety and catastrophic biorisk)[0] receive the
> attention of perhaps only 100 dedicated researchers and
> policymakers, compared to the (billions or trillions of dollars)[2]
> that (go into more familiar priorities)[3], such as (international
> development)[4], (poverty relief in rich countries)[5], (education,
> and technological development)[6]. This makes them perhaps more than
> (a factor of 1000 more neglected)[7].

**Claim**: [1] are highly neglected.

**Example**: AI safety receives the attention of 100 dedicated
researchers, for a probability of extinction of [5% by 2100](https://en.wikipedia.org/wiki/Global_catastrophic_risk).

**Claim**: [2] is spent on [3].

**Question**: The US spent 600 billion$ on military in 2015

**Claim**: [2] is spent on [4]

**Example**: US spends 5.7 billion in foreign aid for Afganistan

**Claim**: [2] is spent on [5]

**Example**: [45m](https://www.google.com/search?q=number+of+people+receiving+poverty+aid+in+US&oq=number+of+people+receiving+poverty+aid+in+US&aqs=chrome..69i57.9752j0j7&sourceid=chrome&ie=UTF-8) people are in poverty receiving aid from US and
living in the US. [18k](http://federalsafetynet.com/poverty-and-spending-over-the-years.html) <span>$</span>/person is being spent by the
US government which is in total 810 billion <span>$</span>.

**Claim**: [2] is spent on [6].

**Example**: "Total expenditures for public elementary and secondary
schools in the United States in 2014–15 amounted to $668
billion". --- [Source](https://www.google.com/search?ei=axIAXaK-Ho6ckgXiybHwAw&q=us+spending+on+education&oq=us+spending+on+education&gs_l=psy-ab.3..0l10.52410.55401..55575...0.0..0.69.1181.24......0....1..gws-wiz.......0i71j35i39j0i67j0i20i263.b7rFg-obFDg)

**Claim**: This makes [1] a factor of 1000 more neglected.

**Question**: How neglected does this make [1]?

**Example**: 10m <span>$</span> is spent on AI safety risk whereas 668b
<span>$</span>  spent on education. 668b/10m=66000

> This neglect suggests that a (comparatively small number of
> additional people working on these risks)[1] could significantly
> reduce them. We suggest specific ways to help in the next section.

**Example**: 

If we double the number of people working in AI safety, we can reduce
the risk by 1%. Which amounts to about 70 millions effective lives for
an extra 100 people.

> This said, we remain (uncertain about this picture)[1]. (Many of the
> ‘crucial considerations’ that led us to our current priorities)[2]
> were only (recently identified and written about)[3]. We may yet
> learn of (other ways to increase the probability of a positive
> long-term future and reduce the chance of widespread future
> suffering)[4], that seem (more promising to address than the
> existential risks we currently focus on)[6].

**Claim**: 80khours is [1].

**Question**: Is 80khours [1]?

**Example**: 80khours [emphsizes on working on "global priorities
research"](https://80000hours.org/problem-profiles/global-priorities-research/), to identify what we should work on AI safety more or
climate change more (for example). 

**Claim**: [2] were only [3].

**Question**: Was [2], [3]?

**Example**: 80khours wrote [this article](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/#what-can-you-do-to-help) on AI safety on April
2015.

**Claim**: there may be [4] than [6].

**Example**: It could turn out that focusing on green energy
excessively is the way to go to for the future rather than identifying
or predicting the temperature rise, as it will always have a ton of
uncertainty.

*I can only give a hypothetical example here! right?*

> For these reasons, we also work to support those creating the (new
> academic field of global priorities research)[1], which (draws on
> economics, philosophy and other disciplines)[2] to work out (what’s most
> crucial for the long-term future)[3].

**Claim**: 80khours supports GPR.

**Example**: 80khours provides its research on which of the issues are
most relevant.

|                   | AI Safety | Nuclear security |
|-------------------|-----------|------------------|
| scale       (16)  | 15        | 15               |
| neglectedness(12) | 8         | 3                |
| solvability (8)   | 4         | 3                |


**Claim**: GPR "draws" on economics, philosophy and other disciplines
to determine [3].


**Split**: 

For GPR, we think of 80khours suggesting new people to work on AI
safety instead of Climate Change, which will result in saving many
more lives over the long-term-future.

For economics, we think of 80khours intention to look at [factors like
scale, neglectedness and solvability](https://80000hours.org/articles/problem-framework/#introducing-how-we-define-the-factors).

| factors           | AI Safety | Climate Change |
|-------------------|-----------|----------------|
| scale       (16)  | 15        | 14             |
| Neglectedness(12) | 8         | 2              |
| Solvability (8)   | 4         | 4              |


For Philosophy we think of, 80khours and their focus to look at
long-term-future instead of near-term-future as the former will save a
lot more number of people.

For other disciplines, we think of 80khours looking into Climate
Change and AI to understand the values of the different factors

**Example**: 80khours suggests that working on AI safety is better
than working on Climate Change based on the fact that the
neglectedness is very high, and the amount of people going to die as a
result would be large.

-------------------------------------------------------------------------------

<!-- We start work here 15 jun -->

## Work starts here. Finish 40-50 phrases today

> In addition, we encourage people to (work on ‘capacity-building’
> measures)[1] that will (help humanity manage future challenges, whatever
> those turn out to be)[2]. These measures could involve (improving
> institutional decision making and building the ‘effective altruism’
> community.)[3]

**Claim**: [1] that will [2], exists

**Question**: Does [1] that will [2] exist?

**Split**: 

For [1], we think of [Niel Bowerman](https://uk.linkedin.com/in/nielbowerman), who is seen to work in different
organizations (CEA) in the role of fund-raising and growing organizations,

For [2], we think of him growing EAO's for "organizations of the
future" say in 'reducing animal cruelty`. He is currently seen in CEA, FHI and
also 80000hours (so clearly his skill is transferable).

If what Niel does today is "helping humanity manage future
challenges", if he continues his work in other EAO's, it seems to
continue to "help humanity manage future challenges."

**Claim**: It is good for people to [1] which will [2].

**Question**: Is it good for people to [1] which will [2]?

**Example**: 

We need to look at impact: Niel Bowerman is one of the 20 staff of
80khours. He would have an impact of /~97k<span>$</span>  per year, in the best case
saving 25 lives a year. This over the next 30 years would be 750
people.

**97k per year**
(There is an estimate of [97k$ per year](https://80000hours.org/2016/08/reflections-from-a-givewell-employee/) in impact (or donations),
for someone working at GiveWell. We assume EAO's have similar impact.)


*I could give an example showing a statement where 80khours encourages
people as it's the "want" that is in question here. Does 80k want or
not. OR was I expected to just look at "it is good to work [1] that
will [2]?" Can you comment?*

<!-- [1] hr for [2] claims and [2] phrases -->


-------------------------------------------------------------------------------


**Claim**: [1] could involve [3].

**Question**: Does [1] involve [3]?

**Example**: Niel Bowerman working in different EAO's to grow them,
could be considered as [3].


> Some other issues we’ve focused on in the past include (ending
> factory farming)[1] and improving (health in poor countries)[2]. They seem
> especially promising if you don’t think (people can or should focus
> on the long-term effects of their actions)[3].

**Claim**: 80khours focused on [1] and [2] in the past.

**Question**: Has 80khours focused on [1] and [2] in the past?

**Example**: 80khours wrote [articles](https://80000hours.org/problem-profiles/) on factory farming and
global poverty

**Claim**: [1] seem promising if you don't think [3].

**Question**: Is [1], promising if you don't think [3]?

**Example**: 

50 billion animals die each year. 1k people are working on
it. "Expected value with intense efforts for the future of humanity"
is [0.05% (average)](https://80000hours.org/problem-profiles/factory-farming/#think-you-should-work-on-something-else), i.e., `0.0005*7 billion human lives` i.e.,
3.5m expected human lives. Assuming that doubling the effort leads to
reducing the size by 1%, we have 35 expected people lives saved for
one additional person working over his lifetime.

	3.5*E6 expected people lives * 1% / 1000 people
	= 35 expected people lives

Which sounds to be ridiculously low as working in GiveWell for 2 years
could beat this with `97k$/4k$*2=48.5 expected people lives`

So, does not look promising!

**Claim**: [2] seems promising if you don't think of [3].

**Question**: Is [2], promising if your don't think [3]?

**Example**: It appears that I can personally gain skills in the
coming years to work in an EAO like 80khours, claiming
[97k](https://80000hours.org/2016/08/reflections-from-a-givewell-employee/)<span>$</span> per year of donations resulting in greater than
700 lives over a career.

This is much more than the 200 expected lives from my current job.

<!-- [4] phrases [3] claims [1] hr -->

------------------------------------------------------------------

> There are (many issues)[1] we haven’t been able to look into yet, so
> we expect there are other (high-impact areas we haven’t
> listed)[3]. We have a (list of candidates)[4] on our (problem
> profile page)[5], and we’d be excited for (people to explore some of
> these as well as other areas that could have a large effect on the
> long-term future.)[6] These areas can be (particularly worth
> pursuing)[7] if you’re (especially motivated by one of them)[8]. We
> cover this more in the section on ‘personal fit’ below.

**Claim**: There are [1], that 80khours has not looked into yet.

**Question**: Are there [1], that 80khours has not looked into yet?

**Example**: [Criminal Justice Reform, medical research into how to
slow aging etc...](https://80000hours.org/problem-profiles/)

**Claim**: There could be other [3].

**Question**: Could there be other [3]?

**Example A**: Until a few years back 80khours thought that the best
places to work on were "reducing near-term life risks aka reducing
global health risks" but when they explored that there were global
catastrophic risks that could kill the entire planet and future
generations, they have now changed their stance on where people should
be working considering the impact.

*In this case, I could give a hypothetical example or an example from
the past? can you help with what's good here? and why?*

**Example B**: If medical research into 'how to slow aging' seems
largely promising (90% chance of making it with 10b <span>$</span>
with a 100 people extra), in delivering a mechanism that doubles the
human life expectancy, it could be beneficial to work on it as it
could save `95% * 7b expected people lives/100 = 66m expected people
lives per person working on it`

**Claim**: 80khours has [4] on [5].

**Example**: https://80000hours.org/problem-profiles/

**Claim**: It's a good idea for [6].

**Question**: Is it a good idea for [6]?

**Example**: 

Working in DS gives an impact over 30 years of life of:

- 75% chance of working in US starting with 150k$ for 30 years
  starting at 35 years
- growth of 5% average until 50 and then 2% average growth till 65
- 10% increase every 5 years
- Donating 35% of salary

Results in saving 530 people

Instead if I get into "promoting effective altruism" and work on my
"people convincing skills" and convert only 10 people who would not have
donated to donate similar amounts as in a DS career, then it appears
that it could result in saving 5300 people. Of course this needs to be
multiplied by the probability of this actually happening which could
be as low as 10% to match the success of a career in DS saving 530
people.

The point is now I am thinking about such a career especially looking
at the personal fit. Maybe I am underestimating it too much.

**Claim**: These areas can be [7], if you're [8].

**Split**: 

For [8], we think of a personal fit of more than 50%

For [7], we think of an impact of `5300*50%=2650 lives` which is
better than working a DS job resulting in 530 net people.

<!--[8] phrases and [5] claims -->

------------------------------------------------------------------

### Which careers effectively contribute to solving these problems

> The (most effective careers)[1] are those that address the (most pressing
> bottlenecks to progress)[2] on (the most pressing global problems)[3].

**Claim**: [1] are those that address [2] on [3].

**Question**: Is [1], [2] on [3]?

**Split**: 

For [1], we think of a career in AI safety, say as a computer science
researcher in MIRI, with an impact of 6.3 million per additional
person.

*It seems to be making sense finally why an STM thought donating to
MIRI was better than donating to GiveWell.*

For [2], we think of the control problem in AI

For [3], we think of AI safety

> For the (same reasons)[1] we think it’s an advantage to work on (neglected
> problems)[2], we also think it’s an advantage to take (neglected
> approaches to those problems)[3]. We discuss some of these approaches in
> this section.

**Claim**: Due to [1], it is good to work on [2].

*because*

**Claim**: It is advantageous to work on [2].

**Question**: Is it advantageous to work on [2]?

**Example**: 

As shown below, we see that the lives saved per person per dollar is
much better for AI safety, aka a factor of 1000 better.

**copy this calculation only everywhere as needed. this is the final
comparison between**

|                                          | AI safety | Climate Change |
|------------------------------------------|-----------|----------------|
| Possible Deaths at the end of 2100       | 21b       | 20% x 21b      |
| % of chance  (middle of given range)     | 5.5%      | 5.25%          |
| people involved                          | 100       | 1000 (guess)   |
| Double effort => X% reduction in risk    | 1%        | 50%*           |
| Multiply everything above                | 57,750    | 55,125         |
| money involved  (minimum) <span>$</span> | 10m       | 10b            |
| Dividing by above                        | 5.7E-3    | 5.5E-6         |

*here double effort is assumed to mean "major effort" cited in their article

	

**Claim**: It is good to work on [3].

*It is difficult to come up [3], for control problem in AI, as I don't
know much about it.*

**Question**: Why is it advantageous to work on [3]?

**Example**: 

MIRI sends out a mail on Christmas saying that they didn't meet their
funding goals. We are talking about a sad fucking million for 15
people working full time. Let alone adding another 100 people to solve
the most important problem (complete anhilation by 2100 with a 1-10%
chance) by 1 more percent, what about keeping the people currently
involved and trying to grow the movement?

For [3], we think of money being the most neglected approach, with
MIRI not able to meet a pathetic 1.2m in funding for 15(I think) full
time working staff on one of the most important problems that could
lead to complete anhilation by 2100 with 1-10% chance.

For advantageous, we think of 57k lives (as above) for every
additional person we add (on average). So my guess is for less than
200k$ (salary and expenses for one person conservatively), we can get
57k m*****r f****** lives. 

<!-- [5] phrases [3]-[4] claims -->


### Last two hours

-------------------------------------------------------------------------------


> Given our take on (the world’s most pressing problems)[1] and the
> (most pressing bottlenecks these issues face)[2], we think the
> following (five broad categories of career)[3] are a good place to
> (start generating ideas)[4] if (you have the flexibility to consider a
> new career path)[5].


**Claim**: Given [1] and [2], it appears that following [3] is a good
place for [4].

**Question**: Is following [3], a good place for [4]?

*I am not sure how to give an example Given [1] and [2]. So I skip
this for now.*

**Claim**: It appears that following [3], is a good place for [4].

**Question**: Is following [3], a good place for [4], if [5]?

**Split**: 

For [3], we think of a career in researching Climate Change

For [4], we think of Niel Bowerman meeting 'Giving What we can' which
led him to go into Earning to Give in Finance, and then slowly
transitioning from there to FHI and then into AI policy with 80khours

For [5], we think of Niel being able to move to finance for earning to
give

**Example**: Niel Bowerman started his career in researching climate
change where he realized that he should probably earn to give, and
moved into a career path in that direction and in the end landed with
AI policy at 80000 hours. As we have seen in the past AI safety >>
Climate Change.

(*At this point we just accept doing anything
AI is good?*)

Not happy with this example!
I was trying so hard for 90 mins to fit this stupid example!

#### research

> (Many of the top problem areas)[1] we focus on are mainly (constrained by
> a need for additional research)[2], and we’ve argued that (research)[3] seems
> like a high-impact path)[4] in general)[4].

**Claim**: [1] are mainly [2].

**Question**: Is [1], mainly [2]?

**Example**: MIRI which does AI safety last year was not able to meet
it's goal of 1.2m euros. On the contrary, AMF received 30m$ this year and
GiveWell moved 110m$ in 2015.

**Claim**: [3] seems like [4].

**Example**: Working in MIRI as a researcher could save 57k lives and
has a bang-for-the-buck as compared to Climate change (about 1000
times better).

*I don't know what general means so I skip it!*

> (Following this path)[8] usually means (pursuing graduate study in a
> relevant area where you have good personal fit)[5], then aiming to do
> (research relevant to a top problem area)[6], or else (supporting
> other researchers who are doing this)[7].

**Claim**: [8] usually means, [5].


**Example**: [>50% of them at MIRI](https://intelligence.org/team/) seem to have a graduate degree
or a PhD.

**Claim**: [8] usually means [6] or [7].

I hereby confirm it doesn't. For me top problem area is Climate change
or AI safety. None of the team of MIRI seem to have worked or done any
research in Climate change or AI safety before joining MIRI, or even
supporting them in some way.


<!-- 13 in [2] hrs with [6] claims -->

-------------------------------------------------------------------------------


<!--16th jun Sunday-->


> (Research)[1] is the (most difficult to enter of the five categories)[2], but
> it has (big potential upsides)[3], and in (some disciplines)[4], going to
> (graduate school)[5] gives you (useful career capital for the other four
> categories)[6]. This is one reason why if (you might be a good fit for a
> research career)[7], it’s often a good path to start with though we
> still usually (recommend exploring other options for 1-2 years before
> starting a PhD)[8] unless (you’re highly confident you want to spend your
> career doing research in a particular area)[9]).

**Claim**: [1] is [2].

**Example**: I am positive MIRI does not want me with my current skill
set. I probably need to work atleast 5 years (magic number), before I can come
to the level of their research. Whereas I could already earn-to-give
to MIRI as small as the amount may be.

*This is a useless claim I think because I am sure I can come up with
1 example for and against the statement 10 times atleast, or I don't
know which particular example they are talking about. Failed coming up
with examples. how to compare something is more difficult though?*

**Claim**: [1] has [3].

**Example**: An additional worker in places like MIRI has an impact of
57k people. This is by far the highest I have seen in terms of
impact. If you look at earning to give for the most money making job I
know, aka Investment Banking, you could save [3771 lives](http://agent18.github.io/Summary-before-applying-to-80k.html) at max (not
including the personal fit).

**Claim**: In [4], going to [5], gives you [6].

**Example**: Jesse Liptrap from MIRI, finished his PhD in Math and was
able to work as SWE in Google (allowing him to earn-to-give). He
currently works at MIRI.

**Claim**: If [7], it might be good to [1].

**Split**: 

For [7], we think of Jesse Liptrap having atleast [3 papers](https://scholar.google.co.in/scholar?hl=en&as_sdt=0%2C5&q=jesse+liptrap&btnG=) on his
name

For it might be good to [1], we think of Jesse having finished his PhD
being able to work in Google (with the possibility of earning to give).

**Example**: Jesse Liptrap has 3 papers to his name and also worked in
Google (which gave him the possibility to earn to give aside from research)

**Claim**: It is better to do [8] unless [9].

*Don't have an example for someone who did phd and skipped and found
it "hard" to get in to academia again.*

> After your (PhD)[1], it’s hard to (re-enter academia if you leave)[2], so at
> this stage if (you’re still in doubt)[3] it’s often best to (continue
> within academia)[4] (although this is less true in (certain disciplines,
> like machine learning, where much of the most cutting-edge research
> is done in industry)[5]). Eventually, however, it may well be best to do
> (research in non-profits, corporations, governments and think tanks
> instead of academia)[6], since (this can sometimes let you focus more on
> the most practically relevant issues and might suit you better)[7].

**Claim**: After [1], its hard to [2]

*Was not able to find an example online for someone who came back to
academia and how "hard" it was for him*


**Claim**: if [3], better to [4]

**Claim**: if [3], better to [4], unless [5]. 

**Claim**: It is better to work in [6], since [7]

**Claim**: it is better to work in [6].

Skipped the whole section, it is very hard to find real examples.

> You can also (support the work of other researchers)[1] in a
> (complementary role, such as a project manager, executive assistant,
> fundraiser or operations)[2]. We’ve argued (these roles)[3] are often
> neglected, and therefore especially high-impact. It’s often useful
> to have (graduate training in the relevant area)[4] before taking these
> roles.

**Claim**: It is good to [1] in [2].

**Example**: As discussed earlier, AI safety is really quite neglected with 100
people working on it with 10m <span>$</span>. Neil Bowerman from
80khours is trying to add people "required" to fill the "talent gaps".

**Claim**: [3] is often neglected

**Example**: As of 2017 only 100 people are working. Adding another
hudred people would reduce the risk by only 1%. 

**Claim**: [3] is high impact

**Example**: As shown above 1 extra person in the field of AI can on
average save 57k people. If Neil is able to add 10 more people and
even claims 1% of their total impact that would be 570 lives saved
just for his work in a few years.

**Claim**: it is useful to have [4] before [3].

**Example**: 

**Split**: 

For [4] before [3]: Neil Bowerman has a PhD (equivalent) in Physics,
where he worked on existential risks of extreme climate change with a
focus on providing emission targets.

Also Sean O hEigeartaigh, from CSER has a PhD in Genome Evolution, he
is also known to increase the number of people at FHI and secure
rougly 3m <span>$</span> in funding. Now he is completely in
operations such as grantwriting, fundraising, long-term planning
etc...

*Not sure how "useful" [4] is before [3]! or how to even go about answering
it!*

This seems to be exactly what happened with Rob Mather from AMF who
claimed that his sales experience helped him.

> (Some especially relevant areas to study)[1] include (not in order
> and not an exhaustive list): (machine learning, neuroscience,
> statistics, economics / international relations / security studies /
> political science / public policy, synthetic biology /
> bioengineering / genetic engineering, China studies, and decision
> psychology)[2]. (See more on the question of what to study.)

*At this point I am really afraid of their claims about graduate
training being useful and them suggesting people to do it. I don't
even have an example of how true this is?*

**Claim**: [1] is [2].

Can I get help?

<!--15 roughly phrases and [10] claims lookied into with [3] hrs time   -->

-------------------------------------------------------------------------------


#### Government and policy in an area relevant to a problem

am not doing the whole thing as it seems to have no bearing for me and
a lot or research to be done to find the right examples.

#### Working at effective non-profits

> Although we suspect (many non-profits)[1] don’t have (much impact)[2],
> there are still (many great non-profits)[3] addressing (pressing global
> issues)[4], and they’re sometimes constrained by a (lack of talent)[5], which
> can make them a (high-impact option)[6].

**Claim**: [1] don't have [2].

**Example**: Many non-profits like Grameen Foundation fail to show
data of their success and in some cases such as the 'Village phone
program' seem to have been evaluated as having no impact on the
trading activity which it was supposed to boost.

**Claim**: [3] addresses [4].

**Example**: MIRI addresses research regarding AI safety

**Claim**: [3] constrained by [5].

**Example**:

For [4], we think of MIRI.

For [5], we think of the Open Philanthropy project ready to pay a mean
value of 3m <span>$</span>, to add a person immediately to places like
MIRI, OpenAI.

**Claim**: [3] constrained by [5], becomes [6].

**Example**: Every additional person added to AI safety(MIRI, OpenAI)
will have on average an impact of 57k lives. 

> (One major advantage of non-profits)[1] is that they can tackle the
> (issues that get most neglected by other actors)[2], such as
> (addressing market failures)[3], (carrying out research that doesn’t
> earn academic prestige)[4], or doing (political advocacy on behalf
> of disempowered groups such as animals or future generations)[5].

**Claim**: [1] is [2]/[4].

**Example**: GiveWell works out which interventions have how much
impact and where we can save the life for the lowest costs. It seems
like there is no other organization that does this.

**Claim**: [3] is [2].

**Example**: 

For [3], we think of supply not meeting the demand or demand not meet
with the right quantity or right price; you wnat a sledge hammer

**Claim**: [5] is [2].

**Example**: 50 billion animals are still being raised and killed
every year. Most experience extreme levels of suffering and pain
according to 80khours. 

*At this point I am feeling like shit. Unable to meet my quota and
finding it hard to come up with examples, liek that is the most
painful shit because it takes too fucking long. When I see the next
set, am like fuck not again! where am I going to find examples for
this shit! Am pretty down yo! plus I am barely able to do 5per
hour. How am I supposed to finish it without paying the fine.
one way to correct it is to read more about...*

*Is there a point to doing this? If so what?*

> To focus on this category, start by making a list of non-profits
> that address the top problem areas, have a large scale solution to
> that problem, and are well run. Then, consider any job where you
> might have great personal fit.

> The top non-profits in an area are often very difficult to enter,
> but you can always expand your search to consider a wider range of
> organisations. These roles also cover a wide variety of skills,
> including outreach, management, operations, research, and others.


### the goal

The goal could be to understand where I could work, what type of work
I could do.

https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/#top

Devour the article!

https://80000hours.org/job-board/ai-ml-safety-research/

http://www.paulgraham.com/selfindulgence.html

how about some stuff from here:

http://agent18.github.io/deliberate-practice.html morgen freeman!




## k-fold

> If (K is small)[1] in a (K-fold cross validation)[2] is the (bias in the
> estimate of out-of-sample (test set) accuracy)[3] smaller or bigger? If
> (K is small)[4] is the( variance in the estimate of out-of-sample (test
> set) accuracy smaller or bigger.)[5] Is K large or small in leave one
> out cross validation?


For [1], we think of k=3

For [2], we think of the following:

- Divide data set into 3 parts.

- Take the first part as test and the rest as training

- perform say linear regression with all variables and obtain coefficients

- Compute Accuracy on test dataset

- Do this for every part and compute average and variance!



Need to show actual code of example!

For 3, 

smaller or bigger than what?


## lets do pg article on saving money instead of earning more and what
he means

## Statistics



| Date       | phrases/hr | claims/hr | actual claims/hr | Comments                |
|------------|------------|-----------|------------------|-------------------------|
| 17-05-2019 | 12         | 7         | -                |                         |
| 18-05-2019 | 10         | 4         | -                |                         |
| 19-05-2019 |            | 1         |                  |                         |
| 20-05-2019 |            | 1         |                  |                         |
| 21-05-2019 | 3          | 2         |                  |                         |
| 22-05-2019 | 5          | 3         |                  |                         |
| 23-05-2019 | 2          | 2         |                  |                         |
| 24-05-2019 | 4          | 2         |                  |                         |
| 25-05-2019 | -          | -         |                  |                         |
| 26-05-2019 | 10         | 7         |                  | Good, did proper one hr |
| 27-05-2019 | 2          | 1         |                  | Quite hard, was work    |
|            |            |           |                  | on the next phrase      |
| 28-05-2019 | 3          | 1         |                  | T'was hard!             |
| 29-05-2019 |            | 5         | 0                | 0 worked out!           |
| 30-05-2019 |            | 0         |                  | **failed**              |
| 31-05-2019 |            | 0         |                  | tried hard, had to read |
| 01-06-2019 | 2          |           |                  |                         |
| 02-06-2019 |            |           |                  |                         |
| 03-06-2019 | 3          | 2         |                  | ok! last example fine   |
| 04-06-2019 | 4          | 2         |                  | good day! repeat 1!     |
| 05-06-2019 | 3          | 1         |                  |                         |
| 06-06-2019 | 3          | 1         |                  | repeated the same!      |
| 07-07-2019 | 2          | 1         |                  |                         |
| 08-07-2019 | 2          | 1         |                  |                         |
| 09-07-2019 |            |           |                  | **failed**              |
| 10-07-2019 | 30         | 17        |                  | [5] hrs                 |
| 11-07-2019 |            |           |                  |                         |
| 12-07-2019 | 3          | 2         |                  |                         |
| 13-07-2019 |            |           |                  | **failed**              |
| 14-07-2019 | 3          | 2         |                  |                         |
| 15-07-2019 | 32         | 20        |                  | [6] hrs!                |
| 16-07-2019 | 26         | 16        |                  | 6                       |
|            |            |           |                  |                         |


I am dreaming most of the time! I don't have a deadline or some focus! I think. I am
rarely able to do this. I am thinking about the life in India! This
should be painful not boring! And I think it is boring and the very
second the clock ticks 58 to 60 mins pandian is out!

Need to finish 10 phrases today period!

### Letter to an STM

Thalaiva,

If You were me, how would you be spending your time? On what exactly would you be spending your time? Would you just keep trying to clock hours after hours of pure DP on Concrete Thinking? Would you also work on DS?

Why am I looking at DS?

Based on 80khours, I came to the conclusion of working on DS, because it will give me more money(1.5 times in the US) than an engineer, and I could move to the US (for cryonics and more money than here). Everyone from different backgrounds are able to do DS so it should be easy to move. I personally know many people who have moved to DS without too much difficulty after a masters in TU Delft. The route I envision is to start DS work within this year and move to some big DS company in a few years (2-3years)  and do a lot of "critical thinking"and make my way to some EAO like say GiveWell within the next 5-8 years and really start saving large numbers of people. 

Should I be working on DP for CT completely instead?

Why I ask this is because I am not sure of the consequence of working on this (DP for concrete Thinking), i.e., I don't have an example where this makes a "difference" in my life. I don't know how to compare DS and DP for CT. But I will take your word for it and slog my ass off atleast for the coming 4 weeks (just the beginning)(>4hrs per day average of DP guaranteed). Also, over the last few weeks there has been a dip in my amount of hours clocked, so I SUCK and I don't want to SUCK. 4.15 hours on a good week and 2.9 hrs/day last week (I start half hour after I have had dinner, I take long breaks scrolling FB etc..., including weekends.). (I count work on DS, and DP for concrete thinking together in the above.) 

I need your thoughts on this. I am not sure how this 1 hr per day od DP for CT is helping, as I barely get shit done in 1 hr (5 claims, sometimes 1 claim, as things take time to puzzle out.). I stop before I get in the groove. If DS should not be my focus right now, I am more than willing to stop with course 8/10 (20 more hrs of work) and not get closure and not be able to save face when people ask why I am not done with these courses yet.

I don't want to do 1 hr if it is the most important thing to focus on. I don't want to do random phrases from texts and move on to others as it gets hard for me. I want to take a full blown essay (80khours key ideas) or chapter from some book on regression and tear it apart over how many ever days it takes full time. Why? that way I guess I get some work done with Large repetitions. Also I can gather some statistics of work done per hour on similar work and compare over the course of the exercise.

So,

Thalaiva, What would you do if you were me? Let's go big or go home!

And last but not the least, Can you please do this for me? It will help me big time to compare and improve. Can you take this section on Longtermism (5 small paragraphs) and detail it out for me as if you were submitting it for correction by including the phrases the claims and the examples. I am having several questions as I have shown my take, I would like to see you do it and carry that "attitude" throughout the entire essay. It feels like this is one type of essay vagueness I need to handle. And for most parts I am wondering what depth I should go in etc... as discussed below.

I know you always say send me 200 phrases! Name your price for this, just this one time.


Thank You for everything. Cheers!
