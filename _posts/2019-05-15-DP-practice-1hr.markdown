---
layout: post
comments: true
title:  "DP 1 hr per day!"
date:    15-05-2019 
categories: drafts
tags: DP, feedback, examples
permalink: /:title.html
published: true
---

## What are you confused about?

career plans!

I want to do 1k claims!

A lot in statistics

I want to check with an STM what I should be doing and then go on it
chento percento! I don't know what I am trying to do with this DS
thingy.

Take a wikipedia page and detail it out on say statistics!

## Goal 

is to do concrete thinking, i.e., thinking with examples.

## automation tasks needed


- numbered ()[] pnn
- 

## Sticking my head out!

## Entire article from start to finish challenge

> At 80,000 Hours, we (help people find careers that more effectively
> ‘make a difference’)[1], ‘do good’, or ‘have a positive impact’ on a
> (large scale)[2].

**Question**: Does 80khours do [1] on [2]?

**Example**: [Here](https://80000hours.org/about/impact/studies-of-career-change/#1-owen-cb) we see studies after studies about how peoples
plans "amazingly changed" to something else. Zero fucks seem to be
given about some numbers that would say things like, "if he continued
he would have got X impact, but he didn't and look at his current
impact! Suck it". Maybe there is more explanation later.

Making a difference on a large scale maybe thought of as things like
saving 1000 people over a lifetime. I will take it as a win then? or

> Here, we lay out what we mean by these phrases. In a nutshell
> ‘(making a difference)[3]’ is about (promoting the long-term welfare
> of everyone)[4] in ways that (respect the rights of others)[5].

Not a claim but a definition.

**Split**:

For [4], what does it mean? increasing the life expectancy? in
underdeveloped countries? 

From [AMF website](https://www.againstmalaria.com/WhyMalaria.aspx), we think of decreasing the number of deaths
from 1.5 million to 0.5 million and allowing 1 million people to
live. All this per year!

For [5], I am not really sure what sort of example would cut it. Right
to pray? right to eat? right to live in their environment, right to
healthcare, right to cheap education. 

But I still don't know what it means to do [3] in ways of [4]. I skip
it for now. Not worth spending time on this shit!

**Example**:

> This section also sketches out some of the ethical considerations
> that inform our advice. Much of our advice doesn’t entirely depend
> on these views, but we think it’s important to be transparent about
> them. If you want to read our practical suggestions about which
> global problems and careers to focus on, skip ahead.

So we skip the whole thing above, and we deal with it as it comes in
the passage. I am not interested if they have actually detailed
something out in the 

#### impartial concern

> When it comes to (making a difference)[1], we aim to be (impartial)[2]
> in the sense that we give (equal weight to everyone’s
> interests)[3]. This means we strive to (avoid bias against people
> based on their race, gender, sexuality or other identities)[4]. We
> also try not to (privilege any particular place, nation, time, or
> even species above any other)[5].

**Question**: Does 80khrs aim to give [3]?

**Example**: I will skip this, and move to the next sentence that has
more concrete explanations of what they mean with "[3]".

*Aim implies that an example needs to be provided which aims at being
impartial, but needn't succeed?*

**Question**: Does 80khours strive to avoid bias against race?

**Example**: 80khours supports to give donation to Effective Altruism
Foundation which in turn pumps funds to GiveWell, which pumps funds to
reduce deaths in sub-saharan Africa. *This only shows that it is not
biased against black people?*

**Question**: Does 80k hours strive to avoid bias agains people based
on gender?

**Example**: 80khours supports donations to sub-saharan africa where
gender doesn't come in the picture.

*Is this an example that is enough?*

**Question**: Does 80khours try not to [5]?

**Split**:

*What does above any other even mean?*

> Instead, we aim to have (moral concern for the interests of all
> sentient beings in proportion to how much they will gain or lose by
> our actions.)[6] This includes (those who are far away from us)[7]
> as well as (those who will (potentially) be members of future
> generations)[8].

**Question**: 80khours has [6].

**Example**: 80khours support donations to GiveWell (indirectly via
EAF) which primarily pumps fund to sub-saharan Africa. GiveWell looks
at the number of lives saved per <span>$</span> spent to identify the
best charities and pumps money to these charities. It happens that in
sub-saharan africa the <span>$</span> value per life saved is the
highest. This way they don't focus on everyone in need, but just the
people in need that they can help the most. 

**Question**:  Does the people 80khours focus on, [7]?

**Example**: 80khours is based in California but tries to focus on
health in poor countries (sub-saharan africa).

**Question**: Does 80khours focus on [8].

**Example**: 80khours rates AI danger, climate change and also factory
farming as top priority problems as it has the potential to end the
world completely reducing the population to 0. which will probably be
tolerable for this generation, but it's really scary what happens to
the rest of the world.

> From this perspective, we aim to increase (the expected welfare of
> others)[9] by (as much as possible)[10], (enabling more individuals to have
> lives that are long, healthy, fulfilled, and free from avoidable
> suffering)[11].

**Question**: Does 80k aim to increase [9]?

**Example**: 80k tries to get people to donate to GiveWell which
inturn pumps funds to AMF, which has reduced the number of deaths in
sub-saharan africa. 

**Question**: Has 80k, done [11]?

**Example**: 80k tries to get people to donate to GiveWell which
inturn pumps funds to AMF, which has reduced the number of deaths in
sub-saharan africa. 

> As individuals, (we all have other goals)[12] besides (impartially
> making a difference in this way)[13]. (We care about our friends,
> personal projects, other moral aims, and so on)[14]. But we think
> the (impartial perspective)[15] is an important one, and it’s what
> our (research and recommendations)[16] are focused on.

**Question**: Do we ALL care about other things, besides [13]?

**Example**: *Am like what does goal mean? based on the definition of
goals, can the claim be true or not true. then once you read the rest
I guess they are talking about caring!*

I care about making a significant design contribution like the entire
layout of the stage, for my work besides EA.


**Question**: [15] is an important one? 

**Example**: GiveWell suggests to put money in sub-saharan africa to
cure malaria and save X number of lives for Y <span>$</span>. GiveWell
doesn't support the money being used to solve homelessness in the US
for example. Thereby GiveWell Saves more lives.

**Question**: 80k's research is focused on [15].

**Example**: 80k, in it's important problem profiles, treats 'health in poor
countries' as well as 'factory farming and its inhumane treatment of
animals'.



#### longtermism (checking and correcting)

> (Homo sapiens)[0] is still an (infant species)[1]. We evolved
> around 200,000 years ago, and industrial civilization only
> began several hundred years ago; however, the average
> species lasts for 1-10 million years. With the (benefit of
> technology and foresight)[5], humanity could in principle survive
> for at least as long as the earth is habitable — (probably hundreds
> of millions of)[6] years.

**Question**: Is [0], [1]?

**Example**: Humans are around since 200k years ago. Average species
lasts for 1 to 10 million years.

**Question**: With [5], we could survive for [6], years.

**Example**: If technology is able to solve potential existential
risks like, climate change, meteor crashing into the earth, and also
able to eradicate life shortening diseases like malaria, cancer,
immunity syndromes etc... then potentially we can live as long as the
earth is habitable.

*How do you give an example of something you predict in
the future?*

> The (possibility of a long future)[1] means there will, in
> expectation, be (far more people in the future than there are alive
> today)[2]. (Impartial concern)[3] most likely implies (we should value
> their welfare as much as anyone’s)[4]. If (our actions)[5] can predictably
> (affect future generations in nontrivial ways)[5a], then because the
> (welfare of so many others would be at stake)[6], (these effects would be
> what most matter morally about our actions)[7].

**Question**: Does [1], mean [2]?

**Example**: 

If:
- we live in a planet where due to technology diseases are going to be
rare and threat from existential risks are low,

- and as a result if continue with the same growth rate 1.07%/year,

then: 
we will have 20 billion people by 21 billion people! (3 times within
100 years!

The Population of the world from 1804 has increased from
1 billion to 4 billion in \< 100 years starting from 1804. If
technology continues to 

*How do you give an example of something you predict in the future?*

**Question**: Does [3], most likely imply [4]?

**Example**: 

*I dont' know how to give an example for `deductions and
reasoning` like the above. or this is a definition*

*I see "most likely", I guess all this means is 1 example from my
side!*

**Question**: Can [5], do [5a]?

**Example**:  [According to here](https://climate.nasa.gov/news/2458/why-a-half-degree-temperature-rise-is-a-big-deal/), we see that causing a 2 degree
increase within this century can increase the sealevels [by 0.9m](https://www.activesustainability.com/climate-change/sea-level-rise-causes-and-consequences/) which
poses an increased chance of floods starting with coastal regions
around the world, expected to displace 200-300 million people.

**Question**: if [5] can [5a], then [7] is because of [6].

**Example**: *"Because", so we skip for now*

As there are many billions more lives at stake this implies that this
is one of the most important problems.

---------------------

**Breaking down 7**

> (these effects)[a] would be what (most matter morally)[b] about our (actions)[c]

For A, we think of the rise in sea levels and potential increase in
chances of floods. i.e., death of billions of lives and potential
failure to continue mankind

For B, we think of billions of lives saved (over a million years)

For C, we think of not doing anything about the rise in temperature,
i.e., no change in policies to control rise in temperatures.

> If (this)[0] is correct, then (approaches to improving the world)[1] should be
> evaluated mainly in terms of (their potential long-term impact, over
> thousands, millions, or even billions of years)[2]. Making these
> evaluations is part of an emerging field of study called
> longtermism.


For "[7] is correct", we think of "saving n number of lives over a
billion years" is what matters to us.

**Claim**: [1] should be evaluated based on [2], if [0]or[7] is true.

**Question**: Should [1], be evaluated based on [2], given [7] is correct?

**Example**: 

Working on keeping the increase in temperature within 2 degrees within
the century could potentially save billions of lives over millions of
years, as the earth would still be available for habitation. Instead
if we choose to improve the quality of lives of the homeless in the
US, we might not end up keeping the temperature within 2 degrees and
possibly loose billions of lives. 

*This one paragraph above took me 2 hrs atleast over multiple days, I
think primarily because I didn't understand [7]. The clue was in
properly understanding what 7 meant, hence the breakdown. Your
comments? Is the depth at which this example was covered satisfactory?
i.e., do we need to know what "potentially save billions of lives over
millions of years" is exactly?*

------------------------------------

> It’s difficult to (predict the long-term effects of our actions)[1],
> but we think it’s clear that (the interests of future
> generations)[2] are neglected by (most people and institutions
> today)[3], suggesting there are (untaken opportunities to
> help)[4]. We also think (some of our actions)[5] do have( very
> long-term effects)[6] — at the very least we can affect the
> (probability of existential risks)[7], as covered in the next
> section, and there may be (other ways to affect the future as
> well)[8]

**Claim**: Difficult to predict [1].
 
**Question**: Is it Difficult to predict [1]?

**Example**: One estimate of temperature increase is [4 degrees](https://www.givewell.org/shallow/climate-change/impacts#Malaria). The
uncertainty lies between 2.4 to 6.4 degrees Celsius. Damages from
climate change are proportional to the square of temperature
change. So it is difficult to predict what is going to happen a 2
degree increase leading to 9 meters sea level increase or a 6 degree
increase which is going to be "much much worse".


*I Googled climate change for a while, but am not able to
come up with a proper example. The example, would need to look like
this I guess: Look at climate model A,B,C and see the variance in the
estimates? or something that shows the uncertainty in prediction of
likelihood of a disaster*

**Claim**: [2] is neglected by [3].

**Question**: Who or What are the [3], that neglect [2].

**Example**: There are about 100 people worldwide working on "control
problem" for AI, so that machines can pursue "realistic human goals
safely". One simple example of the harms of AI could be that AI thinks
killing people is a better way to contain a virus from growing. AI
could thus affect the future generation.

**Claim**: [2] neglected by [3] suggests [4].

**Question**: Is there [4], given [3] neglect [2]?

**Example**: "Positively shaping AI" has 100 people working it with a
budget of 100m$ for something that could "potentially save billions of
lives." Every additional person working here could potentially
contribute to saving billions of lives.

*I don't know if the above example makes sense for "given [3] neglect [2]"*

**Claim**: [5] has [6].

**Question**: What are the [6], of [5]?

**Example**:

According to [here](https://www.nature.com/scitable/knowledge/library/what-happens-after-global-warming-25887608), it takes much greater than 100 years before
the greenhouse gases subsude to levels that bring the temperature down
by 1 degree even. We are expecting a 6 degree rise with a 10%
probability by 2100. The number of people living in water-stressed
river basins increases by 

*Actually I can't find one spot where I can identify it as a long term
impact". In most cases either the number of lives are not accounted
for or the outcome is better with CC than without!*



**Claim**: We can affect [7], other than 7. 

**Claim**: There may be [8].




> We remain unsure about (many of these arguments)[1], but overall
> we’re persuaded that (focusing more on the very long-term
> effects of our actions)[2] is (one of the most important ways we can
> do more good)[3]. Such a (radical claim)[4] requires (much more
> argument)[5], and we outline the (considerations for and against
> it)[6], as well as (list further reading)[7], in our full article on
> this topic.

**Claim**: We are unsure about [1].

**Question:** Are we unsure about [1]?

**Example**: I don't know what they are unsure about and why! 

For [1] we think of, "With the benefit of technology and foresight,
humanity could survive at least as long as the earth is habitable -- a
few 100's of millions of years."

**Claim**: We are persuaded that [2] is [3].

**Question**: Are we persuaded that [2] is [3]?

**Example**: We can save a few 10's of people by focusing on homeless
people in US, or we can save possibly billion people by focussing on
climate change. The later keeps in mind future generations!

**Claim**: [4] requires much more of [5].

**Question**: Does [4], require much more of [5]?

**Example**: There is probability involved. There are no numbers
given. *is this an example?*

**Claim**: We outline [6], as well as [7] in [this article](https://80000hours.org/articles/future-generations/).

**Question**:  

**Example**: 

Need to reflect on the article! no internet access for now!


### Moral uncertainty and respecting rights!


> As covered, we think that the most important thing for us to
> focus on from an impartial perspective is increasing the long-term
> welfare of everyone, such as by helping people have longer, more
> fulfilling, and happier lives. However, we are not sure that this is
> the only thing that matters morally.

> (Some moral views)[1] that were widely held in the past are regarded
> as (flawed or even abhorrent today)[1a]. (This)[2] suggests we
> should expect our (own moral views)[3] to be (flawed in ways that
> are difficult for us to recognize)[4]. What’s more, there is still
> (significant moral disagreement)[5] within (society)[6], among
> contemporary moral philosophers)[7], and, indeed, (within the 80,000
> Hours team)[8]. It’s also (extremely difficult)[9] to know all the
> (ethical implications of our actions)[10], and (grand projects to
> advance abstract ethical aims)[11] often go badly.

**Claim**: [1], that were widely held in the past are regarded as
[1a]. 

**Question**: Were [1] held in the past abhorrent today?

**Example**: There was a time when blacks were to be seen only as
slaves, but now it is widely unpopular to do anything like that!

**Claim**: [2] suggests [3] to be [4]. 

**Question**: Does [2] suggest that [3] is [4]?

*This is reasoning. How do you give an example that it suggests [3]
is flawed; Skipping this for now!*

**Claim**: [3] could be [4]

**Question**: Is [3], [4]?

**Example**: Until a few years back all I was focusing on was
Women and not on things like EA which is what I should have always
been focusing on. If it were not for an STM who explained to me with
countless hours how my moral views were flawed and needed to be
oriented towards EA, I am not sure I would have recognized it.

**Claim**: There is [5] within [6].

**Question**: Is there [5] within [6]?

**Example**: Recently Anti-abortion laws were passed in Alabama,
Missouri and Georgia. The law said that even in the case of rape the
abortion cannot go through. This is a predominantly Republican view
and Democrats seem to be completely against that.

**Claim**: there is [5] within [7].

**Question**: Is there [5], within [7]?

**Example**: *Can't find an example for it. Looked at Robin Hanson and
Eliezer and also things to do with Peter Singer*

**Claim**: There is [5] within [8].

*Not sure I can come up with this without needless research on the
whole 80000 website. So I skip this!*

**Claim**: It is [9], to know all [10].

**Question**: Is it [9], to know all [10]?

**Example**: I work for a world leader in Lithography and I am able to
donate 10% of my income. I think I have saved at the end of the year
approximately one life. What I don't know is how many lives I have
taken as a result of promoting this company. For example, this company
needs material and sources from the earth, I don't know where its
materials are sourced from and if for example child labor was
involved, or poor conditions of work and health leading to reduced
life of someone else. Or another example would be the depletion of
resources in the end bringing the earth to a stop a few days
earlier. I have no way of estimating it and hence it is difficult.

**Claim**: [11] often goes bad!

**Question**: Does [11], often go bad?

**Example**: 

I am dreaming most of the time! I don't have a deadline or some focus! I think. I am
rarely able to do this. I am thinking about the life in India! This
should be painful not boring! And I think it is boring and the very
second the clock ticks 58 to 60 mins pandian is out!

Need to finish 10 phrases today period!

## k-fold

> If (K is small)[1] in a (K-fold cross validation)[2] is the (bias in the
> estimate of out-of-sample (test set) accuracy)[3] smaller or bigger? If
> (K is small)[4] is the( variance in the estimate of out-of-sample (test
> set) accuracy smaller or bigger.)[5] Is K large or small in leave one
> out cross validation?


For [1], we think of k=3

For [2], we think of the following:

- Divide data set into 3 parts.

- Take the first part as test and the rest as training

- perform say linear regression with all variables and obtain coefficients

- Compute Accuracy on test dataset

- Do this for every part and compute average and variance!



Need to show actual code of example!

For 3, 

smaller or bigger than what?


## Statistics



| Date       | phrases/hr | claims/hr | actual claims/hr | Comments                |
|------------|------------|-----------|------------------|-------------------------|
| 17-05-2019 | 12         | 7         | -                |                         |
| 18-05-2019 | 10         | 4         | -                |                         |
| 19-05-2019 |            | 1         |                  |                         |
| 20-05-2019 |            | 1         |                  |                         |
| 21-05-2019 | 3          | 2         |                  |                         |
| 22-05-2019 | 5          | 3         |                  |                         |
| 23-05-2019 | 2          | 2         |                  |                         |
| 24-05-2019 | 4          | 2         |                  |                         |
| 25-05-2019 | 10         | 7         |                  |                         |
|            |            |           |                  | Good, did proper one hr |
|            |            |           |                  |                         |

