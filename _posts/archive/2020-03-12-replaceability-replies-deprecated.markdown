---
layout: post
comments: true
title:  "Should you work at an EAO or ETG for that EAO"
date:    12-03-2020 
categories: posts
tags: 
permalink: /:title.html
published: false
---

## Entry question and Claims Claims Claims 

80khours saying it is better to work at eao than do ETG.


How much ETG for a GR at Open Phil, Rethink Charity, TLYCS, GiveWell?

How much ETG for AI strategy policy person?

How much ETG for editor types?

How much ETG AI safety positions?

How much ETG for management positions in any org?

First year alone?

What about life time?

Of course this begs the question if onvr I knw this, these numbers
will also make sense for me. Emmm... They will make sense for me if
I am able to get the job there. I assume making it to the top is
already a big deal. And it is not clear how much of the impact you
actually make. So lets gets the daaaaay staaaarted waaaat....

I am trying to look at what people typically are and trying to see if
the climb the steep hill is even worth it?

## What do you need to determine how much ETG? 

- how replaceable it is

- spillover effects (aka displacement chain) how big it is

- amount of dollars moved/raised by the org

- % of contribution to you vs donar

- % contribution within the org associated to you

## Open Phil

Some of the hardest things to estimate would be Open replaceability
and spill over effects.



## Introduction

Jobs are not TC. great. But how replaceable are there... how far do
the displacements go until... 

What is the value of working in EA vs ETG 5 examples spanning the
different things.... 

What can you do? Ask the organizations them selves which is a bullshit
advice... because orgs are super busy and wern't able to help me out. 

## TC

TC is not true, atleast I barely know of cases where a position is
TC. 

## Why replaceability

Whether an org is TC or not, has implications on the impact made. The
true impact you make when a job is TC at an EA org is (much) higher,
than when the job is not TC. A junior GR at GiveWell is expected to
move 2.4m<span>$</span> if the job was TC. The same GR is expected to
move only [244k](https://80000hours.org/career-reviews/working-at-effective-altruist-organisations/#but-wont-i-be-replaceable) in the case that the hired GR is better than the
next-best-candidate by 10% (Not TC). Such is the distinction between
being TC and not. 

So what is this replaceability and how do we come about it?

Say there is only one EAO and they have an opening. Peter and Jay are
the only people "worthy" of that job. Peter can produce a value of
500k at that EAO and Jay can produce a value of 400k at that job. 

If Peter gets the job the value for the world is 500k. If Peter sits
on his ass at home and masturbates and B takes the job, then the value
for the world is 400k. So when Peter takes up a job and doesn't he
only makes the world better by 100k and that is his value. 

You are only as good as you are better than the last candidate. 

The above is a very simple case, where Jay doesn't take up any other
job or where there is only one EAO and one job and not many more like
Jay. These also influence what is the impact. 

So let's say there are a dozen jobs then what happens?

Peter creates 500k value, and then Jay creates 400k value and then
what we see is that there is 900k value. 

There are not many GR openings (5 in Open Phil in 2018). And 100s of
applicants applied for such a position. Not sure what to make of the
English... The current situation feels like there are not many places
those 100s can go. Let's say another 5 could go to? Maybe they go to
other places and create impact and then the ones left over are people
like me and there are many of them... all claims... Like how will I
get far with determining this ? This seems like I can't do shit about
it. 


Maybe I can say as peter suggested that it is between this and
that... this being one person and the other guy turned off? and that
being there are plenty of jobs 

The spectrum is somewhere between the second best not getting the job
and everyone in line getting a job because it is TC. But we all know
that is not true. 

You wont get into EA... It is so FULLLLLLLLLLLLLLLLLLLLLLLLLL.

Level 1 thinking... Level 2 thinking 

Its so hard to get values such as what would be your contribution? 


Level 1 thinking and Level 2 thinking:

Level 1 thinking is when it is TC (which it clearly is not), and level
2 thinking is when it is 

## Comparing two different interventions...

Comparing ETG to where and a GR at GiveWell...

Should I donate to a priority cause? or give to GiveWell? 

So then the discussion is how much you move (under level 1 and level 2
thinking)  

GiveWell does RCTs and ensures weather an org is worthy or not. So it
is the only org that has estimates on where ETG can best go... ETG to
valliammai polytech and ETG

**Claims**: there are more people responsible for the impact than just
the donor and the grants researcher.

**Example**: When you give money to GiveWell, you naturally share
impact. The grants researcher will have some impact as he is the one
who guides your money. This money is guided to say AMF, where there
are 2 people and many volunteers working, who will claim some impact
for guiding the money to buy nets... are there other poeple getting
money from here? salary of the employee?

There are so many people and it would be naive to assume that it is
only the GR and donor who share the impact. 

**Definition**: check 

**Checklist**: 
 
**Claims**: You can compare ETG to working at an EAO

**Example**:  Let's take working at GiveWell as a scene and donating
to GiveWell.

I could donate 50k to GiveWell. In 2015 Milan Griffes estimates 2m
moved by him. And a good chunk went to AMF probably. 

Donor comes with 113m. Org costs say take up a few million and
givewell reports moving 110m

- Donor (60%)
- GiveWell (20%)
  - GR (0.2%)
	- replaceability (10%)
	- actual impact (2%)
  - other people
	- replaceability (x%)
	- actual impact (98%)
- Intervention master (AMF evidence aravind action) (20%)
  - employees
  - employers 
  - Org costs?
  
A donation of 50k will lead to 30k impact. GiveWell moved 110k donated
dollars in 2015 of which the GR Junior could be associated with
(0.2%). Of this 0.2% moved only 20% is associated with the
employee. So I think a GiveWell employee would contribute 0.04%. This
implies that in 2015 junior GR moved 44k. So working at GiveWell being
10% better than the last hire in a level 1 assumption is still better
than donating 50k. Interesting... Impact wise.... and this number
could only be higher as it moves towards the TC assumption.

Anyways this is GiveWell...

I am wondering whether salary of the GR should also come in the
picture. Operations costs... As per the impact, it is split between
diff orgs. Not as per the cost. Regarding replaceability of the org
itself, I think it is hard to estimate. Am more inclined to the level
0 thinking in this case. If GiveWell is removed then are there other
organizations that can do the job? not yet.. but also maybe because
GiveWell is alive. 

Is GiveWell replaceable? If it is then I need to take it into
account. As I have split GiveWell into people and their contribution,
it doesn't seem to bother my part of the contribution. I claim that
because the replaceability is brought to a personal level it doesn't
affect me. 

For example if there is as good of a person working available instead
of holden enbadhadi shoulder then if holden doesn't work he then the
other person takes care of the work. So this doesn't affect GiveWell's
output. I look at it as GiveWell has 110k to distribute. That is
replaceability on a personal level. So if donors didn't have giveWell
what would he do? he would probably spend on the best guesses he can
make. So what is the contribution of GiveWell when it is there - when
it is not there, in terms of output... Insanely hard to
calculate... And I don't have any resources to do it. 


**Claims**: It is fair to compare ETG to GiveWell with working at GiveWell

I don't know what it is fair means. 

but I can do the following: 

**Example**: ETG of say 50k to GiveWell (donor), say 50% goes to donor
and 50% to the person who decides where the money should go. If you
give 50k to GiveWell, based on their research they gonna say 


**Definition**: 

**Checklist**: 
 

**Claims**: It is not fair to compare ETG to Not-GiveWell with working at GiveWell

## Are there spillover effects?

Possibly


## Impact at a job what all do you need?


## Estimate AI safety or research job

## Estimate GR at open Phil

## Peter's company

## GiveWell

## TLYCS

## references

https://www.lesswrong.com/posts/3Ss29ihXsBb8tuoxK/earning-to-give-vs-altruistic-career-choice-revisited

https://www.facebook.com/jefftk/posts/613456690752?comment_id=713258

https://forum.effectivealtruism.org/posts/M958XZGP6w6anfEGQ/what-s-the-median-amount-a-grantmaker-gives-per-year


## Reply to Arun Gertler

First of all. Thank you very much. Appreciate that you provided me
valuable feedback.

> I've made some comments. 

By this you mean this email is your 'comments' right? or was there
supposed to be an attachment?

> There were a few spots where your interpretation of evidence didn't
> match mine.

Can you please provide an example? and also say where else? so I can
check it.

> Overall, I think the truth is complicated -- some areas of EA are
> talent-constrained, some aren't, and which is which depends on your
> definition of "constrained".

But the definition is clear right? The post starts off with it, from
80khours or did I misinterpret their definition? TC is when you are
unable to find skilled people (e.g., Disentanglement Research). Are
there other areas that are TC? 

> The job market is tough for some people and easy for others. 

Other than in the end of the essay I didn't really talk about
this. So, I think you are pointing to the end where it says, "They
severely downplayed how competitive it is to get jobs in EA orgs."
80khours say that as long as I am "EA-minded" that gives me a decent
chance of getting a job in EA, but the truth is far from it (link).

> 80K said a lot of true things, some things that weren't
> well-supported, 

For example?

> and some things that were true at the time but became less true
> later.

For example? Operations jobs being not TC you mean? do you have other
examples?

> I don't know how you should integrate this into your post. Overall,
> I thought the best parts were:

> * You pointing out that a lot of 80K's claims have weak or unclear
>   backing 
> * You looking at real hiring rounds for evidence of
>   whether different orgs had a hard time finding people

Do you know other hiring rounds posts. I didn't find any other.

> The parts that weren't as good:

> * Generalizing from a couple of orgs, or a few personal statements,
>   to make claims about the whole EA movement (which involves
>   hundreds of people working full-time positions across dozens of
>   orgs) 

1. So looking at Carrick's statment and using it to represent the AI
   strategy community was wrong? Even Nick Beckstead said that jobs
   were "quite competitive" (in a podast) in this field (suggesting
   that strategy community is not TC).

2. Concluding from Open Phil, EAF and RC that GRs are not TC is wrong?

	I really find it unlikely that other orgs in GR are TC. These orgs
	point out that they had to turn down many of those who applied and
	were good. This suggests that several other jobs would be filled in GR
	in other orgs as well.

3. "Contrary to the claims from 80khours, it appears that several orgs
   are not TC." is wrong?
   
   I just want to point out here that when I looked for evidence there
   was very little in favor of TC and many people from EA suggesting
   otherwise.

> * Blaming 80K for a lot of individuals' problems when those
>   individuals had many other ways to look for advice (as you've
>   shown by emailing me and posting on the Forum).

You are absolutely right. I was gullible enough to believe that
80khours would have done their research, are good in communication and
that it was going to be hard for me to verify all the research that
they have done. Everywhere I turned, people were talking about 80ks
advice. I feel stupid now that I wasted so much time reading
those. But many people have fallen prey to this (footnote 8). 80khours
needs to hear this that they were one of the reasons for it, I
think. They need to be held responsible for the content they put up.

Also Aaron, looking for advice seems to be not easy. For example, I
recently reached out to several people from the EA community to get a
feel of how replaceable are the specific jobs at EA. This would allow
me to compare ETG to working at an EAO. I barely got any
response. What am I doing wrong? What else can I do?

I am hoping the EA conference in October and my posts in EA forum help
clear out the air a bit more.

> These are all just my opinions, and you can post whatever you'd
> like. But if you do end up revising the draft, I'd be glad to look
> over a new version sometime! It's really good that you're trying to
> make evidence-based critiques, and checking with experienced people
> to hear their thoughts.

Thank You indeed for your kind words. I hope to hear from you
regarding this email as that will pinpoint the things I would need to
change. Once I hear from you on this mail I will change parts of the
essay to take in your criticism. If I didn't do too many changes then
I don't ask you for feedback. And I will post the essay. Otherwise I
will write to you again.

Thank You very much.

## replying to Peter_hurford

> I mean, it's kinda intertwined, right? Presumably you are earning to
> give to fund people to do stuff. So someone needs to do that
> stuff. That person could be you. Or you could be the one funding. I
> think it really comes down to comparative advantage / personal fit
> (how good are you personally at doing vs. earning?) and marginal
> resources (of the orgs you would donate to or work for, how much do
> they want a talented person versus more money?).

How do I do this Peter? I would think I need to start with what values
of impact I can get with ETG and working at an EAorg? And based on the
outcome I can choose to get better/pursue in ETG or GR-sills.

For example, if it turns out that 30k donation is enough to meet the
EA org impact, then I would do an MS and get a job in the FAANG and
30k would be easy to donate. But if it turns out that working at
GiveWell creates an impact of 200k as a GR, then I would rather spend
the next few years doing focused practice on GR-skills as I know for
that 200k in donations is going to be super hard unless I do something
like trading (which I can't). I would like to maximize my impact.

So I am looking for examples that show how people came to the
conclusion that it is better to work in research in an EAO rather than
ETG. These examples would include replaceability and other factors I
think.

> In short, I think getting general examples of people having a high
> impact by working in an EA org would be misleading for anyone actually
> making this kind of career path decision.

I don't want general examples. I would like specific examples of
impact of people in GR and management positions in Open Phil (and the
like), AI safety (technical researcher) positions in OpenAI (and the
like) etc...

AT WHAT ETG DO I BECOME INDIFFERENT TO WORKING IN EA ORGS
(specifically GR, Safety strategy research and management positions)?


## replay to Michael St Jules

> I suppose I'm not directly answering your question, but I think it
> might be pretty hard to answer well, if you want to try to account
> for replaceability properly, because many people can end up in
> different positions because of you taking or not taking a job at an
> EA org, and it wouldn't be easy to track them.

If one hasn't taken into account [replaceability](https://www.jefftk.com/p/replaceability-thinking-on-the-margin), or the
[displacement chain](https://forum.effectivealtruism.org/posts/nn4nPvxF59AAr2G2n/replaceability-with-differing-priorities), how do you know it is better to work in EA
orgs rather than ETG (for X dollars).

Milan Griffes reports with a replaceability of 10% (guess) and
attributing 60% (guess) contribution to the donor, that his impact was
244k. Now if you remove the replaceability it is 2.4m. 

> I doubt anyone has tried to. See this and my recent post.

And the 80khours [article you cited on replaceability](https://80000hours.org/2015/07/replaceability-isnt-as-important-as-you-might-think-or-weve-suggested/) seems to be
so off with its suggestions. 80khours are suggesting that "Often if
you turn down a skilled job, the role simply won't be filled at all
because there's no suitable substitute available". Whilst the only
evidence I can find says completely otherwise: [Carricks take on AI
S&P](https://forum.effectivealtruism.org/posts/7bp9Qjy7rCtuhGChs/survey-of-ea-org-leaders-about-what-skills-and-experience?commentId=ySRBeBocRz7oSahAC), [Peter representing RC](https://forum.effectivealtruism.org/posts/7bp9Qjy7rCtuhGChs/survey-of-ea-org-leaders-about-what-skills-and-experience?commentId=ySRBeBocRz7oSahAC), [Open Phil's hiring round](https://www.openphilanthropy.org/blog/reflections-our-2018-generalist-research-analyst-recruiting),
[Jon Behar's comments](https://forum.effectivealtruism.org/posts/7bp9Qjy7rCtuhGChs/survey-of-ea-org-leaders-about-what-skills-and-experience?commentId=tojT8rKhwCz9rfJbB), [EAF's hiring round](https://forum.effectivealtruism.org/posts/d3cupMrngEArCygNk/takeaways-from-eaf-s-hiring-round).

As for your post, I saw it as well, and gained on the "displacement
chain" verbiage and calculation. It was very difficult for me to
follow the discussion on difference in priorities. In any case, I
think we need atleast one real example to test a claim.

How are people so confident in saying that working at an EAO is better
than doing ETG especially considering how "full" the talent pool is
([Carricks take on AI S&P](https://forum.effectivealtruism.org/posts/7bp9Qjy7rCtuhGChs/survey-of-ea-org-leaders-about-what-skills-and-experience?commentId=ySRBeBocRz7oSahAC), [Peter representing RC](https://forum.effectivealtruism.org/posts/7bp9Qjy7rCtuhGChs/survey-of-ea-org-leaders-about-what-skills-and-experience?commentId=ySRBeBocRz7oSahAC), [Open Phil's
hiring round](https://www.openphilanthropy.org/blog/reflections-our-2018-generalist-research-analyst-recruiting), [Jon Behar's comments](https://forum.effectivealtruism.org/posts/7bp9Qjy7rCtuhGChs/survey-of-ea-org-leaders-about-what-skills-and-experience?commentId=tojT8rKhwCz9rfJbB))?

What is the evidence?


## People are like parrots

Just repeating the same story that 80khours says. jesus.

## Parrot

> Firstly, it's maybe worth being explicit that lots of people should
> neither EtG nor work for an EA org: a lot of people should be
> focused on building skills for future work, and others should be
> working in places (like in government) which are not explicitly EA.

I am not sure what this means for me. 

> To answer the question as best I can, I would do the following:
>
> - Which roles am I considering?

This I have figured. GR in EA orgs or Research in AI safety Strategy
policy and maybe management positions.

> - In each of these roles, what value would I provide over the
>   counterfactual?

How am I supposed to do this? And my question to you is mainly
oriented towards this! Ask organizations? They
are too busy to help me. Check out at 80khours? Well, there is no
evidence there only more claims. The whole Talent constrain discussion
seems to fall
apart:http://agent18.github.io/is-ea-bottlenecked-2.html. The values
of new hires estimated as shown by Michelle's link is not right. They
are inflated according to people in the comments due to adding several
costs such as finding new people etc... And their stands on
replaceability currently is "depends". 

> - What does the organisation I most recently donated to do (on the
>   margin) with my donations?
> - Which looks better? 

This is easier said than done. I am really looking for examples of
people showing how this is done. As in the post my only example is
from Milan Griffes. I don't think anyone has done this and that people
are piggy backing on 80k's "research". I have spent a lot of time on
80khours.

> Therefore, it depends on what you think the best funding opportunity
> is as well as what roles you are considering. This in turn depends on
> your beliefs about what you want to make happen and your skills. Even
> the way I broke up the question is somewhat tailored to situations
> I've had in the past.

> There is no general calculation for all orgs or
> all people, though you may be able to copy the decision of someone who
> shares your rough values and rough skillset.  

Now this is something I haven't thought off.

> It may also be possible to resolve this by answering the simpler
> question 'for an organisation that I think is really valuable, how
> much would more money would they need to be able to add equivalent
> value to having me around' (which is sort of what the survey Michelle
> links to tries to help with) but this is often trickier than it seems.

I doubt I would get a response. I reached out to Open Phil, EAF, RC,
FHI, TLYCS, CEA asking them about value created by a GR in the
respective orgs. People seemed to be busy and didn't have such numbers
ready to deploy.

## reply to PH

Peter please bear with me.

> To make a very long story very short, I think you should focus on
> trying to get a direct work job while still doing what is needed to
> keep your FAANG* options open. Then apply to direct work jobs and
> see if you get one. If you don't, pivot to FAANG.

1. So it looks like you are suggesting that ALL DIRECT WORK (DW) any
day is better than FAANG type of work, provided you get a job, EVEN if
THE MARKET pool IS has [many strong applicants](https://forum.effectivealtruism.org/posts/7bp9Qjy7rCtuhGChs/survey-of-ea-org-leaders-about-what-skills-and-experience?commentId=ySRBeBocRz7oSahAC). Is that correct?

2. I think I can focus on one, either on keeping FAANG open or on DW
opportunities. I am 29, Indian by birth and working in Netherlands
right now. The common route to a Big Bucks FAANG job (hence
California), would require 50k$ in costs and a Master's degree to get
into the US. And I probably need to start masters in 1-2 years max, if
I hope to be a FAANG guy in US (Guess, feeling). So prepping on this
from "now" on would be option 1.

	I don't think I will make it to Direct work jobs now based on what I
have seen. I would need to work intensely on it separately as well,
depending on what type of job. This would be option 2 provided I know
what to focus on. Focusing on option 1 and 2 I think will be hard at
the same time I think in this case! Thoughts?

3. Direct work in what? Each seems to need its own separate prep: GR,
   AI safety tech researcher, Management positions

	How do I compare different opportunities? It circles back again I
think to calculations, examples of values.

4. **On the other hand I could try to COPY YOU.**

- Get a Data Science Job in the US (by doing a Master's maybe?) 
- Be REALLY GREAT at something! Have atleast a Triple Master Rank on Kaggle
  (for e.g.,) (2-3 years maybe)
- Be involved with EA community (treasurer, research manager-->No idea
   how to get there though!)
- Build relevant skills for direct work (Not sure what "relevant
  skills" mean)
- And SOMEHOW IT WILL WORK OUT! (possibly because there is a lot of
  overlap between research, Data science?)

>Also, while doing a FAANG job, you could still aim to build relevant
>skills for direct work and then switch. This is what I did (except I
>didn't work in FAANG specifically).

Can you give 2 examples of relevant skills you built for a particular
direct work? And how you built it?

>Also, from what I know, donating $200k/yr while working in FAANG is
>possible for the top ~10% of engineers after ~5 years.

Wow. The Power of ETG at FAANG. 

## Response to Carrick

> New charities will sometimes be started to make more EA org
> positions, and they wouldn't get far if they didn't have people who
> were the right fit for them. Rethink Priorities and Charity
> Entrepreneurship are relatively new (although very
> funding-constrained, and this might be the bottleneck for their
> hiring and the bottleneck for starting new charities like
> them). Charity Entrepreneurship is starting many more EA orgs with
> their incubation program (incubated charities here). Maybe worth
> reaching out to them to see what their applicant pool is like?

Good idea. I will contact them as well to see the talent pool. If they
still need "high-quality people", somehow getting better (gaining) in that
direction seems like a good opportunity.

> I think there are also specific talent bottlenecks, see [1], [2],
> [3]. 

Micheal, I have written an article here:
http://agent18.github.io/is-ea-bottlenecked-2.html in my unfinished
blogspace about [1] and [2]. I really don't find evidence for their
claims of bottlenecks. Or I don't understand what they are trying to
say. For example, GR in GPR is recommended by 80khours in their
[high-impact-careers](https://80000hours.org/articles/high-impact-careers/) post, also in the [surveys](https://forum.effectivealtruism.org/posts/TpoeJ9A2G5Sipxfit/ea-leaders-forum-survey-on-ea-priorities-data-and-analysis), also in the separate
[problem profiles](https://80000hours.org/problem-profiles/global-priorities-research/#what-is-most-needed-to-solve-this-problem) etc... but yet during [open phil's round](https://www.openphilanthropy.org/blog/reflections-our-2018-generalist-research-analyst-recruiting) on there is
literally 100s of "good resumes" and "many candidates worthy of
positions" but OP could not consume all of them.

Peter Hurford can also be seen talking about the [lack of Talent
constrian](https://forum.effectivealtruism.org/posts/7bp9Qjy7rCtuhGChs/survey-of-ea-org-leaders-about-what-skills-and-experience?commentId=ySRBeBocRz7oSahAC) in GR (I think)

> Actually, this last one comes from Animal Advocacy Careers, a
> charity incubated by Charity Entrepreneurship to meet the effective
> animal advocacy talent bottlenecks.

This I really need to look into. Thanks for that.

> Btw, I think you have the wrong link for Carricks.

Thanks. Corrected it. Sorry about that.


**Bottom line**

I don't know how people evaluate which career to choose. Many people
are redirecting me to posts from 80khours. But I find only claims
there. When I ask organizations on value generated replaceability I
don't get any info from them. I think people do a guess at max, falling prey to vague words
like Career Capital or possibly primarily focusing on what they are
good at or I don't know.

Anyways... It seems like a dead end to think that I can actually
evaluate what I should be doing. Your thoughts?

How did you end up choosing to go to DarwinAI? Why not something else
like GR in GPR or FAAANG?

## Reply to MSJ

>I'm currently trying to transition to effective animal advocacy
>research, reading more research, offering to review research before
>publication, applying to internships and positions at the orgs, and
>studying more economics/stats, one of the bottlenecks discussed here,

Your options sounds solid. I guess your 28 and can thus still get into
relatively different quantitative Finance.

But, how did you decide that it is best for you to dedicate your time
to AAR? You could be working at GiveWell/Open Phil as a GR, or in
OpenAI/MIRI in AI safety research (especially with your CS and Math
background), you could also be working in ETG at the FAANG. Also
80khours no where seems to suggest that AAR of all the things are
"high-impact-careers" nor does the EA survey say anything about it. In
fact the survey talks about GR and AI safety.

>I feel that EA orgs have been a bit weak on causal inference (from
>observational data), which falls under econometrics/stats.

So you hope to apply causal inference in AAR?

Lastly I want to thank you from the heart for taking your time and effot to
respond to me. Appreciate it brother.

## replay to arun

Thanks for the advice and detailed corrections in the previous mail. I will make changes this weekend and see If I need to run it by you again.w
Regarding the podcast, I started to listen to it yesterday and it seems like it is mainly about the article:key ideas.
This post seems to be largely recycled from the August 2018 article on high-impact-careers. For example, the Career paths suggested (including the text) is hasn't changed verbatim: key-ideasand old-article. Another example is the priority paths: key ideas and old-article.

## Story so far

**EA is not TC**

We know this from my TC article. Atleast GR and I would go out on a
limb and say they are not TC at all.

This is good, we know now that our value wont be as high. The
difference between 244k and 2.4m as in Milan Griffes article.

Now that I know this about TC, what does it mean for me? Should I
forget working at an EA org?

**Value, Replaceability**

We often hear that in most cases it is better to work in an EAO rather
than work in ETG. Hmmm, how. Is there one example where that is true?
I don't know. It looks like Milan Griffes is the only example that
talks about this. No one else seems to do this to come to a
conclusion.

Why am I panicking so much?

If EA is not TC and the replaceability of you is easy then it is as
good as you wanking. 

But of course depends of if everyone gets displaced. But currently the
pool of candidates is so high that we are unsure or favoring towards
many people being quite replaceable. 

  * [ ] so displacement chain, replaceability, spillover effects and value
included you can make a comparison. You can make an estimation of what
you should be doing. 

But no one seems to have done it. I have very little info and based on
the info I have, I think that it is very replaceable. Peter said
people whom they didn't hire were exactly the same. Jon said 5%. By
the looks of it, AGB discussion and  it looks like people who don't
get a job are going to end up at some ETG job. The last I saw unless
you are ETGing in FAAANG you are not going to drop 50-100k per
year. That's going to be hard. 

  * [ ] Continue reaching out to people how they came to make thier
        decision
		
But the few examples I have that is all I am going to look at to
understand the value a person. 
		
**Most people make decision based on being parrots**

- Peter said I needed to keep FAANG open and try for a job, without
  providing any evidence for it. And most of them don't seem to think
  that they need to make such a calcualtion (e.g., MSj and other
  people in FB). 
  
- They seem to be good at something and pursue that  thats it and when
  they hear from AAC that they need someone they decide to go there. 
  
- No one seemse to have made such a calculation.


**What should I do**

1. I could copy others

If I copy PETER, then I start with getting better at something and
move further. 

2. I could do calculations

I am afraid I will get no where. 

3. One example check claims...

There si still no way I am going to be able to check out value. 


**Counter arguments**

- the replaceability, lack of jobs is only now (why will it reduce?)

- It's only increasing. the movement is growing strong. 200 to 2000
  people for CE (insanity). 
  
**I wont get a job now as is**

I have seen the type of people being rejected. EA applicant, some
marketting guy and some other examples in that same webiste. 

It is super competitive. why would I try to get in?

**I wont get a job after 6 months of prep?**

I don't know about this. but it takes a lot of time to apply and go to
interviews and get rejected. 

**Even with 0 replaceability, this could be your chance to grow in the
industry**

?


**Taking a job now vs doing some thing else?**

**DS and DS in EA**

### What gets you in?

**Being GREAT at something gets you in**

William McAskil  PhD DPhil in oxford --> 80khours CEA founding

Varsha Venugopal UIUC + 15 years experience --> CE

Peter Hurford literally is awesome kaggle level master etc... and he
pivoted by getting skills into the field. 

Jon Behar, top IB firms and then moved to TLYCS in research

dAVID ROODMAN

Phil Tetlock

Can you be great at anything though?

**Region**

Most people are from Axford or US (look at CE hiring round).


### Different options... 

If it does happen that EA orgs are providing more impact, and more
opportunities

**Management people have high impact** (atleast according to 2015 givewell)
and every other payment scheme of companies. 

I know being a GR I will not have much impact, but what if I grew to
be a Management guy? I think that is kind of thing I should work
towards. Hopefully as seen in CE it is not truly replaceable.

This is gonna be hard, there are not going to be fallback options.

Broadly I see two options,,, app for faang, donate be really good at
such a job and then try to get into EA org in what ever capacity,
mostly DS or SwE or Management. 

The other option being work your ass off towards an EAO position from
now on. 

The third option would be that I try for one year and realize I wont
get now and then try for faang.

The fourth option is that I would try to get into faaang without a
masters in DS (the probability of that happening am I right?)

<!-- Some thing that I could separately research --> I don't know one
case where someone has gone to the US from NL without a masters degree
directly. This is really important. (So o this first). and further
maybe through ASML!

Is my clock running out to do a masters in DS? 
<!-- Something that I need to research separately. How long do I have -->
<!-- -->

Is it possible to get into EA within 1 year? In any aspect and then
climb the ladder from there?
<!-- something to research -->

Or should I spend my time wisely on becoming great first as getting
great while at the job might be hard (due to little time to do other
things like improve way beyond other people). 
<!-- Is it true I can't get great while at a job (Warren Buffet? FTW) -->



## Reply to Jamie

**TL;DR**

I think we might be on the same page.

---

I think it is worthwhile to note that in your [latest article](https://forum.effectivealtruism.org/posts/jT2hFsYbi3mKTy3s9/effective-animal-advocacy-nonprofit-roles-spot-check) in
the abstract you make a few claims such as: "EAs are struggling to
fill fundraising and operations roles". But you also think it is
important and have dedicated a [whole article](https://forum.effectivealtruism.org/posts/jT2hFsYbi3mKTy3s9/effective-animal-advocacy-nonprofit-roles-spot-check) to a bunch of
similar claims on bottleneck, showing why you think there is "weak
evidence" and explain what the "weak evidence" is.

If you are saying you will make representative statements but provide
the evidence you have for it, then this discussion is moot (rendered
unimportant by recent events). For me evidence gives a way to
understand how "struggling" EAAs are and quickly test it.

---

**Claims**: Representatives for ~~most~~ certain purposes seems to be
more useful than specificity/concreteness.

**Example**: 

> If, via some research, you have the ability to either 1) make claims
> about TC across a movement or range or orgs, with moderate confidence
> or 2) make claims about TC in one or two orgs, with higher confidence,
> an individual might opt for (2), as they can focus on orgs they're
> more interested in. But 80k/AAC would opt for (1), because the advice
> is useful to a larger number of people

**Discussion**

This doesn't look like an example that satisfies the claim. Atleast I
am unable to see how it is "useful". Plus there is another claim in
the explanation that this type of advice will be useful for a larger
number of people. Instead, can you show me one actual
"representatives-statement" that satisfies "being more useful" than
its "concreteness" alternative. In the previous reply to you I believe
I clarify with one example how "concreteness" overpowers
"representatives" in being "useful", when people read it.


> Given that the ideal distribution of roles and applicants and how
> this compares to the current situation is only really one
> consideration among several important considerations that affect
> career decisions (i.e. it affects your comparative advantage), maybe
> a high level of precision isn't that important?

And I don't get what you mean by "ideal situation and current
situation is an important consideration for career decisions".

Are you trying to say that looking at one example might not be useful
as it is somehow not precise? and that we should be rather happy with
general statements? Do you have an example to show what you mean?

Thanks.








## Reply to Jamie


## reply to arun last

Meta-note: This is probably the last time I'll write a detailed reply
to you, as you're no longer working on a Forum post and I don't think
I can be very helpful with questions about EA careers beyond what
we've already discussed. If you have other questions about Forum
writing or CEA, feel free to send them, though I may only be able to
give brief answers.



So my writing aka "trying to understand claims with a few examples" is somehow going to create disappointment because it is going to be hard to get evidence? 

In some cases, I think you're looking for stronger evidence than anyone has or is likely to produce within a helpful timeframe. For example, in this comment, you wonder whether donating $30,000 might equate to the counterfactual impact you'd have working at GiveWell. But that depends on how good you'd be at GiveWell research, who else would have taken the job if you didn't, where you donate the $30k, the donation opportunities that are available in future years, and other factors besides. Any number that someone gives you for what donation would be equivalent to a given direct-work position will be an estimate with a lot of uncertainty around it. 

Even EA orgs' own estimates for the value of additional staff are highly subjective; I'm guessing that most people who took the survey referred to here were making very rough estimates they came up with on the spot. (And of course, all those estimates were colored by the exact recent hire they were thinking about.)

More information I'll share: I work with a small foundation outside of my CEA work. We've considered funding staff at an EA org before, and we've asked for estimates of how valuable new staff would be (specifically, "if you hire this person to fundraise, how many additional donations do you expect that to create?"). 

Even though the org had a strong incentive to come up with a solid, impressive number, they gave us an estimate with high uncertainty -- the new person would be doing different work than anyone else they had, and fundraising numbers would depend not only the employee's quality but on the specific people who expressed interest in funding the org, the state of the 2020 economy, the org's progress on other projects and how exciting it would be to donors, etc. The org's 90% confidence interval was between two numbers that were at least an order of magnitude different.

You may not feel "disappointment" in response to all of this (and I apologize for ascribing internal states to you when I shouldn't have), but I think that any estimate you get of the numbers you want to see will include the kinds of uncertainty I mentioned already. Perhaps you can find even more rough estimates if you keep asking people, but the qualitative nature of the data you have doesn't seem likely to change.


I am unsure how to filter "GET CC" advice. People say grad degree is good CC. Data science is also "good CC" and a bunch of other things. What am I supposed to do with this again? How am I supposed to use ^^this general advice above. 

If people share their stories, or advice based on personal experience, you might filter it based on how similar that person's background/skills are to yours, or whether they had the same goals as you for the kinds of direct-work positions they wanted to secure. 

To repeat my beauty example, if someone tells me "running made me feel better about my body," I might say: "How much time/money would it cost for me to start running? Do I want to achieve a runner's physique? Do I enjoy running, or could I learn to enjoy it? What's the risk of injury from running, and how does that trade off with the potential benefits?"

And if someone says: "Data science is good for career capital," I might say: "How much would it cost for me to study data science? Do I want the kind of job that data science prepares me for? Do I have similar skills to the people who went on to succeed in data science? Are there a lot of data-science-related positions available on the 80K job board that I think I'd be a good fit for in other ways, if I were to add data science to my toolset? If the data science direct-work job market dries up while I'm still studying, does that skillset let me earn more in a non-direct-work job?"

You can probably get better information on many of these questions individually, but even if you have reasonable answers, you'll have to make a judgment call at some point about which of many options feels best to you, even though you'll be comparing things that are hard to compare and different options will look better in different ways.

I really don't know what CC means. If CC somehow means number of lives saved or dollars impacted or then why not say that directly? This is my confusion. CC can mean a bunch of things (skills, runway, connections etc...).

I think of "CC" as a general term for "ability to move into one or more high-impact career paths". Different kinds of CC might matter more for different paths -- skills for research positions, connections for political positions, a good-looking resume for finance positions, etc.

I think you are exaggerating what I am trying to you. Nothing that I pursued hints at precision.

Maybe "precision" is the wrong word. But I'll repeat that I don't expect you to find information that is qualitatively different from the information you have already -- you'll just get more rough estimates and personal anecdotes.


Slightly paralyzed by lack of really strong evidence??? How about any evidence in the case of the TC claims for long termism, EA orgs, GR, GPR, AI (assuming everything that I provided has flaws). All I have right now is the claims made by 80khours (especially in words) that could mean many things ("I have a decent chance of getting in EA" can
mean 50%, 30% or even 70%).

All evidence has flaws. The people whose experience you cited are real, and their experience is real; it's just not wholly representative. Every person who has an EA job got it through a process unique to them, and their experiences are real too. It seems like you want data quantifying the impact of different career choices accounting for replaceability and such (which, as I said above, is going to be very hard to find).

What kind of evidence would satisfy you that "talent constraints" really do exist within a field? If orgs claim to be talent-constrained in surveys, what further proof are you looking for that they aren't wrong? 

Maybe think about this prompt: If you could ask a set of orgs any questions you wanted about their recent hiring rounds, what information would you want, and how would you know whether the results satisfied your definition of "talent-constrained"?

Finally, I really think that the "decent chance" point is about as specific as 80K could be at the time -- because even among people in the group they specified, there's still going to be a wide range of skill and experience. Some people in that group might have a near-guarantee of being hired, while others might have almost no chance, with the difference being determined by traits like conscientiousness that are hard to measure. (80K can't say "if you're in the 90th percentile of conscientiousness, your odds of being hired are X" -- how will any reader know what percentile they're in?)


Say I want to test the claim:

**Claims**: ~~80khours says~~ Data science or grad degree is good for Career Capital.
**Example**: Peter Hurford chose Data Science instead of a Grad degree.

Great! Does the example match the claim for Peter Hurford's case? Does Peter have "better CC"?

It's hard to tell! What might Peter be doing if he had gone after a grad degree? Is Peter typical of other EAs? (Probably not, given that he's one of the earliest EA bloggers and is so productive that he's practically been working two full-time jobs for several years at this point; his trajectory, like many EA trajectories, is unique to him.) 

For that matter, how might you even quantify Peter's impact, given that most of it is bound up in his EA work rather than his donations (or so I would assume)?

Answering "how much CC" an individual has seems fruitless. What you can instead do is watch hundreds of people look for jobs and see which kinds of backgrounds tend to work out better in which kinds of positions. You can also talk to people who already work in these fields and learn which kinds of experience they believe are most valuable. You won't get perfect information from this, but you'll get lots of examples of people who are similar in certain ways, which you can eventually convert into impressions of what backgrounds offer the most "career capital". The general pattern won't perfectly fit any individual, but it lets you give better advice than you could without having those impressions. And that, as far as I'm aware, is basically what 80K does.


I generally think it is very important to be ABLE TO TEST CLAIMS.

I agree! How would you test a claim like what you made above? You can't create more Peter Hurfords and have them go to grad school. You can't randomly assign people like Peter to choose one of two paths and see what happens (well, you could, but I doubt people would be happy choosing their careers as part of a small experiment).

Instead, you might engage in a process of "continuous testing" where you keep talking to lots of applicants and lots of experienced workers about what the job market is like in various fields, and tracking changes in demand for various skills. That's what 80K does; it's not perfectly scientific, but it is informative.

Are there types of data or experiments you think 80K should collect/run that they aren't?


Then let's not use words like beauty is one approach, but use words like, instagram followers or number of modeling assignments or dollars earned, QS ranking for models... Something that people CAN TEST and check.

Sure! Unfortunately, these metrics are hard to come by in EA. You could look at how much money people earn who have different skills, I suppose. 80,000 Hours did that. You could look at how many people with different backgrounds actually get EA jobs, and try to compare this to the rate of those backgrounds in the population of applicants to get a sense for which backgrounds work out best... but because the EA job market is so small, I don't know that you'd get significant results, and meanwhile you'd have to coordinate EA orgs to share information on everyone who applies to their jobs, plus categorize the backgrounds of applicants. I don't have any idea whether this approach would be valuable enough to make up for the annoyance of gathering all the data.

What metrics would you want to know about, that could realistically be collected and give meaningful results in a community the size of EA?


I would list "reasonable" options. And show why they are "REASONABLE" and show how much impact such options have ("Top 10% software engineers can donate 200k$"). David Roodman works for Open Phil, previously he worked on determining why fewer people in prison does not imply that crime goes up etc... This is "great". But doesn't tell anything about his impact. If 80khours instead showed, he "improved the lives" of 100,000 people (by X DALYS) or something, and that is why they think it has. 

How would 80K show this? This seems like an absurdly difficult question to answer, and I don't know that I'd trust any number they gave me. Open Phil doesn't even track the impact of most of their grants yet, and they have a bigger team and more resources than 80K -- but tracking the impact of Open Phil grants seems necessary to calculate Roodman's impact.

Every job has a wide range of potential impacts, which depend not only on the work involved but also the person who takes the job and how the world is changing as that person works. 80,000 Hours provides a set of jobs they believe are "reasonable", both in their career profiles and on their job board, based on their impression of what forms of work tend to give opportunities for impact/high donations. I really think that's about as well as a person can do at the task of rating the EV of careers.

If you have ideas for how to measure the impact of individuals or positions in better ways, post about it on the Forum! But try not to assume that doing so is simple or even possible.


First, I think I need some estimates of what is a good career I think in terms of "dollars or lives saved or something". 

I don't think this information exists, or will exist anytime soon. 

You can get a reasonable estimate from GiveWell of how many lives your money might save, because they support interventions that have lots of experimental evidence behind them, collected by legions of professional economists over decades. And even that estimate has a pretty wide confidence interval.

EA career choice, as a field, has no experiments. It has no literature comparing social impact across various jobs (at least, not that I'm aware of, and I don't know how you could create such estimates in a reliable way). Every kind of position will differ greatly in impact based on all the circumstances I've already mentioned. The flow-through effects of direct work are very hard to untangle. And so on.

I understand why you want these estimates, but I just don't see how they would come to exist. I suppose that orgs could think back through the years and try to figure out how much impact each separate employee generated... and even then, the estimates would be so rough and involve so many assumptions that I don't think I could ever trust them. I can't begin to imagine how CEA would figure this out for any one person, let alone all of them (and of course, no position at CEA has ever been held by more than five people or so, so sample size is an immediate issue).




I don't think I have anything else I'd say on a call. My conclusion: specific career impact is extremely hard to estimate, and working with impressions and anecdotes is probably the best we can do with realistic amounts of time and effort. I wish I could say something more satisfying, but I can't.


Best of luck with the CE application, and with whatever career path you end up going down.
