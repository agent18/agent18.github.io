## conversation

[Link](https://docs.google.com/document/d/1xaDBxxSWhXPAbT_UmEJnSoRvAmNFnkLigmUlbOoI5E0/edit?usp=sharing) to document:
[Link](https://calendly.com/jamie-a-harris94/60min) to call:

<<So, you want me to give comments on the writeup i.e., proof-reading, internal and external validity, commenting on methodology etc.?>>

> Yes please! All of these aspects would be welcome. I'm more
> interested in whether the methodology makes sense and seems good
> than on phrasing clarity, but all such feedback is welcome.


## Entry question

Does this methodology make sense?

## Study

There is limited research on the effects that career advice can have
on individuals’ expected impact for altruistic causes, especially for
helping animals. In two studies, we evaluate whether individuals who
receive a one-to-one careers advice call or participate in an online
course provided by Animal Advocacy Careers (a nonprofit organisation)
seem to perform better on a number of indirect indicators of likely
impact for animals, compared to randomly selected control groups that
hadn’t received these services. The one-to-one calls group had
significantly **higher self-assessed expected impact** for altruistic
causes and **undertook significantly more career-related behaviours than
the control group** in the six months after their initial application to
the programme. There was no significant difference between the two
groups’ attitudes related to effective animal advocacy or their career
plan changes. In contrast, the online course group **made significantly
higher levels of career plan changes in the six months** after their
application than the control group, but there were no significant
differences for the other metrics. A number of supplementary analyses
were undertaken which support the main conclusion that the one-to-one
calls and online course likely each caused meaningful changes on some
but not all of the intended outcomes.

**one to one**

**Claim**: The one-to-one calls group had significantly higher
"self-assessed expected impact for altruistic causes"

**Example-sub**: 

**Definition**: 

**Checklist**: sub; Yes; pre; Yes; ecm; Yes;

**Time**: 

**Claim**: The one-to-one calls group undertook significantly more
"career-related behaviors" than the control group in the six months
after their initial application to the programme.

**Claim**: No significant difference between the two groups’ "attitudes"
related to effective animal advocacy or their "career plan changes"

**online course**

**Claim**: "no significant differences" for the "other metrics"

**Claim**: made "significantly higher levels" of "career plan changes" in
the six months


**General**

**Claim**: the one-to-one calls and online course "likely each caused"
"meaningful changes" on "some but not all of the intended outcomes"

based on supplementary analyses

**Methodology**


**Claim**: randomised block design that ensured that the intervention
groups and their respective control groups did not differ
substantially in terms of the extent to which they saw impact for
animals over the course of their career as an important priority;

## Criticism

**Randomization**: Allotment of people to the different groups is
done. Causal inferences can be drawn (attrition withstanding).

**Random selection of units**: The population we are interested in is
people around the world, at all times in a year, who are not
sure/sure how to make impact, who are seeking ways to help the animal
world, who probably are signed up to EA facebook groups or not,
whether they know about AAC program or not etc.

Just stating the obvious perhaps:  
As the prime mode of recruitment was through "ads" on certain groups
and emails, we do not end up with a random selection of people (from
"intended population"). So inferences to the population (mentioned
above) *may be drawn* by assuming that the "sample is similar to the
population", but is certainly open to question.

For example, there could be geographical biases or the RCT happened in
a particular time of the year which somehow excluded a part of the
population etc.

**Participants**

did they decide they wanted online course or one to one or did you
also randomize that? 

what is the control group doing in the mean time? why do you have two
control groups and not just one control group? for both interventions?

**High attrition rate**

High attrition seems to suggest the potential for bias. For e.g., this
could suggest that Beyond that I
am unsure this can be countered for.

- 22% (1-78%), 57% (1-43%) in the one-to-one call (in total it was 39%
attrition) left in the main and control group respectively.

- 61% (1-39%), 69% (1-31%) did not fill in the online courses (in
  total it was 65% attrition) in the main and control groups
  respectively.
  
- They wanted 102 as sample, but ended up with 81 and 112 people for
  the different programs in total. What does this mean for "detecting
  medium-sized effects"?

**Long-term effects** of greater than 1 year are unknown and possibly
also have even worser attrition. Maybe 6 months is "too short" to see
anything actionable?



**Other**: 

1. did you manage to send followup reminders to improve reduce
the attrition rates?

2. First cohort and second cohort seem different, any visible
   differences in the two

3. Are you able to comment on one-to-one and online courses and how
different they are on the metrics?

4. It is not clear what the control group is exposed to? considering
   they didn't do anything would it have been possible to have one set
   of control followed by one set of intervention A and intervention
   B?
   
5. did you tell anyone that you were conducting a study?


### other



other metrics for measuring effect size?

what about supplimentary tests and instruments to measure?


### Results

so this means intervention - intervention after 6 months and the same
for control. These two numbers were compared right?

Mean difference not ok? not getting 0.67. Is it the mean in the table
you show?

Perhaps consider computing SMD's and showing it in a table? then we
can compare them all together I think...

My question to you is why is it varying as much? between the different interventions?

### instruments and how vague they are?


For career plans questions, not sure why this is relevant:

whether they changed which nonprofits they donated to.

perhaps the number of hours could have been a gradient or a couple of
options

"substantiaally and intentionally the amount of money"

It's all weighted byt is the weighting ok?

## todo

- read the study part
- make notes on the internal and external validity aspects of the paper
- make notes on the study part ("methodologies")
- give comments in doc

- give internal and external validity feedback, 

- give feedback on things that are not clear

- give suggestsions feedback such as what about the effect size
  between one on one and other?
