---
layout: post
comments: true
title:  "Superintelligence wat waaat!"
date:    18-08-2019 
categories: posts
tags: DP, feedback, examples
permalink: /:title.html
published: false
---

## Mission #9. 

**Mail dated Aug 13,**

Great job on spending 29 hours in a week! *I recommend measuring rate
of phrases per hour.*

Overall, I didn't see you take on Superintelligence, which was the
mission. Please work on either that or stuff you feel *confused*
about. *Only failures matter* (apart from maintenance of existing
performance).

Note down patterns of failure, such as "X prefers A to B". Spend, say,
half your time searching for and practicing on examples of those
types. You can spend the rest of your practice time on new examples,
from which you will hopefully find other patterns of failure.

**Mail dated June 24,**

Mission #9: Your mission, should you choose to accept it, is to
concretely analyze the key claims in the book Superintelligence by
Nick Bostrom (the book mentioned in the Elon Musk tweet above). He's a
PhD at Oxford who's been writing about AI safety along with guys like
Eliezer for nearly two decades. The book has detailed arguments and
examples about all the topics like possible paths to
"superintelligence" (whatever that means), types of
"superintelligence", the control problem, etc.

No need to write "Question: " - doesn't seem to have changed your
answers.

Don't have to go sentence by sentence; *look at one key claim for each
section, usually the one in the first few paragraphs*, or one for each
paragraph if you feel it's an important section. For example:

> CHAPTER 2 Paths to superintelligence
>
> Machines are currently far inferior to humans in general
> intelligence. Yet one day (we have suggested) they will be
> superintelligent. How do we get from here to there? This chapter
> explores several conceivable technological paths. We look at
> artificial intelligence, whole brain emulation, biological
> cognition, and human-machine interfaces, as well as networks and
> organizations. We evaluate their different degrees of plausibility
> as pathways to superintelligence. The existence of multiple paths
> increases the probability that the destination can be reached via at
> least one of them.

The key claim is "How do we get from here to there? Answer: Artificial
intelligence, whole brain emulation, ..."

Feedback checklist:

1. Could it be that this claim has no any example at all? For example,
   "civilization is at stake".

2. Could this claim be false? Remember the "there is no doubting"
   example.

3. Does this claim say anything about "best" (need to compare against
   the entire set) or "most" (need to show it's the majority in the
   set) or "no" (need to show that nothing in the set matches)?

4. Did you stick to examples that are in the chapter itself? That way
   you don't have to search online for too long.

5. Did you use a running example for a technical phrase? There will be
   lots of new phrases in the book, like "convergent instrumental
   value" and "orthogonality thesis". Whenever you see them, you
   should recall whatever running example you've used.

6. If this is an "if-then" claim, did you either get a concrete
   example or mark it as having no example?

Short names: none; false; best; chapter; running; if-then.

Please refer to the checklist after every claim analysis to ensure
you're not making old mistakes. If you want to add to the checklist
based on mistakes found in past feedback, that's great.

## Feedback list used in this essay

## Chapter 1 History (22)

one key claim per section
For now I don't know which are my failures. 

In an attempt to keep the fire going. I would like to focus on 10
phrases an hour?

Let's see how that goes and refine this shit accordingly. I am
ready. Pandindian pandian.

- black for claims
- red for examples
- blue for conclusions
- yellow highlight for other "interesting stuff"

### Growth modes and big history (23)

> (History at the largest scale)[1], seems to exhibit (a sequence of
> distinct growth modes)[2], each much more rapid than (its predecessor)[3].

**Claims**: [1] seems to exhibit [2], each much more rapid than [3].

**Subject**: [2] exhibited in [1].

**Predicate**: much more rapid than [3]. 

**Example**: "A few hundred thousand years ago, in early human (or
hominid) prehistory, growth was so slow that it took on the order of
one million years for human productive capacity to increase
sufficiently to sustain an additional one million individuals living
at subsistence level. By 5000 bc, following the Agricultural
Revolution, the rate of growth had increased to the point where the
same amount of growth took just two centuries. Today, following the
Industrial Revolution, the world economy grows on average by that
amount every ninety minutes."

**Definition**: checks out.

**Checklist**: yes; true; none; chapter; not-running; none  

"However, the case for tak- ing seriously the prospect of a machine
intelligence revolution need not rely on curve-fitting exercises or
extrapolations from past economic growth. As we shall see, there are
stronger reasons for taking heed."

### Great expectations (2)

> (Machines matching humans in general intelligence)[1]—that is,
> possessing com- mon sense and an effective ability to learn, reason,
> and plan to meet complex information-processing challenges across a
> wide range of natural and abstract domains—have been expected since
> (the invention of computers in the 1940s)[2].

**Claims**: [1] has been expected since [2].

**Subject**: When [1] has been expected

**Predicate**:  since [2].

**Example**: *No example of someone expecting in 1940 in the book.*

**Definition**: -

**Checklist**: yes; neither; none; not-chapter; not-running; none; ""  

> From the (fact that some individuals have overpredicted artificial
> intelligence in the past)[1], however, it does not follow that (AI is
> impossible or will never be developed)[2].

**Claims**: From [1], [2] does not follow.

**Subject**: The future as a result of [1].

**Predicate**: [2] wont happen.

**Example**: *No examples for future*

**Definition**: 

**Checklist**: no; neither; none; not-chapter; not-running; none; "predictions of the future";   

> The main reason (why progress has been slower than expected)[1] is that
> (the technical difficulties of constructing intelligent machines have
> proved greater than the pioneers foresaw)[2].

*because*

**Summary**

"The AI pioneers for the most part did not countenance the possibility that
their enterprise might involve risk.11 They gave no lip service—let alone seri-
ous thought—to any safety concern or ethical qualm related to the creation of
artificial minds and potential computer overlords: a lacuna that astonishes even
against the background of the era’s not-so-impressive standards of critical tech-
nology assessment."

### Seasons of hope and despair (6)

> In the (six decades since this brash beginning)[0], (the field of
> artificial intelligence)[1] has been through (periods of hype and
> high expectations alternating with periods of setback and
> disappointment)[2].

**Claims**: Since [0], [1] has been through [2].

**Subject**: [1], since [0]. 

**Predicate**: has been through [2]. 

**Example**: After the Dartmouth meeting, researchers built systems
like the Logic Theorist, which was able to prove most of the theorams
in the second chapter of WhiteHead and Russell's Principia
Mathematica, and even came up with one prood that was much more
elegant than the original.

Post 1970, funding decreased and skepticism increased.

*This seems to be the best one can do while giving examples. There are
claims in the example, but atleast we can ask the author what exactly
he meant when needed to investigate further.*

**Definition**: checks out!

**Checklist**: yes; true; none; chapter; not-running; none; 


**Combinatorial explosion**

> (The methods)[1] that produced (successes in the early demonstration systems)[2] often
> proved difficult to (extend to a wider variety of problems or to harder problem
> instances)[3].

**Claims**: [1] that produced [2] often proved [3].

**Subject**: Extension to a harder problem instances based on [2],

**Predicate**: often proved to be difficult

**Example**: "For instance, to prove a theorem that has a 5-line long
proof in a deduction system with one inference rule and 5 axioms, one
could simply enumerate the 3,125 possible combinations and check
each one to see if it delivers the intended conclusion", based on the
exhaustive search method.

"Proving a theorem with a 50-line proof does not take ten times longer
than proving a theorem that has a 5-line proof: rather, if one uses
exhaus- tive search, it requires combing through 5 50 ≈ 8.9 × 10 34
possible sequences—which is computationally infeasible even with the
fastest supercomputers."

**Definition**: checks out

**Checklist**: yes; true; often; chapter; not-running; none;  

**Summary?**

Graphical models and Bayesian statistics have become a shared focus of
research in many fields, including machine learning, statistical
physics, bioinformatics, combinatorial optimization, and communication
theory.


### State of the art (5)

> (Artificial intelligence)[1] already (outperforms human intelligence in many domains)[2].

**Claims**: [1] already does [2].

**Subject**: [1]. 

**Predicate**: already does [2]. 

**Example**: "2010: IBM’s Watson defeats the two all-time- greatest
human Jeopardy! champions, Ken Jennings and Brad Rutter. 51 Jeopardy!
is a tel- evised game show with trivia questions about history,
literature, sports, geography, pop culture, science, and other
topics. Questions are presented in the form of clues, and often
involve wordplay." --- Games

There are not other examples of [1] already [2], in the section. 

**Definition**: Does not check out, assuming gaming is just one domain
I need atleast one other domain where [1] has already [2].

**Checklist**: yes; false; many; chapter; not-running; none; "lack of examples"  

> These achievements might not seem impressive today. But this is
> because (our standards for what is impressive)[1] keep adapting to the
> (advances being made)[2].

**Claims**: [1] keep adapting to the [2]. 

**Subject**: [1] 

**Predicate**: keep adapting to [2]. 

**Example**: In late fifties: "if one could devise a successful chess
machine one would seem to have penetrated to the core of human
intellectual endeavor."

I don't know anyone who is amazed by a chess AI today. However

**Definition**: checks out!

**Checklist**: yes; false; none; not-chapter; not-running; none; "not
sure if this is because type of not"


### opinions about the future of machine intelligence (4)

> Progress on two major fronts—towards a more solid statistical and
> information- (theoretic foundation for machine learning on the one
> hand)[1], and (towards the practical and commercial success of various
> problem-specific or domain-specific applications on the other)[2]—has
> restored to AI research (some of its lost prestige)[3].

**Claims**: Progress on [1], and [2], has restored [3] to AI research.

**Subject**: Progress on [1] and [2].

**Predicate**: has restored [3] to AI research.

**Example**: *No examples in the section*

**Definition**: -

**Checklist**: yes; neither; none; not-chapter; not-running; none; "no
examples in section" 

> One result of (this conservatism)[1] has been (increased concentration on
> “weak AI”—the variety devoted to providing aids to human thought—and
> away from “strong AI”—the variety that attempts to mechanize
> human-level intelligence)[2].

**Claims**: result of [1] has been [2].

**Subject**: Result of [1].

**Predicate**: has been [2]. 

**Example**: *no examples in the section*

**Definition**: -

**Checklist**: yes; neither; none; not-chapter; not-running; none;
"not sure if this is because type"  


> (Expert opinions about the future of AI)[1] vary (wildly)[2]. 

**Claims**: [1] varies [2].

**Subject**: [1].

**Predicate**: varies [2].

**Example**: A survey documented [here](https://intelligence.org/files/PredictingAI.pdf) shows the prediction of
HLAI of "experts", ranging from 2020 to 2100.

**Definition**: checks out.

**Checklist**: yes; true; none; not-chapter; not-running; none  

**Summary**

Small sample sizes, selection biases, and—above all—the inherent unreliability
of the subjective opinions elicited mean that one should not read too much into
these expert surveys and interviews. They do not let us draw any strong conclu-
sion. But they do hint at a weak conclusion. They suggest that (at least in lieu of
better data or analysis) it may be reasonable to believe that human-level machine
intelligence has a fairly sizeable chance of being developed by mid-century, and
that it has a non-trivial chance of being developed considerably sooner or much
later; 



## Chapter 2 Paths to Superintelligence (26).
### Artificial intelligence (8)

> How do we get from here to there? AI is a conceivable technology
> path.

**Claims**: AI is a conceivable technology path to go from current to superintelligent.

**Subject**: AI

**Predicate**: is a conceivable technology path to go from current to superintelligent.

**Example**: ~~"blind evolutionary processes can produce human-level
general intelligence, since they have already done so at least
once. Evolutionary processes with foresight—that is, genetic programs
designed and guided by an intelligent human programmer—should be able
to achieve a similar outcome with far greater efficiency."~~

*These are reasons, more claims. You can't give examples for "could be"*

**Definition**: Also, what does conceivable even mean? How
would I know if it is conceivable. How would I know the definition?

**Checklist**: no; neither; none; not-chapter; not-running; none;
"future with no ex"; "definition unclear"

> (Evolutionary processes with foresight—that is, genetic programs)[1]
> designed and guided by (an intelligent human programmer)[2]—should be
> able to achieve a (similar outcome with far greater efficiency)[3].

**Claims**: [1] designed and guided by [2] should be able to achieve
[3].

**Subject**: [1] designed and guided by [2].

**Predicate**: should be able to achieve [3].

**Example**: ~~"If we were to simulate 10^25 neurons over a billion
years of evolution (longer than the existence of nervous systems as we
know them), and we allow our computers to run for one year, these
figures would give us a requirement in the range of 10^31–10^44
FLOPS. For comparison, China’s Tianhe-2, the world’s most powerful
supercomputer as of September 2013, provides only 3.39×10 16 FLOPS. In
recent decades, it has taken approximately 6.7 years for commodity
computers to increase in power by one order of magnitude. Even a
century of continued Moore’s law would not be enough to close this
gap"~~
 
*"Should" is hard to get an example for*, *This again talks about
some prediction for the future*

*If we assume the calculations scratched out above as evidence, then we
see the evolution is not possible within this century. But any
calculation is not an example, just like we don't consider [Elizebeth
Warren's statement](https://www.forbes.com/sites/zackfriedman/2019/06/17/elizabeth-warren-student-loan-debt-forgiveness/#60261e105e7b) of voiding all student debt and countering for
it via increasing taxes for the ultra millionaires*. 

*It's never been done before.*

**Definition**: -

**Checklist**: no; neither; none; not-chapter; not-running; none;
"should"; "future with no ex";

> Another way of arguing for the (feasibility of artificial
> intelligence)[1] is by pointing to the (human brain )[2] and
> suggesting that we could use it as (a template for a machine
> intelligence)[3].

**Claims**: [1] can be done by using [2] as [3].

**Subject**: [1] using [2] as [3].

**Predicate**: can be done.

**Example**: ~~whole brain simulation, taking inspiration from brain,
neuromorphic approaches, recursive self-improvement~~

*There is no example for being superintelligent or even close to
superintelligent, let alone with whole brain emulation or taking
inspiration from the functioning of the brain*

**Definition**: -

**Checklist**: no; neither; none; not-chapter; not-running; none;
"future with no ex"


**Summary**

"Lot of ways" to make SI from AI but there are currently 0 examples
for it in the section.

### Whole brain emulation (6)

**Claims**: We get from current to superintelligent by using Whole
brain Emulation.

**Subject**: That which will lead us from current to superintelligent

**Predicate**:  is  Whole brain emulation

**Example**: ~~"No brain has yet been emulated. Consider the humble
model organism Caenorhabditis elegans, which is a transparent
roundworm, about 1 mm in length, with 302 neurons. The complete
connectivity matrix of these neurons has been known since the
mid-1980s, when it was laboriously mapped out by means of slicing,
electron microscopy, and hand-labeling of specimens..."~~

There is no example of even a small organism whose brain is emulated currently.

**Definition**: -

**Checklist**: no; neither; none; not-chapter; not-running; none;
"future with no ex"  

**Summary**

"Nevertheless, compared with the AI path to machine intelligence, whole
brain emulation is more likely to be preceded by clear omens since it
relies more on concrete observable technologies and is not wholly
based on theoretical insight."

### Biological cognition (8)

**Claims**: We get from current to superintelligent by using
biological cognition.

**Subject**: That which will lead us from current to superintelligent

**Predicate**: biological cognition

**Example**: ~~"Pre-implantation genetic diagnosis has already been
used during in vitro fertilization procedures to screen embryos
produced for monogenic disorders such as Huntington’s disease and for
predis- position to some late-onset diseases such as breast cancer."~~

**Definition**: -

**Checklist**: No; neither; none; not-chapter; not-running; none;
"future with no ex"  

### Brain-computer interfaces (4)

**Claims**: We get from current to superintelligent by using
Brain-computer interfaces.

**Subject**: That which will lead us from current to superintelligent

**Predicate**: brain-computer interfaces

**Example**: ~~"Impressive work on the rat hippocampus has dem-
onstrated the feasibility of a neural prosthesis that can enhance performance in a
simple working-memory task."~~

~~"This prosthesis can not only restore function when the normal
neural connection between the two neural areas is blockaded, but by
sending an especially clear token of a particular memory pattern to
the second area it can enhance the performance on the memory task
beyond what the rat is normally capable of."~~

**Definition**: -

**Checklist**: no; true; none; not-chapter; not-running; none; "future with no ex"  

### Networks and Organizations (4)

> (Another conceivable path to superintelligence)[1] is through the (gradual
> enhancement of networks and organizations that link individual
> human minds with one another and with various artifacts and bots)[2]

**Claims**: [1] is through [2].

**Subject**: [2].

**Predicate**: is [1].

**Example**: -

**Definition**: -

**Checklist**: no; true; none; not-chapter; not-running; none; "future
with no ex"

### Conclusion

There are many "possible paths". But all of them seem to be
predictions based on what all needs to be done. The author is claiming
that the gap to Superintelligence can be fixed if we do XYZ. For
example, they claim cyborgization of man will lead to
Superintelligence. Yet, the examples we see are the ones where
patients are able to communicate by moving a cursor on a
screen. Another example, was the rat whose performance was "enhanced"
in a simple working-memory task. What I fail to see is an example
showing glimmers of Superintelligence, i.e., something that can do
**many things** at **much higher performance** than current
humans/rats/whatever.

Yes computers can beat the shit out of humans in games. Great. but we
do not seem to consider this as super-intelligence. Based on the
examples I see above, the claims seem to not be met. Agree or
disagree? if so why? Everyone "could do" everything in theory, but
when testing a claim, we want examples. This is very similar to people
claiming "human civilization is at risk due to AI".

## Chapter 3

> This chapter identifies (3 different forms of Superintelligence)[1],
> and argues that they are, (in a practically relevant sense,
> equivalent)[2], practically relevant sense, equivalent. We also
> show that the (potential for intelligence in a machine substrate)[2] is
> vastly greater than (in a biological substrate)[4].

**Claims**: [1] are [2].

**Subject**: [1].

**Predicate**: are [2].

**Example**: ~~Superintelligence in any of these forms could, over
time, develop the technology necessary to create any of the others.~~

I don't have any examples for this other than a claim to support
this. And I have no way of testing this "example claim" as well, as it
has SI as subject, for which I have zero examples.

**Definition**: -

**Checklist**: yes; false; none; not-chapter; not-running; none  

**Claims**: [3] is greater than [4].

**Subject**: [3] compared to [4].

**Predicate**: is greater.

**Example**: ~~"Biological neurons operate at a peak speed of about 200
Hz, a full seven orders of magnitude slower than a modern
microprocessor (~ 2 GHz)"~~

There are several more "advantages for digital intelligence", but I
don't think it helps. Because, "potential" seems to be talking about
the future, I don't think I can test it. Just like I can't test "I
have the potential to become CEO one day". However I can test "Sundar
Pichai" has the potential to become the CEO of Google".

**Definition**: -

**Checklist**: no; neither; none; not-chapter; not-running; none; "future with no ex" 

> (Many machines and nonhuman animals)[1] already perform (at superhuman
> levels)[2] in (narrow domains)[3]. Bats interpret sonar signals better than
> man, calculators outperform us in arithmetic, and chess programs
> beat us in chess.

**Claims**: [1] already perform [2] in [3].

**Subject**: What [1], does.

**Predicate**: perform at [2] in [3].

**Example**: "Google’s AI AlphaGo has done it again: it’s defeated Ke
Jie, the world’s number one Go player, in the first game of a
three-part match."

**Definition**: I don't understand the definition "superhuman
levels". If we assume it means way better than the average human, then
yes the definition checks out.

**Checklist**: yes; neither; none; not-chapter; not-running; none;
"definition unclear" 

### Speed Superintelligence

> Because of this apparent time dilation of the material world, a
> (speed superintelligence)[1] would prefer to (work with digital
> objects)[2].

**Claims**: [1] would prefer to [2].

**Subject**: What [1] does.

**Predicate**: would prefer to [2].

**Example**: No example

**Definition**: Also no idea how to test would prefer to

**Checklist**: no; neither; none; not-chapter; not-running; none;
"future with no ex"; "would prefer to"  

> (The speed of light)[1] becomes an (increasingly important
> constraint)[2] as (minds get faster)[3], since (faster minds face
> greater opportunity costs in the use of their time for traveling or
> communicating over long distances)[4]. Light is roughly a million
> times faster than a jet plane, so it would take a digital agent with
> a mental speedup of 1,000,000× about the same amount of subjective
> time to travel across the globe as it does a contemporary human
> journeyer. Dialing somebody long distance would take as long as
> getting there “in person,” though it would be cheaper as a call
> wouldrequire less bandwidth.

*I don't understand what they are trying to say here with this
paragraph. Why would it take a digital agent with a mental speedup of
one millx, about the same amount of "subjective time" to travel across
the globe as it does a contemporary human journeyer. What is
subjective time even mean?*

**Claims**: [1] becomes [2] as [3], ~~since [4].~~

**Subject**: [1] as [3].

**Predicate**: becomes [2]. 

**Example**: No examples here.

**Definition**: -

**Checklist**: yes; neither; none; not-chapter; not-running; none; "no
examples"; '

### Collective Superintelligence

> (Collective intelligence)[1] excels at (solving problems that can be
> readily broken into parts)[2] such that (solutions to sub-problems
> can be pursued in parallel and verified independently)[3]. Tasks
> like building a space shuttle or operating a hamburger franchise
> offer myriad opportunities for division of labor: different
> engineers work on different components of the spacecraft; different
> staffs operate different restaurants.

**Claims**: [1] excels at [2] such that [3].

**Subject**: What [1] does.

**Predicate**: excels at [2] such that [3].

**Example**: People working together on building a space shuttle at
say NASA.

**Definition**: Does it excel though? compared to what? *Not sure if
the claim is not testable or if the definition is unclear or I am
supposed to assume something to compare it with.*

**Checklist**: yes; neither; none; not-chapter; not-running; none; "A
excels at B"; "failed"  

> Collective superintelligence could be either loosely or tightly
> integrated.  To illustrate a case of loosely integrated collective
> superintelligence, imagine a planet, MegaEarth, which has the same
> level of communication and coordination technologies that we
> currently have on the real Earth but with a population one million
> times as large. With such a huge population, the total intellectual
> work- force on MegaEarth would be correspondingly larger than on our
> planet. Suppose that a scientific genius of the caliber of a Newton
> or an Einstein arises at least once for every 10 billion people:
> then on MegaEarth there would be 700,000 such geniuses living
> contemporaneously, alongside proportionally vast multitudes of
> slightly lesser talents. (New ideas and technologies)[1] would be
> developed at (a furious pace)[2], and (global civilization on
> MegaEarth)[3] would constitute (a loosely integrated collective
> superintelligence.)[4]

**Claims**: [1] would be developed at [2] and [3] would constitute
[4].

**Subject**: What [1] would develop at

**Predicate**: [2].

**Example**: *Would be*

**Definition**: -

**Checklist**: no; neither; none; not-chapter; not-running; none; "would be"; "future with no ex"  

**Claims**: [3] would constitute [4].

**Subject**: What [3] would.

**Predicate**: constitute [4].

**Example**: "would"

**Definition**: Not sure if I have an example I would be able to test
if it constitute [4]. I don't think I know what [4] could look like. I
have a feeling.

**Checklist**: yes; false; none; not-chapter; not-running; none;
"future with no ex"; "would"; "definition unclear";"failed" 

### Quality Superintelligence

> (Such examples)[0] show that (normal human adults)[1] have a range of
> (remarkable cognitive talents)[2] that are not (simply a function of
> possessing a sufficient amount of general neural processing power)[3] or
> (even a sufficient amount of general intelligence: specialized
> neural circuitry is also needed.)[4]

*This one I read a few times, and had no idea wtf NB is talking
about. NB is making me furious in many cases, just by not providing
examples and hanging on to abstract bullshit. Such a bastard he
is. How does he expect people to read this shit? especially if they
don't have a background on this and worst of all, he seems to have
extensively used the thesaurus to sound cool. So many words I had to
look up. Why not write it like Harry Potter? huh? why not? eugenics,
affliction, adduced, intractable, circumscribed, "dogs walking on hind
legs, docility"*


**Claims**: [0] shows that [1] have a range of [2] that are not [3]

**Subject**: [1].

**Predicate**: have range of [2] that are not [3].

**Example**: *People with autism spectrum disorders who may have
striking deficits in "social cognition" while function well in other
"cognitive domains"*

This above is hardly an example. Let's consider [this](https://youtu.be/zNGjdYY7WVE?t=142), where we
see a guy with autism making highly detailed drawings of scenery, for
an actual example to test the predicate on.

**Definition**: *how are these "examples" even talking about normal
human adults?*. *For sure the example doesn't seem to hint at giving
any information on [3]. I probably have to go deep into the subject to
test this example.*

**Checklist**: yes; false; none; not-chapter; not-running; none; "no
example in chapter"; 

**Claims**: [1] have a range of [2] that are not [4].

ditto!

*He just loves to use labels after labels* *Sufficient?* *How do I
determine that?*

### Direct and indirect reach

> (Superintelligence in any of these forms)[1] could, over time,
> develop (the technology necessary to create any of the
> others)[2]. The (indirect reaches of these three forms of
> superintelligence)[3] are therefore (equal)[4]. In that sense, the
> (indirect reach of current human intelligence)[5] is also in the same
> equivalence class, under the supposition that we are able eventually
> to create some form of superintelligence.

**Claims**: [1] could over time develop [2].

**Subject**: [1] over time.

**Predicate**: could develop [2].

**Example**: *Could*. Also *I have no example for [1].*

**Definition**: -

**Checklist**: no; neither; none; not-chapter; not-running; none;
"future with no ex"; "could"

**Claims**: [3] are all equal.

**Subject**: [3].

**Predicate**: are all equal.

**Example**: *No example for subject*

**Definition**: - 

**Checklist**: no; neither; none; not-chapter; not-running; none;
"No example"; 

**Claims**: [5] also same as [3].

**Subject**: [5].

**Predicate**: also same as [3]. 

**Example**: no example

**Definition**: don't understand the predicate either.

**Checklist**: no; neither; none; not-chapter; not-running; none; "no
ex"

> And one can speculate that the (tardiness and wobbliness of
> humanity’s progress)[1] on (many of the “eternal problems” of philosophy)[2]
> are due to the (unsuitability of the human cortex for philosophical
> work.)[3]

**Claims**: [1] on [2] are due to [3].

**Subject**: [1] on [2]. 

**Predicate**: are due to [3].

**Example**: *because* + *no example*

**Definition**: 

**Checklist**: no; neither; none; not-chapter; not-running; none; "no example"

### Sources of advantage for digital intelligence

> Minor changes in (brain volume)[1] and (wiring)[2] can have (major
> consequences)[3].

**Claims**: Minor changes in [1] and [2] can have [3].

**Subject**: Minor changes in [1] and [2]. 

**Predicate**: can have [3].

**Example**: ~~"intellectual and technological achievements of humans
with those of other apes"~~ The scratched part is not an example.

**Definition**: -

**Checklist**: yes; false; none; not-chapter; not-running; none;
"hard"; "causation type?"; "failed"


## Data science stuff

### Attempt 1 

- Joshua Starmer (StatQuest) 

I generally look at the wikipedia articles on each subject and then
just use google in general to see what comes up. One good source is
Penn State Statistics:
https://newonlinecourses.science.psu.edu/stat414/node/287/

- Statistical sleuth

R Book: https://cran.r-project.org/web/packages/Sleuth3/

Book: https://drive.google.com/open?id=1DLpuj4MDfwWV3NIBMgGtv5j5aOAK4Ks3

or look at stuff in R-learning notes and pick them and see where you
understand and go deeper.

Plan is to search for patterns for 10 hrs atleast! 

(knitr)

### Doing the first chapter of Stat sleuth

one key claim atleast per section just like pandian studied the super
intelligence.

## Chapter 1: Statistical sleuth
### 1.1.1

**Claims**: Rewards may operate in precisely the opposite way from
what is intended.

**Subject**: What rewards do.

**Predicate**: increase productivity, skill creativity. (general predicate)

**Example**: A study by Teresa Amabile takes people with "considerable
experience" in creative writing and assigns them randomly to two groups;
Intrinsic and Extrinsic groups.

The Intrinsic group of 24 were made to answer a questionnaire which
was focused on "triggering" intrinsic motivation such as "you get a
lot of pleasure out of reading something good that you have written".

The extrinsic group of 23 were made to answer a questionnaire which
was focused on "triggering" extrinsic motivation such as "You have
heard of cases where one bestselling novel or collection of poems has
made the author financially secure".

After the questionnaire all subjects were asked to write a poem on
Haiku style about "laughter" and were subjectively rated by 12 poets
on a 40 point scale.

| Sample size | I/E       | Avg.  | SD   |
| 24          | Intrinsic | 19.88 | 4.44 |
| 23          | Extrinsic | 15.74 | 5.25 |

"There is strong evidence that a subject would receive a lower
creativity score for a poem written after the extrinsic motivation
questionnaire based on the two sided t-test p-value=0.005"

**Definition**: "Since this was a randomized experiment, one may infer
that the difference in creativity scores was caused by the difference
in motivational questionnaires". 

Creativity seems to be increased by type of motivational
questionnaires. Great.  The connection between doing these
questionnaires and What rewards do, is still not clear to me. 

So in this case, what I seem to be not agreeing or unsure about is
that the subject "What rewards do" and the example being for "what
extrinsic motivation questionnaires do"

*Note to self*: Look at the "confusion" in terms of where the
confusion is... subject or predicate. That seems to have narrowed my
confusion down in this case. Loved that insight :)

**other**

"Because the subject was not selected at random from ANY POPULATION,
extending this inference to another group is purely speculative".

i.e., these results are valid for a given [baseline](http://pradeep90.github.io/narrow-the-diff.html). That's fine.

**Checklist**: yes; false; none; chapter; not-running; none;
*example-matching-subject*; *unsure*; *time* (46mins); *missed the
comparison*; *failed*; (The question seems to have had a comparison
which I didn't realize until I wrote the summary).

**Summary**: Rewards seem to be creating lesser productivity compared
to intrinsic motivation.

Maybe there are some other questions unanswered such as "is the sample
size enough?". I haven't looked into it.

### 1.1.2

**Claims**: Did a bank disciminatorily pay higher starting salaries to
men than to women? Not-sure(says the book). There could be other
factors.

**Subject**: What the bank did.

**Predicate**:  discriminatory pay

**Example**: "mean starting salary for males is estimated to be
<span>$</span>560 to <span>$</span>1080 larger than the mean starting
salary for females for the same position (entry level clerical
employees)".

**Definition**: 

- one sided p-value <0.00001 from a two sample t-test

*The difference between last section and this section is that this is
not "randomized". I was more than happy to conclude upon initial
reading that the bank had fucked up. BUT I CANNOT BE SURE, until I see
"all the data".*

It is not possible to say that this is conclusively enough. For
example, "the males may have had more years of previous experience".

**Checklist**: yes; false; none; not-chapter; not-running; none  
*example-matching-definition* ;*failed*


### 1.2.1

**Claims**: Causal inference can be justified by proper use of random
mechanisms

**Subject**: Causal inference by "proper" use of random mechanisms
(aka randomized experiments)

**Predicate**: can be "justified"

**Example**: We take the same creativity example from last time where
a randomization of the "selected participants" led to
interventions. One with intrinsic group and the other with extrinsic
group.  

**Definition**: The example could have been "justified", if this
"statistical analysis" matched the reality. But that is not part of
the examples or info I see in the book. 

**Checklist**: yes; neither; none; chapter; running; none;
*incomplete-example*;

???

---

**Claims**: ~~Statistical inferences of cause-and-effect~~ Causal
inference cannot be drawn from observational studies.

**Subject**: Causal inference from observational studies 

**Predicate**: cannot be drawn.

**Example**: We think of the example from 1.1.2 where a bank seems to
have paid more (average <span>$</span>) to men than women.

**Definition**: Regarding the causal inference, the book comments that
there could be confounding variables such as (years of experience of
males), which could have tipped the scale in favor of the men. But I
don't have any more data than that. 

I cannot check if it can be drawn.

**Checklist**: yes; neither; none; chapter; running; none  
*incomplete-example*;

---

**Claims**: (The chance---that the randomization turned out in such a way
that the intrinsic motivation group received many more of the
naturally creative writers)[]---is incorporated into (the statistical
tools that are used to express uncertainty.)[2]

**Subject**: [1]. 

**Predicate**: is incorporated in [2].

**Example**: We think of the creativity example in 1.1.1 where it
turns out that a questionnaire on motivation can improve your
creativity. 

**Definition**: example is incomplete.

**Checklist**: yes; false; none; not-chapter; not-running; none  
*incomplete-example* 
???

---

**Claims**: Is there any (role at all for observational data in serious
scientific inquiry)[1]? Yes.

**Claims**: Establishing causation is not always the goal

**Subject**: [1].

**Predicate**: is there.

**Example**: A study was conducted with 10 American men of Chinese
descent and 10 American men of European descent to examine the blood
pressure-reducing drug. The result was that the men of Chinese
ancestry tended to exhibit a different response to the drug. 

**Definition**: This does not prove causal inference that being of
Chinese descent is responsible for the difference. In fact, it could
be diet or a particular gene or something. Nevertheless, the study
provided "important" information to the doctors prescribing the drug
to people from these populations.

**Checklist**: yes; true; none; chapter; not-running; none

---

**Claims**: Establishing causation may be done in other ways whilst
still using observational studies.

**Subject**: Establishing causation using observational studies

**Predicate**: may be done.

**Example**: Radiation biologists counted chromosomal aberrations in a
sample of Japanese atomic bomb survivors who received radiation from
the blast, and compared these to counts on individuals who were far
enough from the blast. Although the data is purely observational, the
researchers are certain that higher counts in the radiation group can
only be due to radiation. And has thus been used to estimate the
dose-response relationship between radiation and chromosomal
aberration.

**Definition**: seems to check out.

**Checklist**: yes; true; none; chapter; not-running; none

**Reflection**: Lets make a thought experiment at observational
studies on smoking. So they look at several people who smoke and don't
smoke. Look for signs of lung cancer in both. See that people who
smoke have higher chances of lung cancer. 

You can control for race by sticking to the same race

You can control for diet by sticking to people who are nonvegetarians 

You can control for weight by sticking to people of a certain weight
class

You can control for ancestry by sticking to people of a certain
ancestry

I guess you need to keep controlling everything. 

As the author said a while ago, it is impossible to go to causation
from observational studies.

What if you saw 100k people and saw that 70k had a chance of cancer of
50% and 30k who didn't smoke had a chance of 20%.

Maybe as a result of social anxiety one gets lung cancer. If you
haven't measured this confounding variable you cannot claim causation
based on correlation.

Isn't this randomization enough? Is this randomization? I guess not.

But how would a randomization study look like though?

I guess you would take 200 people, use a chance mechanism to separate
them into two groups and feed one cigarettes and the other no
cigarettes. The "only" difference between these two groups seems to be
the smoking and its effects.


I still can't wrap my head around the fact that you cannot do this
with already existing data of tons and tons of people and smoking.
Lets come back after the next claim.

---

**Claims**: (Analysis of observational data)[1] may lend evidence toward
(causal theories and suggest the direction of future research)[2].

**Subject**: [1] 

**Predicate**: may lend evidence towards [2].

**Example**: Many observational studies indicated an association
between smoking and lung cancer, but causation was accepted only after
decades of OS, experimental studies on laboratory animals and a
scientific theory for the carcinogenic mechanism of smoking. 

**Definition**: It seems that those observational studies were the
first straw for further investigation. 

**Checklist**: yes; true; none; chapter; not-running; none  


### 1.2.2

**Claims**: Inference to populations can be justified by the proper
use of random mechanisms.
<!-- Come back -->

> (Inferences to populations)[1] can be drawn from (random
> sampling studies)[2], but not (otherwise)[3].

**Claims**: [1] can be drawn from [2].

**Subject**: [1] from [2].

**Predicate**: can be drawn.

**Example**: 

	## Population
	> n <- 100000
	> x1 <- runif(n/3,1,100) # gen random numbers from 1-100
	> x2 <- runif(n/3,500,600)
	> x3 <- runif(n/3,900,1000)
	> x  <- c(x1,x2,x3)
	
	> mean(x)
	[1] 517.0248
	
	## Sample
	
	> mean(x[sample(1:length(x),100,replace=T)])
	[1] 548
	[1] 531
	[1] 529
	

**Definition**: The sample mean is very close (6%) from the actual
mean of the population. 

**Checklist**: yes; true; none; not-chapter; not-running; none; 

---

**Claims**: [1] cannot be drawn from [3].

**Subject**: [1] from [3].

**Predicate**: cannot be drawn

**Example**: 

	## Population
	> n <- 100000
	> x1 <- runif(n/3,1,100) # gen random numbers from 1-100
	> x2 <- runif(n/3,500,600)
	> x3 <- runif(n/3,900,1000)
	> x  <- c(x1,x2,x3)
	
	> mean(x)
	[1] 517.0248
	
	## Sample
	> mean(x[1:1000])
	[1] 50.7674
	

**Definition**: When sampling is not random we don't seem to be able
to match infer to population.

**Checklist**: yes; true; none; not-chapter; not-running; none  

---

**Claims**: (Random sampling)[1] ensures that (all sub-populations are
represented in the sample in roughly the same mix as in the overall
population)[2]

**Subject**: [1].

**Predicate**: ensures that [2].

**Example**: We take the same example as above where we have `X`
comprising of 3 sets of random numbers between 3 different ranges.

	## Population
	> n <- 100000
	> x1 <- runif(n/3,1,100) # gen random numbers from 1-100
	> x2 <- runif(n/3,500,600)
	> x3 <- runif(n/3,900,1000)
	> x  <- c(x1,x2,x3)
	
Looking at the mean of the population and the mean of the sample,
implies that the sample is a "decent" mix of the population, courtesy
of [1]. 

	> mean(x)
	[1] 517.0248
	
	## Sample
	
	> mean(x[sample(1:length(x),100,replace=T)])
	[1] 548
	[1] 531
	[1] 529

To suggest that [1], allows for [2] and that it does not by chance, we
toggle [1]. and take the first 100 numbers and find that it doesn't
represent the population.
	
	> mean(x)
	[1] 517.0248
	
	## Sample
	> mean(x[1:1000])
	[1] 50.7674

**Definition**: Atleast there is one example where using random
sampling seems to enable inference to population.

**Checklist**: yes; true; none; not-chapter; running; none;
*because-should-due-to*; *unsure*; *missed-comparison*; ()

*I am unsure if I have answered this claims adequately. How to you
check ensures? To me it looks like [2] is because of [1].*

---

**Claims**: (Random selection)[1] has a chance of producing
(non-representative sample)[2]

**Subject**: [1].

**Predicate**: [2].

**Example**: 

	## Population
	> n <- 100000
	> x1 <- runif(n/3,1,100) # gen random numbers from 1-100
	> x2 <- runif(n/3,500,600)
	> x3 <- runif(n/3,900,1000)
	> x  <- c(x1,x2,x3)
	
	> mean(x)
	[1] 517.0248
	
When I sampled from `X` 10000 times the mean was less than 400 once.

	[1] 381.0892
	
**Definition**: [1] seems to have a chance of producing [2], which in
this case is off by \>25%. This seems to happen in this case once in
10000 times. Checks out.

**Checklist**: yes; true; none; not-chapter; running; none  

---

**Claims**: (The statistical inference procedures)[1] incorporate (measures
of uncertainty that describe that chance)[2].

yet to see an instance of this!

<!-- Come back -->


### 1.2.3

**Claims**: (No such statement about broader context based on
available data)[1] can be made with (absolute certainty)[2]

**Subject**: [1] 

**Predicate**: can be made with [2].

**Example**: ~~The same creativity study. The creative study concluded
that 'creative writers are able to write better with intrinsic
motivation'. Generalizing~~ 

The example we look for is where someone generalized it to absolute
certainty but then reality turned out to be far different. *I don't
have an example from the book in this form.*

**Definition**: -

**Checklist**: yes; neither; none; not-chapter; not-running; none;
*no-example*; *time*; *unsure*;

---

**Claims**: (Every statement)[1] includes some measure of uncertainty.

**Subject**: What [1] includes.

**Predicate**: includes some measure of uncertainty.

**Example**: We think of the same creativity example. Here we saw at
the beginning that the average for the intrinsic group was higher than
the extrinsic group by 4 points (for that particular instance of
chance). There is a chance that it so happened that most of the best
creative writers were in the intrinsic group. So we don't really know
if the intrinsic group is better than extrinsic group or that the
chance mechanism made the grouping skewed such that most of the
creative people are at the intrinsic group.

A statement such as 'intrinsic group have same creativity as the
extrinsic group' has a measure of uncertainty.

We look at a **null hypothesis:** 'No difference between intrinsic and
extrinsic group creativity averages'.

Under the null hypothesis, we could essentially have the results from
different groupings, as we now have the creativity score of all the
people. We simulate 1000 of them and see that difference higher than 4
points occurs only 4 times out of the 1000 random simulations. If the
null hypothesis is correct, then we see that similar groupings had a
chance of 0.4% only.

We are only 0.4% certain that the null hypothesis is true.

Not dealing with *every* but attempting to provide one example.

**Definition**: checks out.

**Checklist**: yes; true; every; not-chapter; running; none  
*example-matching-definition*; *unsure*; *time*;

---

**Claims**: (Chance mechanisms)[1] enable (the investigator to calculate
measures of uncertainty to accompany inferential conclusions)[2]. 

**Subject**: What [1] does.

**Predicate**: enables [2].

**Example**: We think of a coin toss which was used in the creativity
study. We see that using it allowed to compute the p-value of 0.4% for
the null hypothesis. This measure of uncertainty (0.4%) is noted along
with the null hypothesis.

*Am I also supposed to show that not having [1], does not lead to
[2].*

**Definition**: checks out.

**Checklist**: yes; true; none; not-chapter; not-running; none  
*because-should-due-to*; *unsure*; (enable);

*am also confused if I can answer this or not. Or have I answered it?*

---

**Claims**: Conclusions (from C&E studies) can be quite strong, even
if observed pattern cannot be inferred to hold in some general
population (self-selected).

**Subject**: 

**Predicate**: 

**Example**: I don't have an example for this claim. I don't think it
is there in the book.

**Definition**: 

**Checklist**: yes; neither; none; not-chapter; not-running; none  

---

> For (observational studies)[1] the (lack of truly random
> samples)[2] is more worrisome, because making an inference about some
> larger population is usually the goal.

**Claims**: [1] has [2].

**Example**: We think of the 'sex discrimination by a bank' study.

**Definition**: In a truly random sample units are selected from a
well-defined population, where all units have the same chance of being
selected. In the sex discrimination study no units were
selected. There was just an already available set of data. That was
it. Hence the example checks out with [2].

**Checklist**: yes; true; none; not-chapter; running; none  
*example-matching-definition*; *time*; (hard one didn't understand the
predicate or its definition to be able to check.)

*If the sampling is random is when you can infer about some larger
population (remember the 3 sets of numbers example about inference to
population). If there is no random sampling then making an inference
about some larger population might not be right.*

---

> In (observational studies)[1], obtaining (random samples from the
> populations of interest)[2] is often impractical or impossible and
> inference based on assumed models may be better than no inference at
> all.

**Claims**: In [1], obtaining [2] is often impractical or impossible

**Subject**: [2] in [1]. 

**Predicate**:  is often impractical to obtain.

**Example**: 

**Definition**: 

**Checklist**: yes; false; none; not-chapter; not-running; none;
*often*; *unsure* ; *example-matching-subject*; *failed*;
*subject-predicate-split*; *failed*; *time* 

### 1.3.1 Probability model for randomized experiments

**Claims**: (The chance mechanism for randomizing units to treatment
groups)[1] ensures that every subset of (24 subjects gets the same chance
of becoming the intrinsic group)[2].

**Subject**: What [1] ensures.

**Predicate**: ensures [2].

**Example**: With 24 black cards and 23 red cards shuffled, we are
able to segregate people to the intrinsic and extrinsic group into
1.6x10^13 groups. Each group occurs only once and has the same chance.

`RBRBRB...RBRB` is one of those groups and has a chance of 1/1.6x10^13
and `RBRBRB...RBBR`, is another group and has a chance of 1/1.6x10^13.

**Definition**: checks out.

**Checklist**: yes; true; none; not-chapter; not-running; none;
*example-matching-definition*; *unsure* 

### 1.3.2 Test for treatment effect in creative study

> (The value of this test statistic is close to or far from zero? The
> answer)[1] to that question comes from what is known about how the
> (test statistic might have turned out in other randomization outcomes)[2]
> (if there were no effect of the treatments)[3]

**Claims**: What is known about [2], is the answer to [1], if [3].

**Subject**: What is known about [2], if [3].

**Predicate**: is the answer to [1].

**Example**: In the creativity study problem, it is known that the
test statistic (difference in average between both groups) of greater
than 4 is obtained only 4 times over 1000 randomized groups, under the
*null hypothesis* (no effect of the intrinsic treatment).

**Definition**: We are able to observe that the test statistic is far
away from 0 in a histogram, as values greater than 4 appear over 1000
randomizations.

**Checklist**: yes; true; none; not-chapter; running; if;

---

**Claims**: It is possible to determine what test statistic values
would have occurred had the randomization process turned out
differently.

**Example**: In the creativity study, under the null hypothesis, that
the difference between the extrinsic and intrinsic treatments made no
difference, we now posses 47 creativity scores with which we can
compute different test statistics for other randomizations.

For current randomization the test statistic is 4. For another
randomization shown in the book the test statistic is 2.07.

**Definition**: checks out.

**Checklist**: yes; true; none; not-chapter; running; none

---

> That conclusion could be incorrect, however, because the
> randomization is capable of producing such an extreme.

**Claims**: (The conclusion that the null hypothesis is wrong)[1], could be
(incorrect)[2].

**Subject**: What [1] could be.

**Predicate**: could be [2].

**Example**: *In order to see the claim to be true, I would need to
look at the actual reality and compare it with the null hypothesis. At
this point I don't have an example*

**Definition**: -

**Checklist**: yes; false; none; not-chapter; not-running; none;
*no-example*; *future-with-no-ex*; *unsure* 

---

> The smaller the (p-value)[1], the more unlikely it is that (chance
> assignment is responsible for the discrepancy between groups)[2],
> and (the greater the evidence that the null hypothesis is
> incorrect)[3].

**Claims**: The smaller the [1], the more unlikely is [2].

**Subject**: Consequences of smaller [1].

**Predicate**: [2] is more unlikely.

**Example**: We take the same creativity study again. The test
statistic we got for the sample is 4.

For a p value of 0.4% under the null hypothesis, we conclude that the
chance assignment would not be the reason for the difference between
the two groups.

For a p value of 50% under the null hypothesis, we would conclude that
the chance assignment would be the reason for the difference between
the two groups.

**Definition**: checks out.

**Checklist**: yes; false; er; not-chapter; not-running; none;
*missed-comparison*; *almost*; *time* ; *example-matching-subject*;
*unsure*;

*I don't know if I can give example. as in if the example is talking
about reality of which I don't have an example*

*I was struggling with the pvalue drawing histogram and thinking about
it abstractly, what would hapen when p value is >50%... imagining the
example quickly put things into perspective. 3 and above, 4 and
above... why was I taking the greater half of the area became clear.*

### 1.4 Measuring Uncertainty in Observational studies

> This is such an important distinction that the test goes by a
> different name---the permutation test---even though the calculations
> are identical to those of the randomization test.

**Claims**: (Uncertainty measures in Observational studies)[1] are
identical to those of the (randomization test (used in C&E studies to
establish C))[2]. 

**Subject**: [1].

**Predicate**: are identical to [2] 

**Example**: For the 'sex discrimination study', we use the p-value,
under the null hypothesis that: the salaries were provided to the two
groups at random i.e., the salaries were shuffled amongst the
different groups.

For the 'creativity study', we use the p-value, but for a different
form of null hypothesis, where: the creativity scores of the person
remained with them, but the people were shuffled amongst different
groups.

**Definition**: checks out that they both seem to have identical
measures aka p-value for a null hypothesis.

**Checklist**: yes; true; chapter; running; none;

### 1.4.2 Testing for a difference in the Sex Discrimination Study

> In the sex discrimination study, there is no interest in the
> starting salaries of some larger population of individuals who were
> never hired, so a (random sampling model)[1] is not
> (relevant)[2]. It makes no (sense)[3] to view (the sex of these
> individuals)[4] as (randomly assigned)[5]. Neither the random
> sampling nor the randomized experiment model applies.

**Claims**: [1], is not [2] in the context of observational studies.

**Subject**: [1],

**Predicate**: is not [2], in the context of observational studies.

**Example**: For [1], we think of random sampling example shown
before: bag with different number ranges (population=100000)
exists. From this bag we take a sample of 1000 numbers randomly and
get see that the average of the population and the sample was quite
close (5%). In this case the sample value can be extended to the
population value as a result of random sampling.

**Definition**: In the case of the 'sex discrimination study' we see
that we are not interested in starting salaries of a larger
population. So it looks like it is not relevant.

**Checklist**: yes; false; none; not-chapter; not-running; none;
*subject-predicate-split*; time; *unsure*; *example-matching-subject*;
*time*; (several repetitions);

*how to give an example for things that don't make sense* ^^

---

**Claims**: (Randomized experiment model)[1] does not applies to
(observational studies)[2].

**Example**: We think of the creativity study where a chance mechanism
like toss of a coin decides who is in intrinsic and who is in the
extrinsic group. This allows for C&E analysis.

**Definition**: In observational model you are usually dealt with data
like in the 'sex discrimination study' after the event has
happened. Here we already have the salaries of the men and
women. And it is thus does not seem to apply to [2].

**Checklist**: yes; true; none; chapter; running; none;
*example-matching-definition*; *unsure*;

### 1.5

> The stem and leaf diagrams show the centers, spreads and shapes of
> distributions in the same way histograms do.

**Claims**: ^^

**Subject**: Comparison of stem-and-leaf diagrams vs histograms

**Predicate**: show the centers shapes and distributions in the same way.

**Example**: Image 1.10 in the book, shows the stem and leaf diagram
for the creativity study. If you rotate it 90 degrees clockwise it
looks like it has the Y and X axis of a histogram. If you increase the
histogram bins to show each integer of score then they will coincide
exactly in shape.

**Definition**: checks out.

**Checklist**: yes; true; none; chapter; neither; none;

*Until I had to actually prove this claim I didn't see it. Now I see everything.*


### 1.5.4 

> In (sampling units such as lakes of different sizes)[1], it is sometimes
> useful to allow (larger units to have higher probabilities of being
> sampled than smaller units)[2].

**Claims**: In [1], it is sometimes useful to allow [2].

**Subject**: While [1], how [2] is.

**Predicate**: is sometimes useful

**Example**: *No examples in the chapter.*

**Definition**: -

**Checklist**: yes; neither; none; not-chapter; not-running; none;
*subject-predicate-split*; *almost-failed*; ("it is B")

### 1.5.5

> (Close examination of the results of randomization of random
> sampling)[1] can usually expose ways in which (the chosen sample is
> not representative)[2]. The (key)[3], however is not to abandon (the
> procedure when its result is suspect)[4].



**Claims**: [1] can usually expose ways in which [2].

**Subject**: What [1], exposes

**Predicate**: exposes ways in which [2].

**Example**: Let's take the creativity study where randomization lead
to choosing of the people in the intrinsic and extrinsic group. *Can't
figure it out by myself. Also there is no example given.*

**Definition**: -

**Checklist**: yes; false; none; not-chapter; not-running; none;
*incomplete-example*; *failed*; *no-example*; *definition-unclear*

---

**Claims**: Not to abandon [4], is key.

**Subject**: Consequences of not abandoning [4].

**Predicate**: is key

**Example**: *Don't have an example for it.*

**Definition**: 

**Checklist**: yes; neither; none; not-chapter; not-running; none;
*no-example*;

---

> If (randomization were abandoned)[5], there would be no way to
> express (uncertainty accurately)[6].

**Claims**: If [5], there would be no way to express [6].

**Subject**: If [5], ways to express [6].

**Predicate**: do not exist

**Example**: *No idea how the example will look*

**Definition**: 

**Checklist**: yes; false; none; not-chapter; not-running; if;
*no-way-to-do-X*; *failed*; *subject-predicate-split*; 

### 1.6 

> (Randomized experiments)[1] eliminate (this problem (of
> confounding variables))[2] by ensuring that differences between groups
> (other than those of the assigned treatments) are due to chance
> alone. Statistical measures of uncertainty account for this chance.

**Claims**: [1] eliminate [2].

**Subject**: What [1], eliminates.

**Predicate**: eliminates [2].

**Example**: *I again think that the only way to give an example for
these claims is by taking an example and comparing it to
reality. That's how we know we eliminate the problem.* 

**Definition**: -

**Checklist**: yes; false; none; not-chapter; not-running; none  
*example-matching-subject*; *failed*; *example-matching-definition*;
failed;

> When the model corresponds to the planned use of randomization or
> random sampling, it provides a firm basis for drawing inferences.

same as last statement. I need an exmaple which shows the way life is
in reality. Here I just have the books claim to rely on that
randomization works. I feel it. But I don't know.

## Chapter 2..

Think about randomization and reality and all that... try falsifying
it. maybe!!

and then move to next chapter. maybe or look for examples where I
fail?

### 2.0 t-distributions

> The t-tools are useful in regression and analysis of variance
> structures

- come back to it later

> The t-tools are derived under random sampling models when
> populations are normally distributed.

- come back to it later.

> The resulting tools also find applications as approximations to
> randomization tests.

- come back to it later

### 2.1.1 Bumpus's data

> As evidence in support of (natural selection)[1], he presented
> (measurements on house sparrows brought to the Anatomical Laboratory
> of Brown university after an uncommonly severe winter storm)[2].

**Claims**: [2] is evidence in support of [1].

**Example**: Difference in lengths between the armbone of 24 adult
male sparrows that perished and 35 adult males that survived. p-value
is 8%.

**Definition**: A p-value of 8% seems to suggest a small chance that
the null hypothesis is true. This seems to be evidence in support of
natural selection, NOT PROOF.

**Checklist**: yes; true; none; not-chapter; running; none;
*example-matching-definition*; *time* 

### 2.1.2 Anatomical Abnormalities Associated with Schizophrenia

> Are there any physiological indicators associated with
> schizophrenia? Yes.

**Claims**: ^^

**Example**: Paired difference of hippocampus volume between 15 sets
of twins (there by controlling for genes and socioeconomic
differences):

- average : 0.199cm^3
- SD : 0.238
- p-value : 0.6%

**Definition**: This p-value seems to suggest that the null
hypothesis (that there is not difference between hippocampus volume of
the twins), has a low chance of being true.

**Checklist**: yes; true; none; not-chapter; running; none 

### 2.2.1 One-sample t-tools and paired t-test

> The mean of the sampling distribution of the average is also mu, the
> (standard deviation of the sampling distribution)[1] is (sigma by
> root n)[2]. The (shape of the sampling distribution)[3] is more
> nearly normal than is the (shape of the population
> distribution)[4]. The last fact comes from the Central limit theorem.

**Claims**: [1] is [2].

**Example**: randomly sampled in R 10000 numbers from Poisson with
lambda=1 (doesn't look normal at all) and these are the findings:

- n_sample=100
- sd(population)/sqrt(100)=0.10008
- sd(sampling distribution of the average)=0.1004

**Definition**: Checks out!

**Checklist**: yes; true; none; not-chapter; not-running; none;

---

**Claims**: [3] is more nearly normal than is [4].

**Example**: In the above case I took a poisson distribution with
lambda=1; No one in their right mind would say it looks normally
distributed.

Contrast that to the sampling distribution of the average and it looks
like a bell curve for the example mentioned in the previous claim.

**Definition**: checks out!

**Checklist**: yes; true; none; not-chapter; running; none;

---

> The (standard deviation in the sampling distribution of an average)[1],
> denoted by SD(Y_bar), is the (typical size of (Y_bar-mu), the error
> in using Y_bar as an estimate of mu)[2]. This standard deviation gets
> smaller as the sample size increases.

**Claims**: [1] is [2].

**Example**: 

sd(Y_bar) = 0.1004
Y_bar-mu = 0.1039

**Definition**: I don't think it checks out. I suspect I made a
mistake with understanding what they meant by "typical size of (Y_bar-mu)"

**Checklist**: yes; false; none; not-chapter; running; none;

come back!

### 2.2.2 The standard error of an average in Random sampling

### 2.2.3 The T-ratio based on a sample average (start from here)

> If the (sampling distribution of the estimate)[1] is normal, then
> the (sampling distribution of Z)[2] is (standard normal)[3], where
> the mean is 0 and the standard deviation is 1.

Z = (estimated mean - Mean of population)/SD(estimate)

**Claims**: If [1] is normal, then [2] is [3].

**Example**: We go further with the same example we have seen till now
where a population is setup using `rpois`.

[1] is normal according to claims addressed before. Now we compute
sampling distribution of Z and determine sd and mean.

mean = 0.009
SD = 1

**Definition**: Checks out!

**Checklist**: yes; false; none; not-chapter; running; if-then
*because-should-due-to*; *unsure*;

*I feel that this is too abstract. I am afraid I am not dealing with a
real example but some abstract thing.*

<!-- > If the (standard deviation of the estimate)[1] is known, this permits -->
<!-- > (an understanding of the likely size of the estimation -->
<!-- > error)[2]. Consequently (useful statements)[3] can be made about the (amount -->
<!-- > of uncertainty with which questions about the parameter can be -->
<!-- > resolved)[4]. -->

<!-- **Claims**: If [1] is known, this permits an understanding of [2]. -->

<!-- *come back later maybe we see examples ahead* -->

<!-- **Claims**: [3] about [3] can be made. -->

<!-- *come back later, maybe we see examples ahead* -->

> The t-ratio does not have a standard normal distribution, because
> there is extra variability due to estimating the standard deviation

*misreading leading to a lot of confusion. I missed the whole standard
thingy. I spent 53 mins and then saw this. Now I know what to test!*

**Subject**: t-ratio

**Predicate**: does not match standard normal distribution

**Example**: We take a Poisson's distribution with 10000 random
values and `lambda=1`. `pop-mu=0.995` & `pop-sd=0.99`.

t-ratio = (estimate-parameter)/std.error

In our case, t-ratio = `(sam.mn-pop.mn)/std.error`. 

We take 100 samples and 10 units per sample. The std.error is
calculated from 10 random samples.

We compute the tratio and display its mean and sd():

mean : 0.08
sd : 1.16

The mean and sd for Zratio are:

mean: 0.069
sd: 1.00

**Definition**: For a std normal mean=0; and sd=1; It seems to be
worthwhile to have a look at the Zratio which is expected to be
std.normal if the sampling distribution is normally distributed (which
is the case here).

Just looking at the t-ratio we see that it is not std normal as is the Z-ratio.

**Checklist**: yes; true; none; not-chapter; not-running; none;
*example-matching-definition*; *time*;

> The (fewer the degrees of freedom)[1], the greater is (the extra
> variability, due to estimating the standard deviation)[2]. 

**Example**: 

With 9 dofs the t-ratio sd: 1.2

with 99 dofs the t-ratio sd: 1.06

**Definition**: checks out!

**Checklist**: yes; true; none; not-chapter; running; none;

> Under some conditions, however, the sampling distribution of the
> t-ratio is known.

come back later?

> if Y_bar is the average in a random sample of size n from a normally
> distributed population, the sampling distribution of its t-ratio is
> described by a student's t-distribution on n-1 degrees of freedom. 

**Claims**: If [1], [2] is described by [3].

**Example**: We take a normally distributed population using `rnorm`
of size 100000 and compute the t-ratio for a sampling distribution
with sample size 10. We take 10000 samples and The mean and sd of the
t-ratio are computed. We take a population of 10000 based on `rt`
which samples from a t-distribution. We compute the mean and sd of the
t-dist with n-1 dofs=9. the mean and sd are computed.

|      | zratio | tratio | xt    |
| ---  | ---    | ---    | ---   |
| mean | -0.003 | 0.0012 | 0.005 |
| sd   | 1.009  | 1.15   | 1.13  |
|      |        |        |       |

**Definition**: There is a close match between the standard deviations
but the match between the means don't seem to exist. I am not able to
place where the error is.

**Checklist**: yes; false; none; not-chapter; running; none;  


> Histograms for t-distributions are symmetric about zero. For large
> degrees of freedom, t-distributions differ very little from the
> standard normal. For smaller dofs, they have longer rails than normal.

Not going to do!

### 2.2.4 Unraveling the t-ratio

> Plausible values for mu, based on data are ... and ...

> if the (sample produces one of the 95% most likely t-ratios)[1], then mu
> is (between 0.067 and 0.331)[2]

**Subject**: if [1], what mu is.

**Predicate**: [2] 

**Example**: We come back to the twins study where one of the twins is
schizophrenic.

Here the estimate average of differences between the volume is 0.199
cm^3. The SE is determined to be 0.0615. 

We know that we are expecting a t-distribution with 14
degrees of freedom. We look at the 2.5th percentile and 97.5th
percentile (95% most likely values). The t-ratios are -2.145 and
+2.145. This gives us possible values of mu if the sample was drawn
with the 95% most likely values.

`-2.145<(0.199-mu)/0.615<2.145`

This gives 0.067 and 0.331

**Definition**:  Check!

**Checklist**: yes; true; none; not-chapter; not-running; none;
*example-matching-definition*; *time* 

> (A 95% confidence interval)[1] will contain (the parameter)[2] if
> (the t-ratio from the observed data happens to be one of the those
> in the middle 95% of the sampling distribution)[3]. Since 95% of all
> possible pairs of samples lead to such t-ratios, the procedure of
> constructing a 95% confidence interval is successful in capturing
> the parameter of interest in 95% of its applications. It is
> impossible to say (whether it is successful or not in any particular
> application)[5].

**Claims**: if [3], [1] will contain [2].

**Subject**: if [3], what [1] will contain.

**Predicate**: will contain [2].

**Example**: We take a population of mean 0 and sd 1. From which we
take a sample of 100 units.

For [1] we think of this random sample where we compute our expected
95% confidence interval for the mean. We compute it as follows:

``` R
## compute sample, se and q
x.sam <- x[sample(1:length(x),size=n.sample,replace=T)]
se <- sd(x.sam)/sqrt(n.sample)
q <- qt(0.975,df=n.sample-1) ## 95% quantiles

## Computing 95%CI
mean(x.sam) + q*se
mean(x.sam) - q*se
```

`-0.011 to 0.34`

For [3] we need to use the pop mean and check if the tratio actually
lies in the middle 95%.

``` R
tratio <- (mean(x.sam)-0)/sd(x.sam)*sqrt(n.sample)
tratio
```

With this we get the t-ratio as 1.85. This is within the middle 95%
t-sampling distribution for dof=99, i.e., `-1.98 to 1.98`.

*took me 3-4 hrs mygod!*

*splitting it helped attacking this claim* *as I write this I realize
I don't know what this 95% CI is, in the light that I was planning to
skip this or that I thought this was too easy. If I wanted to learn
this, I would look at solving more questions and that would be enough
it appears*

*I can't remember the amount of time I have been staring at this and I
still don't understand. I am unable to even point to my
confusion. Whereas I went to khan academy and life already seems much
simpler. I can blame two things the book, and/or my technique of
claims. I have having serious doubts to the value of this technique*

**Definition**: We thus see that the CI (`-0.011 to 0.34`) contains
the mean (`0`), when the t-ratio is within the middle 95% of the
t-sampling distribution. I checked this for a couple of random samples
and it checked out for all.

**Checklist**: yes; false; none; not-chapter; not-running; if-then;
*example-matching-definition*; *time* 

*To show if, I probably need to toggle*

**Claims**: To say [5] is impossible.

*No id how to prove this*

**Checklist**: yes; neither; none; not-chapter; not-running; none;
*impossible*; *failed*;

### 2.3 t-ratio for two-sample inference

Nothing

### 2.3.1 Sampling dist of the diff between 2 independent sample
averages

> The spread of the sampling distribution will be smaller with larger
> sample sizes. The sampling distribution is approximately normal, and
> will be more so with larger sample sizes. As was the case in Section
> 2.2.1, (the theoretical results about the sampling distribution of
> Y2-Y1)[1] are insufficient for (making inferential statements)[2],
> because SD(Y1_bar-Y2_bar), the standard deviation of this sampling
> distribution, depends on unknown parameters.

**Claims**: [1] are insufficient for [2].

**Subject**: [1] for making [2]. 

**Predicate**: are insufficient.

**Example**: "The sampling distribution is approximately normal, and
will be more so with larger sample sizes"

**Definition**: *I don't get what the statement wants to say. What do
they want to make inferential statements about? I don't know what
situation they are refering to with [1].*

**Checklist**: yes; neither; none; not-chapter; not-running; none;
*example-matching-subject*; *example-matching-definition*;
*definition-unclear*;

moving on.

### 2.3.2 SE for diff of 2 averages

> However, (comparing averages)[1] provides a (complete analysis)[3] only if (all
> other features of the two distributions are similar)[2]. Therefore
> assume in the following that the two populations have equal standard
> deviations: sigma1 = sigma2 = sigma

**Subject**: if [2], then what [1], provides.

**Predicate**: provides [3].

**Example**: *No example provided*. 

*We think of the schizophrenia study. Here we subtract the differences
in volume of hippocampus for every twin pair. We would like to obtain
if the average difference based on the samples is at a p-value of <5%
under the null hypothesis. That is what [1] could mean but the claim
suggests unless the two distributions are similar, we cannot do a
complete analysis. I don't understand why or how? or what this [3],
could mean. I don't have an example. Moving on. I want to get an
answer to this. But I think it will take far too much time. Maybe I
can come back to it.*

**Definition**: -

**Checklist**: yes; false; none; not-chapter; not-running; none  

*At this point this is very hard. I don't know what the end game is
(what complete analysis is needed)*

> If the two populations have the same standard deviation, sigma, then
> the sample standard deviations, s1 and s2, from the two groups are
> pooled to form a single estimate of it. Pooling standard deviations is
> accomplished using a weighted average on the variance scale, variance
> being the square of the standard deviation.



### 2.3.3 CI for diff between pop means

>If the populations are normally distributed, this t-ratio has a
>t-distribution with n1 + n2 -2 degrees of freedom.

I skip this. This is simple to test that can be simulated in R. If A,
B will correspond to X.

> Now a (statement about the likely values for the t-ratio from the
> distribution)[1] can be translated into a statement about (the plausible
> values for mu2-mu1)[2].

We go from computation of the t-ratios from the SE, and from there go
to predicting what mu is via CI's

**Subject**: [1] to [2] 

**Predicate**: can be translated.

**Example**: For the Bumpus data of the birds that died vs survived,
we look at the Humerus length. 

We think of computing (Y2\_bar-Y1\_bar)=0.01008, along with the
SE(Y2\_bar-Y1\_bar)=0.00567. We use SE as that is going to lead to the
"likely" value of t-ratio.

t-ratio = (estimate-mean-under-null-hypothesis)/SE.

The t-ratio is 1.77. Where the 95% middle t-ratios according to a
t-distribution with 57 degrees of freedom are: 2.002 and -2.002. So
this t-ratio is within the middle 95% of the sampling distribution.

For [1], we think of "The sample that we have is within 95% of the
samples that would be picked under the null hypothesis"

We can use the same t-ratio formula to compute the
mean-under-null-hypothesis for the extremes which is in this case:

`-0.00127 < mean-under-null-hypothesis < 0.02143`. 

For [2] we think of the above values for the mu, between which 0
lies. 

**Definition**: I don't know if "it is translated". But I guess the
message could be interpreted as, either we find the t-ratio and see if
it lies within the 95% band under the null hypothesis, or we determine
the extreme mean values and check if the mean under the null
hypothesis falls inbetween. Both inform us that the sample is within
95% of possible samples, i.e., the null hypothesis cannot be rejected.

**Checklist**: yes; false; none; not-chapter; not-running; none;
*example-matching-definition*; *unsure*

>There is a trade-off between the level of confidence and the width of
>the confidence interval. The level of confidence can be specified to
>be large by the user (and a (high confidence level)[1] is
>(good)[2]). but only at the expense of having (wider interval)[3]
>(which is (bad)[4] since the interval is less specific in answering
>the question of interest).

**Claims**: [1] is good.

**Checklist**: yes; false; none; not-chapter; not-running; none;
*no-example*; *failed*; (as it is important)

**Claims**: [3] is bad.

**Checklist**: yes; false; none; not-chapter; not-running; none 
*no-example*; *failed*; (as it is important) 

### 2.3.4 Testing a hypothesis about diff between means


> The p-value may be based on a probability model induced by random
> assignment in a randomized experiment (section 1.3.2) or on a
> probability model induced by random sampling from populations, as
> here.

> If the p-value is small, then either the hypothesis is correct---and
> the sample happened to be one of those rare ones that produce such
> an unusual t-ratio---or the hypothesis is incorrect. Although it is
> impossible to know which of these two possibilities is true, the
> p-value indicates the probability of the first of these results.

not sure if there is anything worthwhile to do here.

> The smaller the p-value, the stronger is the evidence that the
> hypothesis is incorrect. A large p-value implies that the study is
> not capable of excluding the null hypothesis as a possible
> explanation for how the data turned out. A possible wording in this
> case is "the data are consistent with the hypothesis being true". It
> is wrong to conclude that the null hypothesis is true.

-

> For the bumpus example the t-statistic for the hypothesis of "no
> difference" in population means is 1.778.

-

The choice of one-sided or two-sided depends on how specific the
researcher can pinpoint the alternative to the null hypothesis. 



When and how to use one and two sided p values.

I know now.


### 2.3.5 The Mechanics of p-value computation
### 2.4 inferences in a two-treatment randomized experiment

> Chapter 2 has thus far discussed inference procedures whose
> motivation stems from considerations of random sampling from
> populations that are conceptual, infinite, and normally
> distributed. While there seems to be considerable difference between
> the situations, it turns out that the (t-distribution uncertainty
> measures discussed in this chapter)[1] are (useful approximations to
> both the randomization and the random sampling uncertainty measures
> for a wide range of problems)[2]. The practical consequence is that
> t-tools are used for many situations that do not conform to the
> strict model upon which the t-tools are based, including data from
> randomized experiments. Conclusions from randomized experiments,
> however, are phrased in the language of treatment effects and
> causation, rather than differences in population means and
> association.

**Claims**: t-tools are derived under random sampling models, when
populations are normally distributed.

**Subject**: How T-tools are derived.

**Predicate**: derived under random sampling models, when populations
are normally distributed.

**Example**: The student's t-distribution on n-1 degrees of freedom

**Definition**: According to the book this is based on random sampling
and normally distributed model.

Theorem types stating: "If Ybar is the average in a random sample of size n from a normally
distributed population, the sampling distribution of its t-ratio is
described by Student's t-distribution."

**Checklist**: yes; true; none; not-chapter; not-running; none;

**Claims**: [1] for randomization is a useful approximation.

**Checklist**: yes; neither; none; not-chapter; not-running; none;
*no-example*; *unsure*; 

**Claims**: [1] for random sampling is a useful approximation.

*For useful application I need something to compare it with, I mean
why is it a useful approximation, aybe something good comes out of it?
maybe it speedens up the process or maybe it simplifies and still
gives the same result. The problem is I don't have a justification
example for useful. Again I can give an example, but unless I compare
it to reality I see no meaning here. nam saying?*

**Checklist**: yes; neither; none; not-chapter; not-running; none;
*no-example*; *unsure*; 

**Claims**: As shown in chapter 3, they often work quite well even
when the populations are not normal.
come back later.

> The (randomization-based procedure)[1] relies on (a relationship between
> testing and confidence intervals)[2]: Any hypothesized parameter value
> should be included or excluded from 100(1-alpha)% confidence
> interval according to whether its test yields a two-sided p-value
> that is greater than or less than alpha.

**Claims**: There is a relationship between testing and confidence
intervals.

Come back late if you can!

**Checklist**: yes; neither; none; not-chapter; not-running; none;
*no-example*; *failed* 


**Claims**: [1] relies on [2].

**Subject**: What [1] relies on.

**Predicate**: relies on [2].

**Example**: We look at the same creativity study.

The randomization procedure to determine the creativity study starts
with looking at one value of delta. 

Let's say delta=5. We subtract 5 from all scores in the intrinsic
group. Now with this homogeneous mixture, we perform the
randomization and check if the estimate is greater than the two-sided
p-value. If so, then that delta=5 is within the 95% confidence
interval. 

By trial and error we keep shifting delta's value to find the limits
of this 95% CI.


**Definition**: This procedure has 2 parts to it. First step involves:
Taking each hypothesized parameter value (delta=5) and then TESTING
if that delta value is likely (>5% p-value). The second step involves:
doing this until you find the LIMITS. 

The relationship between testing and limits seems to be that TESTING
over and over again can lead to LIMITS.

Confidence intervals imply, what are most likely values of the
parameter are.

**Checklist**: yes; false; none; not-chapter; not-running; none  
*example-matching-definition*; *failed*; *time* (understanding if the
example and why it was so took time)

*This is really hard. How long it takes to understand this is still
puzzling. A few hours >3hrs easily and no answer. How to show the
relationship is hard.*

> The t-based p-values and confidence levels are only approximations
> to the correct values from the randomization distributions.

There are many claims.... like randomization are the correct values,
tbased pvalues are approximations to the correct values.

Later if time permits.

> The approximation based on the t-distribution is quite good.

**Claims**: The Approximation of the Randomization Distribution of the
t-statistic is quite good (as compared to the "actual").

**Subject**: How the approximation of the randomization distribution
of the t-statistic compares with the "actual"

**Predicate**: is quite good.

**Example**: We go back to the creativity study we saw in the first
chapter. Here, a group of "creative people" were picked and then
randomized into two interventions. The goal is to identify causal
relations between the intervention and the creativity score.

In total there were 47 people split as 23 and 24 into the different
interventions namely intrinsic and extrinsic.

For the given group we have the t-statistic = `(4.14-0)/1.42 =
2.92`. This gives a 0.0027 p-value when looked up on a t-distribution
of 45 dofs.

We would like to compare this to the "actual". In chapter 1, there is
an estimation of the p-value based on the randomization
procedure. This is 0.004 one-sided p-value. This is obtained by
looking at all randomizations of the creativity scores and the
resulting difference between the two groups and comparing how often a
number as high as 4.14 is obtained.

0.0027 and 0.004 both seem to give the same conclusion if we assume a
cutoff of 5%. but the values as such are different (48% off). The text
book however uses another number to compare to 0.0027. It uses the
p-value from the randomization procedure not of the creativity scores
but of the t-ratio. And then the p-value is 0.002.

**Definition**: The t-ratio p-value from the randomization procedure
(0.002) is close to the value obtained by the t-statistic. According
to the book these two numbers should be compared and the result seems
to check out, that its "quite good". But it is not clear why the other
p-value is not relevant.

**Checklist**: yes; true; none; chapter; running; none;
*example-matching-definition*; *time* 

### 2.5 related issues

**Claims**: It is difficult and unwise to decide on absolute cutoff points for
p-values in all situations.

*One reason I can think of is say you land up with a value of 5.1% but
you don't accept it but you land up with 5% then you accept it. There
seems to be no difference between 5 and 5.1% such that you reject a
hypothesis or fail to reject it. But we need an example for ""*

**Subject**: How difficult absolute cutoff points for p-values are to decide.

**Predicate**: are difficult and unwise

**Example**: If we take the absolute cutoff point to be 5%, and we get
a sample with 5.1% then we accept the null hypothesis.

If we get 4.9%, then we are going to reject the null hypothesis.

*This example is made up, but I don't have an example where you get
such numbers. Either I don't give an example, or I try to explain the scene.*

**Definition**: Looks like choosing 5% and saying it is valid for all
cases as a hard cut-off seems to be difficult.

Also choosing 5% and seems to be unwise as this would result in 2 very
close percentages being interpreted completely differently.

**Checklist**: yes; true; none; not-chapter; not-running; none;
*no-example*; *unsure*; *example-matching-subject*; *unsure*;

*Maybe writing out the subject and predicate is not overrated
after all. helps me decide with I need to give an example for.*

*Although important for leading to advances in the theory of
statistics. The rejection region apporach has largely been discarded
for practical applications and p-values are reported instead.*

**Claims**: p-values can be comprehended by comparing them to events
whose probabilities are more familiar. 

**Subject**: Comparing p-values to events whose probabilities are more familiar

**Predicate**: 

**Example**: We look at the probabilities with respect to a fair coin.

p(4 heads in a row) = 6.3% 

P(5 heads in a row) = 3.1% 

P(6 heads in a row) = 1.5%

P(10 heads in a row) = 0.1%

If we get 10 heads in a row, then we would start expecting that maybe
our coin is probably not fair.

**Definition**: 

*Sounds more like a thought experiment than anything else. Not sure if
this is accepted as empirical evidence.*

**Checklist**: yes; false; none; not-chapter; not-running; none  
*example-matching-subject*; *unsure*; *example-matching-definition*;
*unsure* 

*unable to think of this in terms of null hypothesis population
etc... I think this is important to formalize the way of thinking.

Formal thinking subject etcc... seems to help me to narrow down my
problem (example ? tolerance... design with otten... formalizing way
to determine what I don't understand... from daniel rixens book)*

**Claims**: The actual confidence levels are not important for some
cases.

**Example**: Experiments were made to understand the value of the
parameter "general relativity" and to see who was right. Newton made a
0 prediction and Einstein made a prediction of 1. A graph showing the
value of gamma over several years of experimentation, shows values
ranging from 0.6 to 1.2 in 1920 and values ranging from 0.9 to 1.1
in 1985. It is more clear with time that the value of the parameter is
1.

And in the past (1920) it was clear that gamma value was not 0 atleast
and that newton was wrong.

**Definition**: In the above example, we see that we don't need
confidence levels. We seemed to be interested in the whole spread. It
allowed to say what the gamma value was not (despite the large
measurement errors in 1920).

#### Thoughts!

Claims which I don't understand and think are important!

Only if I go deep do I understand the tupe it is
... *example-matching-definition* type or whatever.


## Chapter 3 a closer look at assumptions (2)

> The two-sample t-tools are often valid even if the population
> distributions are non-normal or the SD's are unequal. 
>
> An effective alternative is to apply the standard tools after
> transforming the data.

comeback later!

### Case studies
### 3.1.1 Cloud seeding to increase rainfail (1)

> (Massive injection of silver iodide into cumulus clouds)[1] can lead to
> (increased rainfall)[2].

**Claims**: [1] can lead to [2].

**Example**:  Clouding seeding to increase rainfall randomized
experiment: 

On 52 days that were deemed suitable for cloud seeding, a random
mechanisms was used to decide whether to seed the target on that day
or to leave it unseeded as a control. An airplane flew in either
cases.

Precipitation was measured on those 52 days.


Estimate = 3.1 times as large as when not seeded. 95% CI is 1.3 times
to 7.7 times.

**Definition**: Since randomization was used, it is safe to inform
causality. Checks out.

**Checklist**: yes; true; none; chapter; not-running; none;



### 3.1.2 Effects of Agent Orange on Troops in Vietnam (2)

> Dioxin levels tends to be higher for the Vietnam veterans than for
> the non-vietnam veterans.

**Claims**: Dioxin levels in Vietnam Veterans than the non-vietnam
veterans is higher

**Example**: In a non randomly selected sample of 646 Vietnam veterans
who served in Vietnam in most heavily treated regions with Agent
Orange (Dioxin) were selected as the test group. 97 non-Vietnam
veterans who served in US and Germany we the control. Blood tests were
done Dioxin levels reported.

A one-sided p-value of 0.4 for the null hypothesis was obtained


**Definition**: The p-value of 40% informs that there is not
sufficient evidence to conclude that the levels are higher for the
test group than the control. 

This shall not be left at this, as the sample was not randomly
selected and we need to investigate more. For example,
non-participating Vietnam veterans may have failed to participate
because of dioxin-related illnesses. At best we can say there isn't
sufficient evidence and that the results could be seriously biased.

**Checklist**: yes; neither; none; chapter; not-running; none;


> Inference to populations is speculative.

*Not sure how to give an example for this in this context. However we
have seen before that without randomness inference to population is
speculative.*


### Robustness of two-sample t-tools
### 3.2.1 The meaning of robustness (2)

> Actual conditions did not seem to match the ideal models upon which
> t-tools are based for both studies.

come back later!


### 3.2.2 Robustness Against Departures from normality (6)

> Underlying normality is not a serious issue, as long as sample size
> are reasonably large.
> Many empirical investigations and related theory, however confirm
> that the t-tools remain reasonably valid in large samples

*no-example* 

> The question about sample size is difficult to answer

> If the two populations have same SD and approximately shapes, and if
> the sample sizes are equal, then the validity of the t-tools is
> affected moderately by long-tailedness and very little by skewness.

> If the two populations have the same standard deviations and
> approximately the same shapes, but if the sample sizes are not
> approximately the same, then the validity of the t-tools is affected
> moderately by long-tailedness and substantially by skewness. The
> adverse effects diminish, however, with increasingly large sample
> sizes.

> If the skewness in the two populations differs considerably, the
> tools can be very misleading with small and moderate sample sizes.


[1] --> Standard Deviation and approximate same shape

[2] --> Sample sizes

[3] --> Validity of t-tools 

[4] --> long-tailedness

[5] --> skewness

**Claims**: If [1] is same, and if [2] is equal, then [3] is affected
moderately by [4].

**Example**: 1000 computer simulations involving a success criterion
that 95% of the simulations will contain the parameter of interest
(mean difference of the two population). All the populations have the
same SD and same sample sizes.

For a given sample size and SD being equal in the population, we see
that using t-tools we get the following:

| Sample size  | Long tail | Short tail  |  
| ------------ | --------- | ----------- |  
| 5            | 98.3      | 94.5        |  
|              |           |             |  

**Definition**: Looking at the output it looks like the output is good
for both short tailed and longtailed non normal distributions. I am
not sure it affects at all, let alone moderately. 94.5% is pretty darn
close to 95% (which is the criterion for success).

**Checklist**: *example-matching-definition*; *unsure*; 

**Claims**: If [1] is same, and if [2] is equal, then [3] is affected
very little by [5].

**Example**: Same example but this time we look at the skewedness.

| Sample size | strongly skewed | mildly skewed |
|-------------|-----------------|---------------|
| 5           | 95.5            | 95.2          |

**Definition**: I have no idea what they are talking about. It looks
perfect in my eyes. There seems to be that nothing is affected.

**Checklist**: *example-matching-definition*; *unsure*;

*Until I actually wrote this, I totally believed their claim.*

**Claims**: If [1] is same, and if [2] is NOT equal, then [3] is affected
moderately by [4].

**Example**: Example not available for this case.

**Definition**: -

**Checklist**: *no-example*
 
**Claims**: If [1] is same, and if [2] is NOT equal, then [3] is
affected substantially by [5].

**Example**: Example not available for this case.

**Definition**: -

**Checklist**: *no-example* 
 
**Claims**: If [5] differs considerably, then the tools can be very
misleading with small [2].

**Example**: The same example given in the book.

| Sample size | strongly skewed | mildly skewed |
|-------------|-----------------|---------------|
| 5           | 95.5            | 95.2          |


**Definition**: If we consider a sample size of 5 as small, then I
have no idea what they talking about. 

**Checklist**: yes; false; 
 
**Claims**: If [5] differs considerably, then the tools can be
misleading with moderate [2].

**Example**: 
| Sample size | strongly skewed | mildly skewed |
|-------------|-----------------|---------------|
| 5           | 95.5            | 95.2          |
| 100         | 94.8            | 95.3          |

**Definition**: I think this is false as well considering moderate
sample sizes as 100.

**Checklist**: yes; false; none; not-chapter; not-running; none;
*claim-reality-mismatch*; *failed*  

> Of the five distributions examined, only the long-tailed
> distribution appears to have success rates that are poor enough to
> cause potentialy misleading statements---and even those are not too
> bad. 

**Example**:

| size | strg. skewed | Mod. skewed | mild. skewed | longtailed | shorttailed |
|------|--------------|-------------|--------------|------------|-------------|
| 5    | 95.5         | 95.4        | 95.2         | 98.3       | 94.5        |


**Definition**: The longtailed distribution appears to have only
higher success rates than shorttailed. How is that a having poor
success rates.

**Checklist**: yes; false; none; not-chapter; running; none  

### 3.2.3 Robustness Against Differing Standard Deviations (3-4)

> In this case, the pooled estimate of standard deviation does not
> estimate any population parameter and the standard error formula
> which uses the pooled estimate of sd, no longer estimates the sd of
> the difference between sample averages.

*no-example* 

> (t-tools)[1] remain fairly valid when the (standard deviations are
> unequal)[2], as long as the (sample sizes are roughly the same)[3]. 

**Claims**: If [2] and [3], then [1] remain fairly valid.

**Example**: A computer simulation involving two populations that have
different std.deviations but are normal.

For n1=10 and n2=10, and sigma2/sigma1=1/4; success rate as a result
of simulation is 95.2%.

**Definition**: [1] remains fairly valid due to the 95% success rate.
 
**Checklist**: yes; true; none; chapter; not-running; none 

> For substantially different sigma's and different n's the CI are
> unreliable the worst situation is when the ratio of the sd is much
> different from one and the samller sized sample is from the
> population with the larger sd.

**Example**: 

A computer simulation involving two populations that have
different std.deviations but are normal.

For n1=10 and n2=40, and sigma2/sigma1=1/4; success rate as a result
of simulation is 71%.

**Definition**: unreliable indeed.

**Checklist**: yes; true; none; chapter; running; none 

### 3.2.4 Robustness against Departures from Independence
### 3.2.4.1 Cluster effects and Serial effects (2-3)

> Whenever knowledge that one observation is, say, above average
> allows an improved guess about whether another observation will be
> above average, independence is lacking.

*no-example* 

*The above is just a definition*

*In any case, I don't have one example for independence lacking and
its consequence.*

> Cluster effect occurs sometimes when the data have been collected in
> sub groups. For example, 50 experimental anmals may have been
> collected from 10 litters.

I don't know what this effect means or what is the impact.

*no-example* 

> The serial effect: The other type of dependence commonly encountered
> is caused in which measurements are taken over time and observations
> close togehter in time tend to be more similar than observations
> collected at distant time points. 

*no-example* 


### 3.2.4.2 Effects of lack of independence (3)

> When the independence assumptions are violated, the standard error of
> the difference of averages is an inappropriate estimate of the sd of
> the difference in averages.

*can be tested*

*no-example* 

> The t-ratio no longer has a t-distribution, and the t-tools may give
> misleading resutls.

*no-example* 

> It is unwise to use t-tools directly if cluster of serial effects are
> suspected.

*no-example* 

### Resistance of Two-Sample t-Tools
### 3.3.1 Outliers and Resistance (3)

> Long-tailed population distributions are not the only explanation for
> outliers.

**Example**: contamination could be another reason for an outlier.

In the Agent Orange study about US veterans and Dioxin, 2 outliers are
identified. 

The sample was about people who were in vietnam in Orange rich
zones. But the two outliers in this study, one of them was not exposed
to herbicides during his stay in Vietnam. The other had 180 days of
indirect military exposure to herbicides.

**Definition**: The two outliers are contaminants rather than
long-tailedness of the population in this case.

**Checklist**: yes; true.
 
> It is irrelevant to distinguish between a natural long-tailed
> distribution and the one that includes outliers that result from
> contamination.

*no-example* *The idea is that it becomes irrelevant because they
both don't capture the group behavior.* 

> It is useful to know how sensitive a statistical procedure may be to
> one or two outlying observations.

**Example**: We comeback to the same Agent Orange data. We have two
outliers here. The sample size is >600 in the case of veterans in
Vietnam. 

The one-sided p-value is 40% with all observations, 48% with one
outlier removed and 54% with both outliers removed.

**Definition**: Although the conclusion didn't change regarding
rejecting or accepting an hypothesis, the variation is quite large
between keeping outliers despite the large number of the sample.

This example I think checks out that it is useful. I think this
example is instrumental in showing the havoc outliers can breathe when
values become closer to the "cutoff"(say 5%)

**Checklist**: yes; true;

### 3.3.2 Resistance of t-tools (2)
> Since t-tools are based on averages, they are not resistant 

**Example**: In a hypothetical sample of 10,20,30,50,70, the sample
average is 36, and the sample median is 30. Now change the 70 to 700,
and we see that the average becomes 162, but the sample median remains
30.

**Definition**: As t-tools are based on averages they are not resistant.

**Checklist**: yes; true; 

*because-should-due-to*; *example-matching-subject*; *unsure*; (I am
almost certain this is wrong, because it doesn't show what happens to t-tools.)

> one or two outliers can affect the CI or change the p-value enough
> to completely alter a conclusion.

*no-example* 


### 3.4 Practical Strategies for the two-sample problem
### 3.4.1 Consider Serial cluster effects (1)

> Were the subjects selected in distinct groups? Were different Groups
> of subjects treated differently in a way that was unrelated to the
> primary treatment? Were different responses merely repeated
> measurements of the same subjects? Were observations taken at
> different but proximate times or locations? Affirmative answers to any
> of these questions suggest that independence may be lacking.

**Claims**: If the answer to above questions is yes then it suggests
that independence may be lacking.

*no-example* 
*definition-unclear*; *failed* (should know how to check for
independence but not given in the text clearly)
 

### 3.4.2 Evaluate the suitability of the t-tools (1)

> A (transformation)[2] should be considered if the (graphical displays of
> the transformed data appear to be closer to the ideal conditions)[1].

**Subject**: If [1], Should [2] be done?

**Predicate**: should be considered

**Example**: We go back to the Cloud seeding case study. We see in the
normal box plots that both the groups look "skewed". And the spread
(SD) looks much larger for the seeded one. 

When we make this into a logarithm chart we see that immediately the
spread looks very similar, the skewness vanishes (closer to normal
looking). These are conditions that are close to what the t-tools are
created based off of.

**Definition**: Checks out. Now we are confident of applying the
t-tools. As we have seen earlier, in the case of long-tailed
populations the t-tools are expected to perform very poorly with their
success rate.

*Something more nicer would be to check the results of using the
actual values and the transformed values and seeing how different the
results are...*

**Checklist**: 
*example-matching-definition*; *unsure*; *because-should-due-to*;
*unsure*
 
 
normality and same sd and close sample size

### 3.4.3 A strategy for Dealing with Outliers (2)

> If investigation reveals that an outlying observation was recorded
> improperly or was the result of contamination from another population,
> the solution is to correct it if the right value is known or to leave
> it out.

**Example**: We comeback to the same Agent Orange data. We have two
outliers here. The sample size is >600 in the case of veterans in
Vietnam. 

The one-sided p-value is 40% with all observations, 48% with one
outlier removed and 54% with both outliers removed.

Here even though it does not affect the conclusion of this study, the
variation in the p-value is 14%.

**Definition**: Quite a large variation, and by removing them we are
more closer to the truth (without contaminants)

**Checklist**: 
 *example-matching-definition*; *unsure* 

> Two statistical approaches for dealing with this situation exist.

> An important aspect of adopting this procedure is that an outlier
> does not get swept under the rug simply because if is different from
> the other observations.

### 3.4.4 Agent orange (2)

> the skewess is mild and unlikely to cause any problems with the
> t-test or the CI

> It is useful to see what else can be learned about these two outliers.

### Transformations of the Data
### 3.5.1 The Logarithmic Transformation (1)

> The most useful transformation is the logarithm for positive data.


### 3.5.1.1 Recognizing the need for Log (5)

> The data themselves usually suggest the need for a log
> transformation.

> If the ratio of the largest to the smallest measurement in a group
> is greater than 10, then the data are probably more conveniently
> expressed on the log scale. 

> If the graphical displayes of the two samples show them both to be
> skewed and if the group with the larger average also has the larger
> spread, the log transformation is likely to be a good choice.

> Small numbers get spread out more, while large numbers are squeezed
> more closely together. 

> The overall result is that the two distributions on the transformed
> scale appear to be symmetric and have equal spread--- just the right
> conditions for applying the t-tools.

### 3.5.2 Interpretation after a Log Transformation (1)

> For some measurements, the results of an analysis are appropriately
> presented on the transformed scale. 


### 3.5.2.1 Randomized experiment model
### 3.5.2.2 Population model (5)

> Mean of logged values is not the log of the mean

> Taking the antilogarithm of the estimate of the mean on the log scale
> does not give an estimate of the mean on the original scale. 

> If, log-transformed data have symmetric distributions, Mean[logY]=
> Median[logY] It is evident that the anti logrithm of the mean of the
> log values is the median on the original scale of measurements.


> the point of this is that a very useful multiplicative
> interpretation emerges in terms of the ratio of population medians.

> Median is a better estimate of the center of a skewed distribution
> than the mean.

> In addition, back-transforming the ends of a confidence interval
> constructed on the log scale produces a condidence interval for the
> ratio of medians

### 3.5.2.3 Example (sex discrimination) (1)

> graphical displays of the log-transformed salaries indicate that
> analysis would also be suitable on the log scale.

### 3.5.3 Other transformations for Positive measurements (5)

> There are other useful transformations for positive measurements
> with skewed distributions where the means and standard deviations
> differ between groups.

> The square root transformation applies to data that are counts.

> The reciprocal transformation 1/Y applies to data that are waiting
> time

> The arcsine square root transformation and the logit transformation
> apply when the measurements are proportions between zero and one.

> Only log gives such ease in converting inferences back to the
> original scale of measurement.

> Situations arise where presenting results in terms of population
> medians is not sufficient.

### Related Issues
### 3.6.1 Prefer Graphical Methods over Formal Tests for Model (3)
Adequacy

> Tests for normality and tests for equal sd are available in most
> computer programs.

> The diagnostic tests are not very helpful for model checking.

> Graphical displays provide a good indication of whether or not the
> data are amenable to t-analysis and, if not, they often suggest a
> remedy.


### 3.6.2 Robustness and Transformation for Paired t-Tools (2)

> When the observations within each pair are positive, either an
> apprent multiplicative treatment effect (in an experiment) or a
> tendency for larger differences in pairs with larger average values
> suggest the use of log transformation. 


### 3.6.3 Example-Schizophrenia
### Summary (1)

> This is the situation where log-transformation confirms the adequacy
> of the transformation.

## Summary

mean of sampling distribution of average = mean of distribution

SD(sampling distribution) = SD(population)/sqrt(n)

## Notes

- I feel that I am not taking too seriously every single sentence,
  unless it is claims of importance.. For example they talk abotu IJ
  GOOd an what he expects to happen in the future.
  
  Who cares? What are the claims? are there examples for it? moving on!
## decide the plan today!

## Statistics Superintelligence

| chapter | time | phrases | dist | claims | pages | mins/phr | mins/page |
|---------|------|---------|------|--------|-------|----------|-----------|
| 2.2     | 10   | 2       | 1    | 1      | (6)   | 5        |           |
| 2.3     | 60   |         | 5    |        | 8     |          | 7.5       |
| 2.3     | 21   | 2       | 0    |        | (8)   | 10       |           |
| 2.4     | 20   |         | 1    |        | 4     |          | 6.25      |
| 2.5     | 15   |         | 1    |        | 3     | 5        |           |
| 2.4 2.5 | 20   |         | 2    |        |       | 4        |           |
| 3       | 27   |         |      |        | 5     |          | 5.4       |
| 3       | 60   |         | 13   |        | 6     |          | 10        |
| 3       | 159  | 34      | 9    |        |       | 4.67     |           |

4.6 phrases/hour phrases including reading

15 hrs and 30 claims i.e., ~70 phrases

## Statistics Statistical sleuth
	
do excercise

| chapter   | time | phrases | dist | claims | pages | mins/phr | mins/pg |       |
|-----------|------|---------|------|--------|-------|----------|---------|-------|
| 1.1.1     | 19   |         |      |        | 3     |          | 6.3     |       |
| 1.1.1     | 40   | 2       |      | 1      |       | 20       |         |       |
| 1.1.2     | 7    |         |      |        | 2     |          | 3.5     |       |
| 1.1.2     | 22   | 2       |      | 1      |       | 11.5     |         |       |
| 1.2.1,.2  | 60   |         | 2    |        | 4     |          | 15      |       |
| 1.2.1,2   | 90   | 16      | 8    | 8      |       | 5.6      |         |       |
| "         | 60   | 3       | 2    | 2      |       | 20       |         |       |
| ""        | 25   | 6       | 1    | 3      |       | 4.16     |         |       |
| 1.3.1,2   | 40   |         | 8    |        | 4.5   |          | 8.8     |       |
| 1.4       | 24   |         | 4    |        | 2     |          | 12      | 3/09  |
| 1.5.1     | 48   |         | 9    |        | 5     |          | 9.6     | 3/09  |
| 1.2.3     | 90   | 8       | 10   | 4      |       | 11.25    |         | 3/09  |
| 1.2.3     | 80   | 6       | 9    | 3      |       | 13.3     |         | 4/09  |
| 1.3.1,2   | 90   | 9       | 11   | 4      |       | 10       |         | 4/09  |
| 1.4,1.5   | 80   | 9       | 11   | 4 (3)  |       | 8.88     |         | 4/09  |
| 1.5       | 75   | 12      | 15   | 6      |       | 6.25     |         | 5/09  |
| 2.1,2*    | 46   |         |      |        | 7     |          | 6.57    | 7/09  |
| 2.1       | 45   | 4       | 7    | 2      |       | 11.25    |         | 7/09  |
| 2.2       | 90   | 8       | 8    | 4      |       |          |         | 7/09  |
| 2.2.4,2.3 | 54   |         |      |        | 8     |          | 6.75    | 8/09  |
| ^^        | 27   |         |      |        | 3     |          | 9       | 8/09  |
| ^^        | 30   |         |      |        | 4     |          |         | 8/09  |
| ^^        | 90   | 3       |      | 1      |       |          |         | 8/09  |
| ^^        | 100  | 4       | 7    | 2      |       | 25       |         | 9/09  |
| ^^        | 28   | 2       | 2    | 1      |       |          |         | 9/09  |
| ^^        | 80   | 0       | 9    | 0      |       |          |         | 9/09  |
| ^^        | 120  | 2       | 8    | 1      |       | 75       |         | 10/09 |
| ^^        | 45   | 3       |      | 1      |       | 15       |         | 11/09 |
| ^^        | 60   | 4       | 9    | 2      |       | 15       |         | 11/09 |
| 3.1,3.2   | 50   |         | 9    |        | 7     |          | 7.14    | 12/09 |
|           | 33   |         |      |        | 4.5   |          | 6.66    | 12/09 |
|           | 60   |         | 9    |        | 5     |          | 12      | 14/09 |
|           | 13   |         | 3    |        | 2.75  |          | 4.7     | 14/09 |
|           | 97   | 18      | 8    | 9      |       | 5.38     |         | 15/09 |
| pr**      | 52   | 20      | 2    | 10     |       | 2.6      |         | 15/09 |
|           |      |         |      |        |       |          |         |       |

*2 pages of images
** started skipping claims if I had no example, and if I could see the
level to which if needed I could run a simulation. The goal seems to
be to think inexamples as much as possible. Simply skip claims or mark
them as no example. Also, I stopeed writing the whole thing. Am done
with the setup! Feels better too.
30 mins 900 words while listening to music.

## todo

- stm reply ages back on less wrong!
- work on failures
- which content to work on? the book or jblevins stuff?

- **identify failures from previous post and check if we failed?**

Identify patterns and inform failure! Make simple statistic on number
of failures over 133 claims.. type of failures and claims.


## Where the examples are hidden at?

Today I am working and trying to explain to a colleague. Why something
works and he doesn't want to accept it. I am telling him I thought
about it and what I say is right. He asked me why the tooling flange
and the actual flange were not connected in the diagram. I told him
they are, via "repros". i.e., I made a claim, and I openly said, I
have no idea how to prove it. I didn't know how I was going to find an
example. It was going to be impossible to find an example or test it
you know. but I had to convince him and more importantly convince
myself.

Me having spent so many hours FAILED majorly, when my boy pointed out
something. 

He made a change to the tooling and asked me if there is any
difference. i.e., we took a real example... that we could test on
whole tolerance train, albeit with "logic". That was it. I was
convinced. Any change we did, to the tooling the result was
agnostic. And taking that motherfucking example with dovel pins, made
everything so much more clear for me.

Same thing a couple of days back I was making tolerance trains with
blocks.. I made a couple and in the end I said fuck this shit. I don't
work with boxes I work with actual parts. So I drew them to get a feel
of what they would look like and what the box represented.
