---
layout: post
comments: true
title:  "  The story so far(3) "
date:    10-08-2017 19:15
categories: The Beginning
permalink: /:title.html
published: True
---
#### Story so far

In [Story so far][story_1] we discussed about how we have conflicting views. One time we want to save people, another time we don't. Our feelings change a lot for the same thing based on time. We further realized that our feelings are inconsistent and that we have circular preferences. It appears that it is quite unclear what we truly want, because there seems to be lot of changes with feelings on the same topic at different times. Looking at people dying makes us cry. We want to do something, but then too bad I have to go on a vacation. If we leave it up to feelings then I am going on a vacation, that is important to me right now. But I will also feel bad for those people who are dying, but then thats that. Our Value system is masked by heuristics and biases. Our heuristics and biases, are our the metaphorical eyes through which we glimpse into our value system. Having seen some of the heuristics and biases and related examples, we understand that the output of the heuristics might not always be the right thing for us, irrespective of the value system.

In the [Story so far(2)][story_2], we realize that everything is a feeling. As much as we might want to frown upon them because they are not consistent/are circular, that is all we have as an indication of what we might want. Having understood that we have feelings and that these attempt to badly inform us what we want, we wonder what options we might have. We check if we can ask someone else, what we should be doing in life, and sadly we can't. We have a [solution inside our head][tj_solution] with which we check if an action seems okay or not. The gap between accepting an action as ok and doing it seems large. For example, you can agree(based on the solution inside your head) that people are dying and that you need to save them, but you end up doing squat. Make no mistake, we only partially know what our true solution inside our head is. We are yet to uncover it fully(!). 

Alas we stumble via [Eliezer's words][ele_intuitions] that you can't give up on feelings. That is the only thing we have. If you give up feelings by not trusting it at all, then you are but an empty philosopher who is but a rock. If you follow feelings to the letter, you are met with circular preferences. Paraphrasing Eliezer, "Feeling is not a curse word when it comes to morality, cause there is nothing else to argue from. Considering the way we have been made, it seems like a good idea to formalize, reflect on, extrapolate out to see if an idea has sensible consequences etcetera."

We briefly attempt to understand how important life is to us. We deduce that 'we want to save lives' and not let go of lives, based on how we emote. We also explore parts where in we see that location, distance, visibility shall not really matter whether we want to save a life or not. 

In this weeks episode...

#### The importance of life

I feel like I am going to have to assume that life is very important to us as evidenced by how strong our feelings are. But how important is it? what all should we be ready to give up for it? How I can determine from my faulty feelings what exactly is the way to go, I am not sure. In other words, what does my true value system want (but for now only with respect to life)?

As discussed in earlier posts, 2 Israeli scientists, observed that people are willing to donate money for the costs of a very expensive treatment, more for an individual rather than for a group of 8. How can it be better to have 1 extra happy child in this world, while it might be somehow worse to have 8 more happy children? How can the value of one child be reduced to such an extent because there are 7 other children? In another study, The willingness to pay is the same whether we want to save 1k birds or 10k birds. Looking at this and the fact the brain never appears to understand large numbers we conclude that there is a high chance that the brain is blatantly making a mistake. We concluded earlier that this brain cannot but do arithmetic subconsciously. So now we are left with the option of understanding what the brain might want. But we don't know what exactly we should do.

Well Eleizer says that we need to see 'shut up and multiply' the value we might get from saving one life. Use cold calculations and come to the answer. If one life is worth X, then 2 lives should be worth 2*X. Every life shall be worth the same, because there is no reason to discriminate one life from another. Unless of course there is "moral" reason to say that blacks are worth 0.5X (which there isn't), or that disabled men are worth much less than normal men, then essentially one life is worth X and n lives worth nX. But the brain does not scale feelings based on the number of lives saved. You don't understand the meaning of saving 1k lives or 10k lives.

Why does the value of saving every successive life have to be the same? Why can't it be that the first life we save is worth more to us than the second life we save and so on the value of each successive life? Don't we see such an observation with the birds study? Earlier we *only* concluded that the conclusion by the brain regarding value of '10k birds = value of 1k birds' seems wrong considering that it doesn't understand large numbers. We are yet to discover what needs to be done. In real life though, if I see a person dying and I just saved him, I can't imagine scenario any different when I see another person dying after that. Looking at examples of natural disasters, I have seen videos over videos where people work round the clock, for some stupid reason to save as many people as they can. This merely suggests that the total value gained is at a peak. It doesn't offer any information as to the value of successive lives. Nevertheless, people being saved has been maximized in each of the cases, where emotional arousal is high. Look at [this for example][story_i], man works round the clock to save people, goes out of the way to stay awake overnight and saves so many children. 

In other words, I don't care regarding the value of each successive person. Fuck that. Its useless for me, right now. What is the action I hope to see when I see two people dying, or many people dying? I hope to see people running around to save as many lives as they can. I believe that I am justified in looking at the doctor in [this example][story_i] as our potential role in society, as we as humans are flawed. For example, When something happens around us, the emotional arousal is much higher, and in other cases not so much. We have seen earlier that the emotional arousal for an event happening in a closed sealed room is 0, as we don't see it. There are things that we would have emotional arousal to, and things that we wouldn't have emotional arousal to, despite the fact that we both lead to the same outcome. Someone telling us that someone is dying 20 km away, and you seeing a video of someone dying 20km away, has different emotional arousal, while both are events of stoppable death of someone.I believe that is why I am justified in looking at the response of someone who is in a place of very high emotional arousal. Hence we look at the doctor to provide us data regarding our potential role in the society.

There were many other doctors, in the same hospital, but only one had the balls while others gave up (according to the story atleast). There were many in the world, but it took Gandhi to make change. Gandhi went all the way, right from his clothing, to not harming anyone at any cost and all his values. Basically, shouldn't emotional arousal be the same for everybody, who was in the situation? Why didn't everyone in the scenario act? I am afraid of naively analyzing the situation but anyways here goes.

How am I justified in assuming the actions of gandhi or this doctor was the right one and not the other doctors or compounders who did little or gave up. Having emotional arousal, doesn't necessarily translate to action. Emotional arousal is only a telling for us as to what is important? Also, Capability, belief in oneself, leadership could be one thing that differentiates one from the other. Not everyone can get work done, or creatively think and get the work done. When you think of yourself, and the cost you will undergo, it is possible I guess for the other doctors to stay away still. Could it be that the other doctors didn't have the capability or the ... Would they have done it if they had the resources such as money and such as ideas to save people. The emotional arousal could have been the same... But anyways, we do not want to naively comment about things we don't have information over. So what do we want to do? 

Why don't we find it in all humans? We see MLA's doing all sorts of shit, not caring for people, allowing them to suffer, why not use these people as role models? Does the same emotional arousal still hold?

How can I say Gandhi represented the value system of 'life importance', and not the others who were killing people? I seem to be coming back to this over and over again!

So maybe, one life is worth X and n lives are worth nX. Now some other event maybe worth Y and m times that might be worth mY. We know we can't necessarily trust the brain with all sorts of calculations. But then the brain can point to what we might want in life. Maybe we can take an isolated incident and run in our minds and determine its possible value. Then we check it against actions in the world. So it maybe possible to know if X>Y or Y>X. It may also be possible to determine its value say using money. I am sure I can come up with something other than the value of life that is worth to me say Y. m*Y can be greater than X (saving one life), where m can be sufficiently large. What happens in this case? What if Y is something as silly as having sex.

And could there be a case when one life is worth more than the other? For example, we have this innate thing I think, where we think that saving a child and mother is much more important, dating back to the days when this heuristic made sense (stone age).

#### Naive reasoning

- I am extremely worried about the way I am reasoning. Sometimes I might say that 'this is all we have, we just need to consult with the solution inside our head', and leave it at that, because thats all we got. But you could easily be wrong. I guess thats why we have peer-reviews, but what if the peer is also misguided, like the souls of terrorists?

For me it appears that I know what I wan't the solution to be, I am consciously adding reasoning to get me there.



The observations we have to determine the reality are the follows:
- One individual child received more money than the entire group (inversely propotional)
- 1k birds and 10k birds having the same willingness to pay (A function like Log(n), which plateaus with large n, where n is the number of lives saved)
- All lives are equal and so every life should be worth the same.
- Every life is not worth the same, we need to look at the contribution to the society?

When you see that people pay the same amount to save 1k birds and 10k birds, something seems not right. It could be that 
I find myself arguing with myself that the whole curve needn't be linear. For every life saved, the value to us needn't be X, it could be less than X with every additional life saved. When we look at examples in Scope insensitivity, the only possible contender is log(n), where n is the number of lives saved. This could be the Is there any indication that it might be otherwise? I am thinking out loud here. When we say that the life value varies with number of lives quatratically, we basically mean to say that the first life is worth X, the second life is worth As we have seen in the examples of scope sensitivity, it appears that the brain doesn't understand large numbers at all. It is very hard to believe the existence of some mechanism to help us go by that curve.

#### **Utility function**

It appears that the brain might have a utility function that needs to be maximized as much as possible with the available resources.
Are we trying to maximize only happiness? No. We are ready to give up sex, food and money, sometimes, and we might also be ready to take some pain to save someone. Remember the movie Gandhi? He gave up everything and led a simple life. There was only one thing that was important to him. He was jailed, he often starved, he went through a wide amount of discomfort. People hit him, beat him, but yet he seemed to be still going in the direction that mattered. He was trying to maximize something. Perhaps the welfare of his folk? But Why? Is it more important to him than his life, health etc.? He also had many 100's of followers atleast, who accompanied him on his way to be beaten and jailed. What were they trying to maximize?

So maybe, one life is worth X and n lives are worth nX. Now some other event maybe worth Y and m times that might be worth mY. We know we can't necessarily trust the brain with all sorts of calculations. But then the brain can point to what we might want in life. Maybe we can take an isolated incident and run in our minds and determine its possible value. Then we check it against actions in the world. So it maybe possible to know if X>Y or Y>X. 

We are able to make choices. For example, when we are in a situation where we have to save a life or continue having sex, we would happily choose to save the life. So we are able to make choices, and tradeoffs. Naively speaking, So it seems like Event Y (sex) is less favorable that event X(saving a life). In other words, value from Event X > Value from Event Y.

Is it then alright to say that it appears that there is a utility function such that:

	$ Utility function = a1 *X + a2 *Y +a3 *Z + ...

Where in at any given point of time in life, you can do a couple of things, which will maximize the Overall utility. So when we save a life, our utility function becomes quite high as compared to having sex at a given moment. Maybe it is much more complex than this, but for now, lets go on with this simple model. 

For now we call this the utility function, a function with all sorts of variables, which outputs a single number for us to know how our actions have faired. And yes you guessed it right, we have to uncover what is important to this utility function as well. Which variable is scaled high etc...

The utility function for example can be made of variables such as 'number of happy life years added', 'contribution back to the society', 'number of lives saved now', 'number of lives saved in the future' but as always we have to discover it, uncover it.
How do I go about determining what is important to this system?(!)

#### **Further options**

All the above is fine. I have a strong want to satisfy my value system. I have feelings which are part of me which attempt to satisfy the value system. But then they only attempt, and are far from satisfying the true value system. 

There is this idea inside me that there are three things I can possibly do. 
1) Follow the true value system, and get benefited as a by-product by the [evidence][martin_ted] shown by Martin Seligman on 'life of flow' and 'life of purpose'.

It's going to be hard. It will probably involve a lot of writing, and critical thinking and most importantly ACTION leading to impact.

2) Do what I am currently doing, like everyone else, continue to have those circular preferences, focus on the pleasurable life i.e., girls, video games, buying stuff etc. Learn an instrument and a sport and really be good at it and enjoy the life of flow. There is [undeniable evidence][martin_ted] where in Martin Seligman shows that the pleasurable life can only take you so far and that the life of flow will take you further. So have a mix of both?

This is what seems to come naturally to me. It's much easier than option 1. It's a mixture of pure feelings and some thought.

3) Don't do anything, just die off, because this whole life doesn't make sense. Why are you listening to some random stupid value system in the first place, especially considering that it just happened somehow that we have values? 

There is just no point in life. Why are we trying to maximize the value system of some random event(evolution). Who cares? Look at this whole life system from the higher point of view. It seems to have no meaning. Just like how we look at ants. Absolutely purposeless, is their existence. Why are we any different? Just like ants, we also do so many things to fulfill our value system.



[martin]:https://en.wikipedia.org/wiki/Martin_Seligman
[martin_ted]:https://www.youtube.com/watch?v=9FBxfd7DL3E
[ele_intutions]:http://lesswrong.com/lw/n9/the_intuitions_behind_utilitarianism/):
[ele_truth]:http://yudkowsky.net/rational/the-simple-truth/
[ele_circular]:http://lesswrong.com/lw/n3/circular_altruism/
[tj_solution]:/solution-inside-my-head.html
[peter_ted]:https://www.youtube.com/watch?v=Diuv3XZQXyc
[sugar]:http://www.sheknows.com/health-and-wellness/articles/1025709/your-body-on-sugar-cravings
[wiki_social]:https://en.wikipedia.org/wiki/Social_proof
[no_link]:/story-so-far.html
[stm]:http://pradeep90.github.io/The-Solution-in-your-Head.html
[story_1]:/story-so-far.html
[story_2]:/determining-the-value-system.html
[story_i]:http://www.dnaindia.com/india/report-gorakhpur-tragedy-meet-dr-kafeel-khan-the-hero-who-saved-the-lives-of-countless-children-2528035
